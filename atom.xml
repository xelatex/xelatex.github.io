<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title><![CDATA[Arthur Chunqi Li's Blog]]></title>
  
  <link href="/atom.xml" rel="self"/>
  <link href="http://chunqi.li/"/>
  <updated>2015-11-09T07:34:05.000Z</updated>
  <id>http://chunqi.li/</id>
  
  <author>
    <name><![CDATA[Arthur Chunqi Li]]></name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title><![CDATA[Docker Multi-host Overlay Networking with Etcd]]></title>
    <link href="http://chunqi.li/2015/11/09/docker-multi-host-networking/"/>
    <id>http://chunqi.li/2015/11/09/docker-multi-host-networking/</id>
    <published>2015-11-09T06:27:55.000Z</published>
    <updated>2015-11-09T07:34:05.000Z</updated>
    <content type="html"><![CDATA[<p><a href="http://docker.io/" target="_blank" rel="external">Docker</a> has released its newest version v1.9 (<a href="https://blog.docker.com/2015/11/docker-1-9-production-ready-swarm-multi-host-networking/" target="_blank" rel="external">see details</a>) on November 3, 2015. This big release put Swarm and multi-host networking into production-ready status. This blog illustrates the configuration and a few evaluations of Docker multi-host overlay networking.</p>
<h1 id="Multi-host_Networking">Multi-host Networking</h1><p><a href="https://blog.docker.com/2015/06/networking-receives-an-upgrade/" target="_blank" rel="external">Multi-host Networking was announced as part of experimental release in June, 2015</a>, and turns to stable release of Docker Engine this month. There are already several Multi-host networking solutions for docker, such as <a href="/2015/11/06/calico-docker/">Calico</a> and <a href="https://coreos.com/blog/introducing-rudder/" target="_blank" rel="external">Flannel</a>. Docker multi-host networking uses VXLAN-based solution with the help of <code>libnetwork</code> and <code>libkv</code> library. So the <code>overlay</code> network requires a valid key-value store service to exchange informations between different docker engines. Docker implements a built-in <a href="https://datatracker.ietf.org/doc/rfc7348/" target="_blank" rel="external">VXLAN-based overlay network driver</a> in <code>libnetwork</code> library to support a wide range virtual network between multiple hosts.</p>
<h1 id="Prerequisite">Prerequisite</h1><h2 id="Environment_Preparation">Environment Preparation</h2><p>Before using Docker overlay networking, check the version of docker with <code>docker -v</code> to confirm that docker version is no less than v1.9. In this blog I prepare an environment with two Linux nodes (node1/node2) with IP 192.168.236.130/131 and connect them physically or virtually, and confirm they have network access to each other.</p>
<p>ownload and run etcd, replace {node} with node0/1 seperately. We need at least two etcd node since the new version of etcd cannot run on single node.</p>
<figure class="highlight bash"><figcaption><span>Download and run etcd</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">curl -L  https://github.com/coreos/etcd/releases/download/v2.<span class="number">2.1</span>/etcd-v2.<span class="number">2.1</span>-linux-amd64.tar.gz -o etcd-v2.<span class="number">2.1</span>-linux-amd64.tar.gz</span><br><span class="line">tar xzvf etcd-v2.<span class="number">2.1</span>-linux-amd64.tar.gz</span><br><span class="line"><span class="built_in">cd</span> etcd-v2.<span class="number">2.1</span>-linux-amd64</span><br><span class="line">./etcd -name &#123;node&#125; -initial-advertise-peer-urls http://<span class="number">0.0</span>.<span class="number">0.0</span>:<span class="number">2380</span> \</span><br><span class="line">  -listen-peer-urls http://<span class="number">0.0</span>.<span class="number">0.0</span>:<span class="number">2380</span> \</span><br><span class="line">  -listen-client-urls http://<span class="number">0.0</span>.<span class="number">0.0</span>:<span class="number">2379</span>,http://<span class="number">127.0</span>.<span class="number">0.1</span>:<span class="number">4001</span> \</span><br><span class="line">  -advertise-client-urls http://<span class="number">0.0</span>.<span class="number">0.0</span>:<span class="number">2379</span> \</span><br><span class="line">  -initial-cluster-token etcd-cluster \</span><br><span class="line">  -initial-cluster node1=http://<span class="number">192.168</span>.<span class="number">236.130</span>:<span class="number">2380</span>,node2=http://<span class="number">192.168</span>.<span class="number">236.131</span>:<span class="number">2380</span> \</span><br><span class="line">  -initial-cluster-state new</span><br></pre></td></tr></table></figure>
<h2 id="Start_Docker_Daemon_With_Cluster_Parameters">Start Docker Daemon With Cluster Parameters</h2><p>Docker Engine daemon should be started with cluster parameters <code>--cluster-store</code> and <code>--cluster-advertise</code>, thus all Docker Engine running on different nodes could communicate and cooperate with each other. Here we need to set <code>--cluster-store</code> with Etcd service host and port and <code>--cluster-advertise</code> with IP and Docker Daemon port on this node. Stop current docker daemon and start with new params.</p>
<p>On node1:<br><figure class="highlight bash"><figcaption><span>Run Docker daemon with cluster params</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">sudo service docker stop</span><br><span class="line">sudo /usr/bin/docker daemon -H tcp://<span class="number">0.0</span>.<span class="number">0.0</span>:<span class="number">2375</span> -H unix:///var/run/docker.sock --cluster-store=etcd://<span class="number">192.168</span>.<span class="number">236.130</span>:<span class="number">2379</span> --cluster-advertise=<span class="number">192.168</span>.<span class="number">236.130</span>:<span class="number">2375</span></span><br></pre></td></tr></table></figure></p>
<p>On node2:<br><figure class="highlight bash"><figcaption><span>Run Docker daemon with cluster params</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">sudo service docker stop</span><br><span class="line">sudo /usr/bin/docker daemon -H tcp://<span class="number">0.0</span>.<span class="number">0.0</span>:<span class="number">2375</span> -H unix:///var/run/docker.sock --cluster-store=etcd://<span class="number">192.168</span>.<span class="number">236.131</span>:<span class="number">2379</span> --cluster-advertise=<span class="number">192.168</span>.<span class="number">236.131</span>:<span class="number">2375</span></span><br></pre></td></tr></table></figure></p>
<p>All preparations are done until now.</p>
<h1 id="Create_Overlay_Network">Create Overlay Network</h1><p>On either node, we can execute <code>docker network ls</code> to see the network configuration of Docker. Here’s the example of node1:<br><figure class="highlight bash"><figcaption><span>Docker network configuration</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">docker@node1:~<span class="comment"># sudo docker network ls</span></span><br><span class="line">NETWORK ID          NAME                DRIVER</span><br><span class="line"><span class="number">80</span>a36a28041f        bridge              bridge</span><br><span class="line"><span class="number">6</span>b7eab031544        none                null</span><br><span class="line"><span class="number">464</span>fe03753fb        host                host</span><br></pre></td></tr></table></figure></p>
<p>Then we also use <code>docker network</code> command to create a new overlay network.<br><figure class="highlight bash"><figcaption><span>Docker network configuration</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">docker@node1:~<span class="comment"># sudo docker network create -d overlay myapp</span></span><br><span class="line"><span class="number">904</span>f9dc335b0f91fe155b26829287c7de7c17af5cfeb9c386a1ccf75c42<span class="built_in">cd</span>3eb</span><br></pre></td></tr></table></figure></p>
<p>Wait for a minute and we can see the output of this command is the ID of this overlay network. Then execute <code>docker network ls</code> on either node:<br><figure class="highlight bash"><figcaption><span>Docker network configuration</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">docker@node1:~<span class="comment"># sudo docker network ls</span></span><br><span class="line">NETWORK ID          NAME                DRIVER</span><br><span class="line"><span class="number">904</span>f9dc335b0        myapp               overlay</span><br><span class="line"><span class="number">80</span>a36a28041f        bridge              bridge</span><br><span class="line"><span class="number">6</span>b7eab031544        none                null</span><br><span class="line"><span class="number">464</span>fe03753fb        host                host</span><br><span class="line"><span class="number">52</span>e9119e18d5        docker_gwbridge     bridge</span><br></pre></td></tr></table></figure></p>
<p>On both node1 and node2, two network <code>myapp</code> and <code>docker_gwbridge</code> are added with type <code>overlay</code> and <code>bridge</code> seperately. Thus <code>myapp</code> represents the overlay network associated with <code>eth0</code> in containers, and <code>docker_gwbridge</code> represents the bridge network connecting Internet associated with <code>eth1</code> in containers.</p>
<h1 id="Create_Containers_With_Overlay_Network">Create Containers With Overlay Network</h1><p>On node1:<br><figure class="highlight bash"><figcaption><span>Docker network configuration</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker@node1:~<span class="comment"># sudo docker run -itd --name=worker-1 --net=myapp ubuntu</span></span><br></pre></td></tr></table></figure></p>
<p>And on node2:<br><figure class="highlight bash"><figcaption><span>Docker network configuration</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker@node1:~<span class="comment"># sudo docker run -itd --name=worker-2 --net=myapp ubuntu</span></span><br></pre></td></tr></table></figure></p>
<p>Then test the connection between two containers. On node1, execute:<br><figure class="highlight bash"><figcaption><span>Docker networks</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line">docker@node1:~/etcd-v2.<span class="number">0.9</span>-linux-amd64<span class="comment"># sudo docker exec worker-1 ifconfig</span></span><br><span class="line">eth0      Link encap:Ethernet  HWaddr <span class="number">02</span>:<span class="number">42</span>:<span class="number">0</span>a:<span class="number">00</span>:<span class="number">00</span>:<span class="number">02</span></span><br><span class="line">          inet addr:<span class="number">10.0</span>.<span class="number">0.2</span>  Bcast:<span class="number">0.0</span>.<span class="number">0.0</span>  Mask:<span class="number">255.255</span>.<span class="number">255.0</span></span><br><span class="line">          inet6 addr: fe80::<span class="number">42</span>:aff:fe00:<span class="number">2</span>/<span class="number">64</span> Scope:Link</span><br><span class="line">          UP BROADCAST RUNNING MULTICAST  MTU:<span class="number">1450</span>  Metric:<span class="number">1</span></span><br><span class="line">          RX packets:<span class="number">5475264</span> errors:<span class="number">0</span> dropped:<span class="number">0</span> overruns:<span class="number">0</span> frame:<span class="number">0</span></span><br><span class="line">          TX packets:<span class="number">846008</span> errors:<span class="number">0</span> dropped:<span class="number">0</span> overruns:<span class="number">0</span> carrier:<span class="number">0</span></span><br><span class="line">          collisions:<span class="number">0</span> txqueuelen:<span class="number">0</span></span><br><span class="line">          RX bytes:<span class="number">7999457912</span> (<span class="number">7.9</span> GB)  TX bytes:<span class="number">55842488</span> (<span class="number">55.8</span> MB)</span><br><span class="line"></span><br><span class="line">eth1      Link encap:Ethernet  HWaddr <span class="number">02</span>:<span class="number">42</span>:ac:<span class="number">12</span>:<span class="number">00</span>:<span class="number">02</span></span><br><span class="line">          inet addr:<span class="number">172.18</span>.<span class="number">0.2</span>  Bcast:<span class="number">0.0</span>.<span class="number">0.0</span>  Mask:<span class="number">255.255</span>.<span class="number">0.0</span></span><br><span class="line">          inet6 addr: fe80::<span class="number">42</span>:acff:fe12:<span class="number">2</span>/<span class="number">64</span> Scope:Link</span><br><span class="line">          UP BROADCAST RUNNING MULTICAST  MTU:<span class="number">1500</span>  Metric:<span class="number">1</span></span><br><span class="line">          RX packets:<span class="number">12452</span> errors:<span class="number">0</span> dropped:<span class="number">0</span> overruns:<span class="number">0</span> frame:<span class="number">0</span></span><br><span class="line">          TX packets:<span class="number">6883</span> errors:<span class="number">0</span> dropped:<span class="number">0</span> overruns:<span class="number">0</span> carrier:<span class="number">0</span></span><br><span class="line">          collisions:<span class="number">0</span> txqueuelen:<span class="number">0</span></span><br><span class="line">          RX bytes:<span class="number">22021017</span> (<span class="number">22.0</span> MB)  TX bytes:<span class="number">376719</span> (<span class="number">376.7</span> KB)</span><br><span class="line"></span><br><span class="line">lo        Link encap:Local Loopback</span><br><span class="line">          inet addr:<span class="number">127.0</span>.<span class="number">0.1</span>  Mask:<span class="number">255.0</span>.<span class="number">0.0</span></span><br><span class="line">          inet6 addr: ::<span class="number">1</span>/<span class="number">128</span> Scope:Host</span><br><span class="line">          UP LOOPBACK RUNNING  MTU:<span class="number">65536</span>  Metric:<span class="number">1</span></span><br><span class="line">          RX packets:<span class="number">0</span> errors:<span class="number">0</span> dropped:<span class="number">0</span> overruns:<span class="number">0</span> frame:<span class="number">0</span></span><br><span class="line">          TX packets:<span class="number">0</span> errors:<span class="number">0</span> dropped:<span class="number">0</span> overruns:<span class="number">0</span> carrier:<span class="number">0</span></span><br><span class="line">          collisions:<span class="number">0</span> txqueuelen:<span class="number">0</span></span><br><span class="line">          RX bytes:<span class="number">0</span> (<span class="number">0.0</span> B)  TX bytes:<span class="number">0</span> (<span class="number">0.0</span> B)</span><br></pre></td></tr></table></figure></p>
<p>Here we can see two NICs in container with IP 10.0.0.2 and 172.18.0.2. <code>eth0</code> connects to the overlay network and <code>eth1</code> connects to docker_gwbridge. Thus the container will both have access to containers on other host as well as Google. Run the same command on node2 and we can see the IP of <code>eth0</code> in worker-2 is 10.0.0.3, which is assigned continuously.</p>
<p>Then test the connections between worker-1 and worker-2, execute command on node1:<br><figure class="highlight bash"><figcaption><span>Docker network configuration</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">docker@node1:~<span class="comment"># sudo docker exec worker-1 ping -c 4 10.0.0.3</span></span><br><span class="line">PING <span class="number">10.0</span>.<span class="number">0.3</span> (<span class="number">10.0</span>.<span class="number">0.3</span>) <span class="number">56</span>(<span class="number">84</span>) bytes of data.</span><br><span class="line"><span class="number">64</span> bytes from <span class="number">10.0</span>.<span class="number">0.3</span>: icmp_seq=<span class="number">1</span> ttl=<span class="number">64</span> time=<span class="number">0.735</span> ms</span><br><span class="line"><span class="number">64</span> bytes from <span class="number">10.0</span>.<span class="number">0.3</span>: icmp_seq=<span class="number">2</span> ttl=<span class="number">64</span> time=<span class="number">0.581</span> ms</span><br><span class="line"><span class="number">64</span> bytes from <span class="number">10.0</span>.<span class="number">0.3</span>: icmp_seq=<span class="number">3</span> ttl=<span class="number">64</span> time=<span class="number">0.444</span> ms</span><br><span class="line"><span class="number">64</span> bytes from <span class="number">10.0</span>.<span class="number">0.3</span>: icmp_seq=<span class="number">4</span> ttl=<span class="number">64</span> time=<span class="number">0.447</span> ms</span><br><span class="line"></span><br><span class="line">--- <span class="number">10.0</span>.<span class="number">0.3</span> ping statistics ---</span><br><span class="line"><span class="number">4</span> packets transmitted, <span class="number">4</span> received, <span class="number">0</span>% packet loss, time <span class="number">3000</span>ms</span><br><span class="line">rtt min/avg/max/mdev = <span class="number">0.444</span>/<span class="number">0.551</span>/<span class="number">0.735</span>/<span class="number">0.122</span> ms</span><br></pre></td></tr></table></figure></p>
<h1 id="Performance_Tests">Performance Tests</h1><p>I did a simple performance test between two containers with <code>iperf</code>, and here is the result.</p>
<p>First I tested the native network performance between node1 and node2:</p>
<pre><code>docker@node2:~<span class="preprocessor"># iperf -c <span class="number">192.168</span><span class="number">.236</span><span class="number">.130</span></span>
------------------------------------------------------------
Client connecting to <span class="number">192.168</span><span class="number">.236</span><span class="number">.130</span>, TCP port <span class="number">5001</span>
TCP window size:  <span class="number">136</span> KByte (<span class="keyword">default</span>)
------------------------------------------------------------
[  <span class="number">3</span>] local <span class="number">192.168</span><span class="number">.236</span><span class="number">.131</span> port <span class="number">36910</span> connected with <span class="number">192.168</span><span class="number">.236</span><span class="number">.130</span> port <span class="number">5001</span>
[ ID] Interval       Transfer     Bandwidth
[  <span class="number">3</span>]  <span class="number">0.0</span>-<span class="number">10.0</span> sec  <span class="number">2.59</span> GBytes  <span class="number">2.22</span> Gbits/sec
</code></pre><p>Then network performance between worker-1 and worker-2:</p>
<pre><code>root@<span class="number">3f</span>8bc51fb458:~<span class="preprocessor"># iperf -c <span class="number">10.0</span><span class="number">.0</span><span class="number">.2</span></span>
------------------------------------------------------------
Client connecting to <span class="number">10.0</span><span class="number">.0</span><span class="number">.2</span>, TCP port <span class="number">5001</span>
TCP window size: <span class="number">81.0</span> KByte (<span class="keyword">default</span>)
------------------------------------------------------------
[  <span class="number">3</span>] local <span class="number">10.0</span><span class="number">.0</span><span class="number">.3</span> port <span class="number">48096</span> connected with <span class="number">10.0</span><span class="number">.0</span><span class="number">.2</span> port <span class="number">5001</span>
[ ID] Interval       Transfer     Bandwidth
[  <span class="number">3</span>]  <span class="number">0.0</span>-<span class="number">10.0</span> sec  <span class="number">1.84</span> GBytes  <span class="number">1.58</span> Gbits/sec
</code></pre><p>The overlay network performance is a bit worse than native. It’s also a little worse than <a href="/2015/11/06/calico-docker/#Performance Tests">Calico</a>, which is almost the same as native performance. Since Calico uses a pure 3-Layer protocol and Docker Multi-host Overlay Network uses VXLAN solution (MAC on UDP), Calico does make sense to gain a better performance.</p>
<h1 id="VXLAN_Technology">VXLAN Technology</h1><p>Virtual Extensible LAN (VXLAN) is a network virtualization technology that attempts to ameliorate the scalability problems associated with large cloud computing deployments. It uses a VLAN-like encapsulation technique to encapsulate MAC-based OSI layer 2 Ethernet frames within layer 4 UDP packets. <a href="https://en.wikipedia.org/wiki/Open_vSwitch" target="_blank" rel="external">Open vSwitch</a> is a former implementation of VXLAN, but Docker Engine implements a built-in VXLAN driver in libnetwork.</p>
<p>For more VXLAN details, you can see its <a href="https://datatracker.ietf.org/doc/rfc7348/" target="_blank" rel="external">official RFC</a> and a <a href="https://www.emulex.com/artifacts/d658610a-d3b6-457c-bf2d-bf8d476c6a98/elx_wp_all_VXLAN.pdf" target="_blank" rel="external">white paper</a> from EMulex. I’d like to post another blog to have more detailed discussion on VXLAN Technology.</p>
<h1 id="References">References</h1><p>[1] Docker Multi-host Networking Post: <a href="http://blog.docker.com/2015/11/docker-multi-host-networking-ga/" target="_blank" rel="external">http://blog.docker.com/2015/11/docker-multi-host-networking-ga/</a><br>[2] Docker Network Docs: <a href="http://docs.docker.com/engine/userguide/networking/dockernetworks/" target="_blank" rel="external">http://docs.docker.com/engine/userguide/networking/dockernetworks/</a><br>[3] Get Started Overlay Network for Docker: <a href="https://docs.docker.com/engine/userguide/networking/get-started-overlay/" target="_blank" rel="external">https://docs.docker.com/engine/userguide/networking/get-started-overlay/</a><br>[4] Docker v1.9 Announcemount: <a href="https://blog.docker.com/2015/11/docker-1-9-production-ready-swarm-multi-host-networking/" target="_blank" rel="external">https://blog.docker.com/2015/11/docker-1-9-production-ready-swarm-multi-host-networking/</a><br>[5] VXLAN Official RFC: <a href="https://datatracker.ietf.org/doc/rfc7348/" target="_blank" rel="external">https://datatracker.ietf.org/doc/rfc7348/</a><br>[6] VXLAN White Paper: <a href="https://www.emulex.com/artifacts/d658610a-d3b6-457c-bf2d-bf8d476c6a98/elx_wp_all_VXLAN.pdf" target="_blank" rel="external">https://www.emulex.com/artifacts/d658610a-d3b6-457c-bf2d-bf8d476c6a98/elx_wp_all_VXLAN.pdf</a></p>
]]></content>
    <summary type="html">
    <![CDATA[<p><a href="http://docker.io/" target="_blank" rel="external">Docker</a> has released its newest version v1.9 (<a href="https://blog.docker.]]>
    </summary>
    
      <category term="Docker" scheme="http://chunqi.li/tags/Docker/"/>
    
      <category term="Multi-host Network" scheme="http://chunqi.li/tags/Multi-host-Network/"/>
    
      <category term="VXLAN" scheme="http://chunqi.li/tags/VXLAN/"/>
    
      <category term="Docker Network" scheme="http://chunqi.li/categories/Docker-Network/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[Calico: A Solution of Multi-host Network For Docker]]></title>
    <link href="http://chunqi.li/2015/11/06/calico-docker/"/>
    <id>http://chunqi.li/2015/11/06/calico-docker/</id>
    <published>2015-11-06T06:06:56.000Z</published>
    <updated>2015-11-09T06:50:43.000Z</updated>
    <content type="html"><![CDATA[<p><a href="https://github.com/projectcalico/calico" target="_blank" rel="external">Calico</a> is a pure 3-layer protocol to support multi-host network communication for OpenStacks VMs and Docker containers. Calico does not use overlay network such as <a href="https://github.com/coreos/flannel" target="_blank" rel="external">falnnel</a> and <a href="https://github.com/docker/libnetwork/blob/master/docs/overlay.md" target="_blank" rel="external">libnetwork overlay driver</a>, it is a pure Layer 3 approach with a vRouter implementation instead of a vSwitcher. Each vRouter propagates workload reachability information (routes) to the rest of the data center using BGP protocol.</p>
<p>This post focus on how to setup a multi-host networking for Docker containers with <a href="https://github.com/projectcalico/calico-docker" target="_blank" rel="external">calico-docker</a> and some advanced features.</p>
<h1 id="Environment">Environment</h1><h2 id="Environment_Prerequisite">Environment Prerequisite</h2><ul>
<li>Two linux nodes (node1 and node2) with Ubuntu Linux distribution, either VM or physical machine is OK.</li>
<li>Install docker on both nodes.</li>
<li>Etcd cluster.</li>
</ul>
<h2 id="Configuration_&amp;_Download">Configuration &amp; Download</h2><p>Setup two linux nodes with IP 192.168.236.130/131 and connect them physically or virtually, confirm that they can ping each other succesfully. Setup docker bridge (default is docker0) on two nodes. Let’s set two docker bridges with different network. Netowrk configuration details are as follows:</p>
<p>Node1</p>
<ul>
<li>IP: 192.168.236.130</li>
<li>Docker bridge network: 192.168.1.0/24</li>
</ul>
<p>Node2</p>
<ul>
<li>IP: 192.168.236.131</li>
<li>Docker bridge network: 172.17.0.0/16</li>
</ul>
<p>Install Docker, should be no error here.</p>
<figure class="highlight bash"><figcaption><span>Install Docker</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">sudo apt-get install docker.io</span><br><span class="line">docker ps</span><br></pre></td></tr></table></figure>
<p>Download and run etcd, replace {node} with node0/1 seperately. We need at least two etcd node since the new version of etcd cannot run on single node.</p>
<figure class="highlight bash"><figcaption><span>Download and run etcd</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">curl -L  https://github.com/coreos/etcd/releases/download/v2.<span class="number">2.1</span>/etcd-v2.<span class="number">2.1</span>-linux-amd64.tar.gz -o etcd-v2.<span class="number">2.1</span>-linux-amd64.tar.gz</span><br><span class="line">tar xzvf etcd-v2.<span class="number">2.1</span>-linux-amd64.tar.gz</span><br><span class="line"><span class="built_in">cd</span> etcd-v2.<span class="number">2.1</span>-linux-amd64</span><br><span class="line">./etcd -name &#123;node&#125; -initial-advertise-peer-urls http://<span class="number">0.0</span>.<span class="number">0.0</span>:<span class="number">2380</span> \</span><br><span class="line">  -listen-peer-urls http://<span class="number">0.0</span>.<span class="number">0.0</span>:<span class="number">2380</span> \</span><br><span class="line">  -listen-client-urls http://<span class="number">0.0</span>.<span class="number">0.0</span>:<span class="number">2379</span>,http://<span class="number">127.0</span>.<span class="number">0.1</span>:<span class="number">4001</span> \</span><br><span class="line">  -advertise-client-urls http://<span class="number">0.0</span>.<span class="number">0.0</span>:<span class="number">2379</span> \</span><br><span class="line">  -initial-cluster-token etcd-cluster \</span><br><span class="line">  -initial-cluster node1=http://<span class="number">192.168</span>.<span class="number">236.130</span>:<span class="number">2380</span>,node2=http://<span class="number">192.168</span>.<span class="number">236.131</span>:<span class="number">2380</span> \</span><br><span class="line">  -initial-cluster-state new</span><br></pre></td></tr></table></figure>
<p>Download calicoctl<br><figure class="highlight bash"><figcaption><span>Download calicoctl</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">wget https://github.com/projectcalico/calico-docker/releases/download/v0.<span class="number">10.0</span>/calicoctl</span><br></pre></td></tr></table></figure></p>
<h1 id="Start_Calico_Services">Start Calico Services</h1><p>Calico services in Docker environment are running as a Docker container using host network configuration. All containers configured with Calico services with use calico-node to communicate with each other and Internet.</p>
<p>Run the following commands on node1/2 to start calico-node</p>
<figure class="highlight bash"><figcaption><span>Run calico-node</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo calicoctl node --ip=&#123;host_ip&#125;</span><br></pre></td></tr></table></figure>
<p>You should see output like this on each node</p>
<pre><code>calico@node1:~<span class="comment"># docker ps</span>
CONTAINER ID        IMAGE                COMMAND             CREATED             STATUS              PORTS               NAMES
<span class="number">40</span>b177803c97        calico/<span class="keyword">node</span><span class="identifier"></span><span class="title">:v0</span>.<span class="number">9.0</span>   <span class="string">"/sbin/my_init"</span>     <span class="number">27</span> seconds ago      Up <span class="number">27</span> seconds                           calico-<span class="keyword">node</span><span class="identifier"></span><span class="title"></span>
</code></pre><p>Before starting any containers, we need to configure an IP pool with the <code>ipip</code> and <code>nat-outgoing</code> options. Thus containers with an valid profile could have access to Internet. Run the following command on either node.</p>
<figure class="highlight bash"><figcaption><span>Configure IP pool</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">calicoctl pool add <span class="number">192.168</span>.<span class="number">100.0</span>/<span class="number">24</span> --ipip --nat-outgoing</span><br></pre></td></tr></table></figure>
<h1 id="Container_Networking_Configuration">Container Networking Configuration</h1><h2 id="Start_Containers">Start Containers</h2><p>Firstly run a few containers on each host.</p>
<p>On node1:<br><figure class="highlight bash"><figcaption><span>Run container on node1</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">docker run --net=none --name worker-<span class="number">1</span> -tid ubuntu</span><br><span class="line">docker run --net=none --name worker-<span class="number">2</span> -tid ubuntu</span><br></pre></td></tr></table></figure></p>
<p>On node2:<br><figure class="highlight bash"><figcaption><span>Run container on node2</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker run --net=none --name worker-<span class="number">3</span> -tid ubuntu</span><br></pre></td></tr></table></figure></p>
<h2 id="Configure_Calico_Networking">Configure Calico Networking</h2><p>Now that all the containers are running without any network devices. Use Calico to assign network devices to these containers. Notice that IPs assigned to containers should be in the range of IP pools.</p>
<p>On node1:<br><figure class="highlight bash"><figcaption><span>Configure network on node1</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">sudo calicoctl container add worker-<span class="number">1</span> <span class="number">192.168</span>.<span class="number">100.1</span></span><br><span class="line">sudo calicoctl container add worker-<span class="number">2</span> <span class="number">192.168</span>.<span class="number">100.2</span></span><br></pre></td></tr></table></figure></p>
<p>On node2:<br><figure class="highlight bash"><figcaption><span>Configure network on node2</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo calicoctl container add worker-<span class="number">3</span> <span class="number">192.168</span>.<span class="number">100.3</span></span><br></pre></td></tr></table></figure></p>
<p>Once containers have Calico networking, they gain a network device with corresponding IP address. At this point them have access neither to each other nor to Internet since no profiles are created and assigned to them.</p>
<p>Create some profiles on either node:<br><figure class="highlight bash"><figcaption><span>Create profiles</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">calicoctl profile add PROF_1</span><br><span class="line">calicoctl profile add PROF_2</span><br></pre></td></tr></table></figure></p>
<p>Then assign profiles to containers. Containers in same profile have access to each other. And containers in the IP poll created before won’t have access to Internet until added to a profile.</p>
<p>On node1:<br><figure class="highlight bash"><figcaption><span>Assign profiles to containers on node1</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">calicoctl container worker-<span class="number">1</span> profile append PROF_1</span><br><span class="line">calicoctl container worker-<span class="number">2</span> profile append PROF_2</span><br></pre></td></tr></table></figure></p>
<p>On node2:<br><figure class="highlight bash"><figcaption><span>Assign profiles to containers on node2</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">calicoctl container worker-<span class="number">3</span> profile append PROF_1</span><br></pre></td></tr></table></figure></p>
<p>Until now all configurations are done and we will test network connections of these containers afterwards.</p>
<h1 id="Testing">Testing</h1><p>Now check the connectivities of each containers. At this point every containers should have access to Internet, try and ping google.com:<br><figure class="highlight bash"><figcaption><span>Check Internet access</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">docker <span class="built_in">exec</span> worker-<span class="number">1</span> ping -c <span class="number">4</span> www.google.com</span><br><span class="line">docker <span class="built_in">exec</span> worker-<span class="number">2</span> ping -c <span class="number">4</span> www.google.com</span><br></pre></td></tr></table></figure></p>
<p>Then check connections of containers in same profile:<br><figure class="highlight bash"><figcaption><span>Check inner profile access</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker <span class="built_in">exec</span> worker-<span class="number">1</span> ping -c <span class="number">4</span> <span class="number">192.168</span>.<span class="number">100.3</span></span><br></pre></td></tr></table></figure></p>
<p>And containers not in same profile cannot ping each other:<br><figure class="highlight bash"><figcaption><span>Check access outer profile</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker <span class="built_in">exec</span> worker-<span class="number">1</span> ping -c <span class="number">4</span> <span class="number">192.168</span>.<span class="number">100.2</span></span><br></pre></td></tr></table></figure></p>
<p>If we add worker-2 into profile PROF_1, then worker-2 could ping worker-1 and worker-3.<br>On node1:<br><figure class="highlight bash"><figcaption><span>Advanced check</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">calicoctl container worker-<span class="number">2</span> profile append PROF_1</span><br><span class="line">docker <span class="built_in">exec</span> worker-<span class="number">2</span> ping -c <span class="number">4</span> <span class="number">192.168</span>.<span class="number">100.1</span></span><br><span class="line">docker <span class="built_in">exec</span> worker-<span class="number">2</span> ping -c <span class="number">4</span> <span class="number">192.168</span>.<span class="number">100.3</span></span><br></pre></td></tr></table></figure></p>
<h1 id="Performance_Tests">Performance Tests</h1><h2 id="Simple_Test">Simple Test</h2><p>I perform a simple performance test using <code>iperf</code> to evaluate the network between two Calico containers. Run <code>iperf -s</code> on worker-1 and <code>iperf -c 192.168.100.1</code> on worker-3. We can get the result:</p>
<pre><code>root@<span class="number">39f</span>db1701da4:~<span class="preprocessor"># ./iperf -c <span class="number">192.168</span><span class="number">.101</span><span class="number">.2</span></span>
------------------------------------------------------------
Client connecting to <span class="number">192.168</span><span class="number">.101</span><span class="number">.2</span>, TCP port <span class="number">5001</span>
TCP window size: <span class="number">85.0</span> KByte (<span class="keyword">default</span>)
------------------------------------------------------------
[  <span class="number">3</span>] local <span class="number">192.168</span><span class="number">.101</span><span class="number">.1</span> port <span class="number">39187</span> connected with <span class="number">192.168</span><span class="number">.101</span><span class="number">.2</span> port <span class="number">5001</span>
[ ID] Interval       Transfer     Bandwidth
[  <span class="number">3</span>]  <span class="number">0.0</span>-<span class="number">10.0</span> sec  <span class="number">1.08</span> GBytes   <span class="number">927</span> Mbits/sec
</code></pre><p>Then run the same test on native host (node1 and node2):</p>
<pre><code>calico@node2:~<span class="preprocessor"># iperf -c <span class="number">192.168</span><span class="number">.236</span><span class="number">.130</span></span>
------------------------------------------------------------
Client connecting to <span class="number">192.168</span><span class="number">.236</span><span class="number">.130</span>, TCP port <span class="number">5001</span>
TCP window size: <span class="number">85.0</span> KByte (<span class="keyword">default</span>)
------------------------------------------------------------
[  <span class="number">3</span>] local <span class="number">192.168</span><span class="number">.236</span><span class="number">.131</span> port <span class="number">54584</span> connected with <span class="number">192.168</span><span class="number">.236</span><span class="number">.130</span> port <span class="number">5001</span>
[ ID] Interval       Transfer     Bandwidth
[  <span class="number">3</span>]  <span class="number">0.0</span>-<span class="number">10.0</span> sec  <span class="number">2.57</span> GBytes  <span class="number">2.21</span> Gbits/sec
</code></pre><p>From the result we can see there’s a great gap between Calico network and native network. But according to the official documents and evaluations, calico network should be similar to the native network. <strong>WHY???</strong></p>
<h2 id="Dive_Deeper">Dive Deeper</h2><p>To find out the reason of slow network, firstly I test the network performance between workker-1 and worker-2, which are in the same host. The result is as follows:</p>
<pre><code>root@<span class="number">51</span>b78d9e6153:/<span class="preprocessor"># iperf -c <span class="number">192.168</span><span class="number">.100</span><span class="number">.2</span></span>
------------------------------------------------------------
Client connecting to <span class="number">192.168</span><span class="number">.100</span><span class="number">.3</span>, TCP port <span class="number">5001</span>
TCP window size: <span class="number">85.0</span> KByte (<span class="keyword">default</span>)
------------------------------------------------------------
[  <span class="number">3</span>] local <span class="number">192.168</span><span class="number">.100</span><span class="number">.2</span> port <span class="number">36476</span> connected with <span class="number">192.168</span><span class="number">.100</span><span class="number">.3</span> port <span class="number">5001</span>
[ ID] Interval       Transfer     Bandwidth
[  <span class="number">3</span>]  <span class="number">0.0</span>-<span class="number">10.0</span> sec  <span class="number">47.3</span> GBytes  <span class="number">40.6</span> Gbits/sec
</code></pre><p>Since speed of my net card is only 1Gbits/sec, it seems that containers on the same host connects each other directly without going through any network device. That really make all sense.</p>
<p>Then I dived deep into the documents and configurations of Calico and found such configuration of IP pool:<br><figure class="highlight bash"><figcaption><span>Configure IP pool</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">calicoctl pool add <span class="number">192.168</span>.<span class="number">100.0</span>/<span class="number">24</span> --ipip --nat-outgoing</span><br></pre></td></tr></table></figure></p>
<p>We use <code>--ipip</code> option when creating IP pool, which means <code>Use IP-over-IP encapsulation across hosts</code>. This option will enforce another layer of IP-over-IP encapsulation when packages traveling across hosts. Since our hosts node1 and node2 are in the same network (192.168.236.0/24), we could avoid this option and the speed should increase as supposed.</p>
<p>Run the following command on either node to override the previous IP pool configuration.<br><figure class="highlight bash"><figcaption><span>Configure IP pool</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">calicoctl pool add <span class="number">192.168</span>.<span class="number">100.0</span>/<span class="number">24</span> --nat-outgoing</span><br><span class="line">calicoctl pool show</span><br></pre></td></tr></table></figure></p>
<p>Then test networking between worker-1 and worker-3 again:</p>
<pre><code>root@<span class="number">39f</span>db1701da4:~<span class="preprocessor"># ./iperf -c <span class="number">192.168</span><span class="number">.101</span><span class="number">.2</span></span>
------------------------------------------------------------
Client connecting to <span class="number">192.168</span><span class="number">.101</span><span class="number">.2</span>, TCP port <span class="number">5001</span>
TCP window size: <span class="number">85.0</span> KByte (<span class="keyword">default</span>)
------------------------------------------------------------
[  <span class="number">3</span>] local <span class="number">192.168</span><span class="number">.101</span><span class="number">.1</span> port <span class="number">39187</span> connected with <span class="number">192.168</span><span class="number">.101</span><span class="number">.2</span> port <span class="number">5001</span>
[ ID] Interval       Transfer     Bandwidth
[  <span class="number">3</span>]  <span class="number">0.0</span>-<span class="number">10.0</span> sec  <span class="number">2.74</span> GBytes  <span class="number">2.35</span> Gbits/sec
</code></pre><p>Hurray!!! That’s the native speed!</p>
<h1 id="References">References</h1><p>[1] Project Calico: <a href="https://github.com/projectcalico/calico" target="_blank" rel="external">https://github.com/projectcalico/calico</a><br>[2] Calico Docker: <a href="https://github.com/projectcalico/calico-docker" target="_blank" rel="external">https://github.com/projectcalico/calico-docker</a><br>[3] Demenstration on calico-docker: <a href="https://github.com/projectcalico/calico-docker" target="_blank" rel="external">https://github.com/projectcalico/calico-docker</a><br>[4] Calico-docker in Yixin: <a href="http://mp.weixin.qq.com/s?__biz=MzAwMDU1MTE1OQ==&amp;mid=400983139&amp;idx=1&amp;sn=f033e3dca32ca9f0b7c9779528523e7e&amp;scene=1&amp;srcid=1101jklWCo9jNFjdnUum85PG&amp;from=singlemessage&amp;isappinstalled=0#wechat_redirect" target="_blank" rel="external">Paper URL</a></p>
]]></content>
    <summary type="html">
    <![CDATA[<p><a href="https://github.com/projectcalico/calico" target="_blank" rel="external">Calico</a> is a pure 3-layer protocol to support multi-h]]>
    </summary>
    
      <category term="Calico" scheme="http://chunqi.li/tags/Calico/"/>
    
      <category term="Docker" scheme="http://chunqi.li/tags/Docker/"/>
    
      <category term="Multi-host Network" scheme="http://chunqi.li/tags/Multi-host-Network/"/>
    
      <category term="Docker Network" scheme="http://chunqi.li/categories/Docker-Network/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[L2TP+IPSec on VPS]]></title>
    <link href="http://chunqi.li/2015/05/21/L2TP-IPSec-on-VPS/"/>
    <id>http://chunqi.li/2015/05/21/L2TP-IPSec-on-VPS/</id>
    <published>2015-05-21T06:59:34.000Z</published>
    <updated>2015-11-10T07:10:03.000Z</updated>
    <content type="html"><![CDATA[<p>L2TP+IPSec is another way to setup VPN on a VPS. L2TP consumes 1701 TCP port to maintain connection and 500/4500 UDP to transfer data. It’s very easy to implement L2TP and IPSec on a Ubuntu 14.04 server.</p>
<p>Before setting up L2TP/IPSec environment, you need to enable PPP support for VPS. See details on section <strong>“Enable PPP Support of VPS”</strong> of my previous post “<a href="/2015/05/20/Setup-PPTP-Server-on-a-VPS/">Setup PPTP Server on a VPS</a>“ to enable PPP support on RamNode VPS.</p>
<p>When I first installed xl2tpd and openswan, it occured to me the following errors and refused my iPhone VPN connection:</p>
<pre><code><span class="type">May</span> <span class="number">19</span> <span class="number">05</span>:<span class="number">48</span>:<span class="number">46</span> xxx xl2tpd[<span class="number">1343</span>]: result_code_avp: <span class="literal">result</span> code endianness fix <span class="keyword">for</span> buggy <span class="type">Apple</span> client. network=<span class="number">768</span>, le=<span class="number">3</span>
</code></pre><p>If you get the same error message, just follow step by step with me to setup L2TP+IPSec VPN.</p>
<h1 id="Install_xl2tpd_and_openwan">Install xl2tpd and openwan</h1><p>Here I use openswan as my IPSec server. Just use the following commands to install xl2tpd and openswan:</p>
<figure class="highlight bash"><figcaption><span>Install pptpd</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo apt-get install openswan ppp xl2tpd</span><br></pre></td></tr></table></figure>
<h1 id="Configure_xl2tpd">Configure xl2tpd</h1><p>We need to configure two files for xl2tpd: <code>/etc/xl2tpd/xl2tpd.conf</code> and <code>/etc/ppp/options.xl2tpd</code></p>
<p>Here’s an example of <code>/etc/xl2tpd/xl2tpd.conf</code> :</p>
<pre><code>[global]
listen-addr = <span class="number">106.186</span><span class="number">.127</span><span class="number">.239</span>

[lns <span class="keyword">default</span>]
ip range = <span class="number">10.20</span><span class="number">.0</span><span class="number">.2</span>-<span class="number">10.20</span><span class="number">.0</span><span class="number">.100</span>
local ip = <span class="number">10.20</span><span class="number">.0</span><span class="number">.1</span>
assign ip = yes
length bit = yes
refuse pap = yes
require authentication = yes
pppoptfile = /etc/ppp/options.xl2tpd
</code></pre><p>“ip range” defined IPs distributed to the client side and “local ip” is assigned to the server side. pppoptfile defines the detailed config file for xl2tpd.</p>
<p>Then create file <code>/etc/ppp/options.xl2tpd</code> and add:</p>
<pre><code>ms-dns <span class="number">8.8</span><span class="number">.8</span><span class="number">.8</span>
ms-dns <span class="number">8.8</span><span class="number">.4</span><span class="number">.4</span>
noccp
asyncmap <span class="number">0</span>
auth
crtscts
lock
hide-password
modem
mru <span class="number">1200</span>
nodefaultroute
debug
mtu <span class="number">1200</span>
proxyarp
lcp-echo-interval <span class="number">30</span>
lcp-echo-failure <span class="number">4</span>
ipcp-accept-local
ipcp-accept-remote
noipx
idle <span class="number">1800</span>
connect-delay <span class="number">5000</span>
</code></pre><h1 id="Configure_OpenSwan_IPSec">Configure OpenSwan IPSec</h1><p>IPSec acts as a role to provide a secure routine for transferring data. OpenSwan is a good choice to set up a simple IPSec. Note that there are many IPSec choices and they should be exclusively installed in your system. And whatever IPSec server you installed, the command to call them is only “ipsec“. Use the following command to identify which IPSec service you’re using now.</p>
<pre><code>ipsec <span class="comment">--version</span>
</code></pre><p>The config file for OpenSwan is /etc/ipsec.conf. Actually this file name is identical for all IPSec service, which the content differs anyway. When you installed another IPSec service with apt-get, you need to change the format and contents of this file.</p>
<p>Here’s an example of this file:</p>
<pre><code>version <span class="number">2.0</span>

config setup
    dumpdir=/var/run/pluto/
    nat_traversal=yes
    virtual_private=%v4:<span class="number">10.0</span><span class="number">.0</span><span class="number">.0</span>/<span class="number">8</span>,%v4:<span class="number">192.168</span><span class="number">.0</span><span class="number">.0</span>/<span class="number">16</span>,%v4:<span class="number">172.16</span><span class="number">.0</span><span class="number">.0</span>/<span class="number">12</span>,%v4:<span class="number">25.0</span><span class="number">.0</span><span class="number">.0</span>/<span class="number">8</span>,%v6:fd00::/<span class="number">8</span>,%v6:fe80::/<span class="number">10</span>
    protostack=netkey
    force_keepalive=yes
    keep_alive=<span class="number">60</span>

conn l2tp-psk
    authby=secret
    pfs=no
    <span class="keyword">auto</span>=add
    keyingtries=<span class="number">3</span>
    type=transport
    left=<span class="number">1.2</span><span class="number">.3</span><span class="number">.4</span> <span class="preprocessor"># change to your own IP</span>
    leftprotoport=<span class="number">17</span>/<span class="number">1701</span>
    right=%any
    rightprotoport=<span class="number">17</span>/%any
</code></pre><p>The “virtual_private” line shows which network could use this IPSec routine, leave it as what it is. The only line you need to change is “left”, which should be your VPS IP address.</p>
<p>Then we need to create and edit file <code>/etc/ipsec.secrets</code>.</p>
<pre><code>: PSK <span class="string">"sharedpassword"</span>
</code></pre><p><strong>Note that there’s blank before and after colon!</strong></p>
<p>“sharedpassword” should be used as the “shared secret” when you connect L2TP.</p>
<h1 id="Add_L2TP_VPN_account">Add L2TP VPN account</h1><p>Edit file <code>/etc/ppp/chap-secrets</code>, which is the same as PPTP server. Use the format like this:</p>
<pre><code>yourname <span class="keyword">*</span> yourpassword <span class="keyword">*</span>
</code></pre><h1 id="Setup_IPv4_forwarding_and_iptables_rules">Setup IPv4 forwarding and iptables rules</h1><p>It’s also the same as PPTP server, you just need to edit file <code>/etc/sysctl.conf</code> and add (or change) a following line:</p>
<pre><code>net<span class="class">.ipv4</span><span class="class">.ip_forward</span>=<span class="number">1</span>
</code></pre><p>Then exit to shell and execute:</p>
<figure class="highlight bash"><figcaption><span>Install pptpd</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo sysctl -p</span><br></pre></td></tr></table></figure>
<p>To add iptables rules,  add the following lines in <code>/etc/rc.local</code> :</p>
<pre><code>iptables -t nat -A POSTROUTING -s <span class="number">10.20</span><span class="number">.0</span><span class="number">.0</span>/<span class="number">24</span> -o venet0 -j MASQUERADE
iptables -A FORWARD -p tcp --syn -s <span class="number">10.20</span><span class="number">.0</span><span class="number">.0</span>/<span class="number">24</span> -j TCPMSS --<span class="built_in">set</span>-mss <span class="number">1356</span>
</code></pre><p>Note “-s 10.20.0.0/24” should be the net range defined in “ip range” section of <code>/etc/xl2tpd/xl2tpd.conf</code> .</p>
<p>At last, restart xl2tpd and ipsec:</p>
<figure class="highlight bash"><figcaption><span>Install pptpd</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">sudo service xl2tpd restart</span><br><span class="line">sudo service ipsec restart</span><br></pre></td></tr></table></figure>
<p>Enjoy you surfing! ;)</p>
]]></content>
    <summary type="html">
    <![CDATA[<p>L2TP+IPSec is another way to setup VPN on a VPS. L2TP consumes 1701 TCP port to maintain connection and 500/4500 UDP to transfer data. It]]>
    </summary>
    
      <category term="IPSec" scheme="http://chunqi.li/tags/IPSec/"/>
    
      <category term="L2TP" scheme="http://chunqi.li/tags/L2TP/"/>
    
      <category term="VPN" scheme="http://chunqi.li/tags/VPN/"/>
    
      <category term="VPS" scheme="http://chunqi.li/tags/VPS/"/>
    
      <category term="Fire on the Wall" scheme="http://chunqi.li/categories/Fire-on-the-Wall/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[Setup PPTP Server on a VPS]]></title>
    <link href="http://chunqi.li/2015/05/20/Setup-PPTP-Server-on-a-VPS/"/>
    <id>http://chunqi.li/2015/05/20/Setup-PPTP-Server-on-a-VPS/</id>
    <published>2015-05-20T06:45:45.000Z</published>
    <updated>2015-11-10T07:10:02.000Z</updated>
    <content type="html"><![CDATA[<p>VPS is becoming more and more cheap, fast and powerful these years. Some cheap VPS, such as Linode, Ramnode, DigitalOcean, DirectSpace, are provided for individuals. There are a lot of comparison article across the Internet and you can chose the one fit for you. Here I will list a bunch of methods to surf the internet across a firewall, and also some using experiments.</p>
<h1 id="Traditional_VPN_solutions">Traditional VPN solutions</h1><p>The traditional VPN solutions includes PPTP and L2TP+IPSec solutions. Both are the most popular VPN solutions which are support by almost any smart devices. PPTP and L2TP are all TCP-based VPN, which means a TCP connection must be contained between both ends to keep the status of VPN connection. Thus data lose or connection interrupted on these TCP connections will terminate the VPN connection. Besides these two VPNs are unable to change their TCP connection ports, that’s why PPTP and L2TP are easy to detect and blocked by the firewall. PPTP consumes TCP port 1723 and L2TP takes 1701. It differs on data transfer between these two VPNs. PPTP uses GRE packages  with value 47, which L2TP uses UDP packages via port 500 and 4500, and L2TP may also utilize ESP packages with value 50.</p>
<h1 id="Install_PPTP_server_on_a_VPS">Install PPTP server on a VPS</h1><p>It’s very easy to setup PPTP VPN on any VPS running a Linux distro. I take Ubuntu 14.04 and a ramnode OpenVZ container VPS as an example (the same environment will be used in the following article), you just need to:</p>
<figure class="highlight bash"><figcaption><span>Install pptpd</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo apt-get install pptpd</span><br></pre></td></tr></table></figure>
<p>Then configure pptpd.conf</p>
<figure class="highlight bash"><figcaption><span>Install pptpd</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo nano /etc/pptpd.conf</span><br></pre></td></tr></table></figure>
<p>change the server IP and client IP</p>
<pre><code>localip <span class="number">192.168</span><span class="number">.0</span><span class="number">.1</span>
remoteip <span class="number">192.168</span><span class="number">.0</span><span class="number">.100</span>-<span class="number">200</span>
</code></pre><p>This set the pptp server IP 192.168.0.1 to its ppp device, and distribute 192.168.0.100-200 to the client side ppp device. You could change these to any value you like. But you’d better not change it besides IP range 192.168.0.0/16 and 10.0.0.0/8, since IPs in these two ranges are assigned to LAN. IPs in other range may used by the public servers, and the NAT mechanism (which will be discussed below) may confuse the traffic from the public servers and VPN clients. Localip and remoteip should be in the same network.</p>
<p>Then uncomment the ms-dns and add google like below or OpenDNS:</p>
<pre><code>ms-dns <span class="number">8.8</span><span class="number">.8</span><span class="number">.8</span>
ms-dns <span class="number">8.8</span><span class="number">.4</span><span class="number">.4</span>
</code></pre><p>Now add a VPN user in <code>/etc/ppp/chap-secrets</code> file.</p>
<figure class="highlight bash"><figcaption><span>Install pptpd</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo nano /etc/ppp/chap-secrets</span><br></pre></td></tr></table></figure>
<p>There are four columns in this file. The first is username, choose your favorite one. The second column is service name, such as pptpd or l2tpd. You can use <em> to allow all services using this config line. The third column is your password, stored in plain test (which is awful :-( ). The fourth column presents the IPs allowed to use this config line. Leave it </em> if you want to connect the VPN from anywhere. Here’s an example:</p>
<pre><code>yourname <span class="keyword">*</span> yourpassword <span class="keyword">*</span>
</code></pre><p>Until now we finished all the configuration of PPTP server and we need to restart it.</p>
<figure class="highlight bash"><figcaption><span>Install pptpd</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo /etc/init.d/pptpd restart</span><br></pre></td></tr></table></figure>
<figure class="highlight bash"><figcaption><span>Install pptpd</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo service pptpd restart</span><br></pre></td></tr></table></figure>
<h1 id="Setup_IPv4_Forwrding">Setup IPv4 Forwrding</h1><p>Besides of the configuration above, we need to enable IPv4 forwarding and setup the rules in iptables for SNAT. To enable IPv4 forwarding permanently, you need to edit file <code>/etc/sysctl.conf</code> and add (or change) a following line:</p>
<pre><code>net<span class="class">.ipv4</span><span class="class">.ip_forward</span>=<span class="number">1</span>
</code></pre><p>Then exit to shell and execute:</p>
<figure class="highlight bash"><figcaption><span>Install pptpd</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo sysctl -p</span><br></pre></td></tr></table></figure>
<h1 id="Setup_SNAT_in_iptables">Setup SNAT in iptables</h1><p>To add a rule in iptables, you can add the following lines in <code>/etc/rc.local</code> :</p>
<pre><code>iptables -t nat -A POSTROUTING -s <span class="number">192.168</span><span class="number">.0</span><span class="number">.0</span>/<span class="number">24</span> -o venet0 -j MASQUERADE
iptables -A FORWARD -p tcp --syn -s <span class="number">192.168</span><span class="number">.0</span><span class="number">.0</span>/<span class="number">24</span> -j TCPMSS --<span class="built_in">set</span>-mss <span class="number">1356</span>
</code></pre><p>The first line means SNAT all the traffics from net 192.168.0.0/24 to the IP of local network interface venet0. If you setup PPTP server on a real machine, it maybe eth0 or em0. Check it with command <code>ifconfig</code>. If you adjust 192.168.0.0/24 to your favorite IPs in <code>localip</code> and <code>remoteip</code> sections above, you should replace -s 192.168.0.0/24 with the same IP range here.</p>
<p>The second line is a little trivial and interesting. It means iptables will change MSS field of all the TCP packages with syn in header to 1356. MSS (Maximum Segment Size) defines the maximum size of a TCP package. The default value may be 1500 in some network (1500 is the maximum size in many Ethernet lines). Since VPN will consume a few spaces in the package header, the final size of a package may be larger than the maximum size which can hold by Ethernet line.</p>
<p>There could be some wired problems without setting the second line. Without this, I can ping/traceroute some website successfully but cannot access the pages in browsers.</p>
<p>Now you could use the username and password set in /etc/ppp/chap-secrets to use PPTP VPN. Remenber to enable MPPE encryption connection.</p>
<h1 id="Enable_PPP_Support_of_VPS">Enable PPP Support of VPS</h1><p>PPP support is disabled by default by some VPS providers. You need to enable it manually. For a ramnode VPS, you need to login to its vps control panel (<a href="https://vpscp.ramnode.com/login.php" target="_blank" rel="external">https://vpscp.ramnode.com/login.php</a>), choose “Settings” tab at the bottom of the page and turn PPP on.</p>
<img src="/images/Setup-PPTP-Server-on-a-VPS-01.png">
<p>PPTP and L2TP uses PPP support by kernel, and other VPNs such as AnyConnect, OpenVPN, ShadowVPN utilize TUN/TAP support. So enable TUN/TAP as well.</p>
<p>In the following post, I will introduce how to setup L2TP+IPSec VPN in a OpenVZ VPS.</p>
]]></content>
    <summary type="html">
    <![CDATA[<p>VPS is becoming more and more cheap, fast and powerful these years. Some cheap VPS, such as Linode, Ramnode, DigitalOcean, DirectSpace, a]]>
    </summary>
    
      <category term="PPTP" scheme="http://chunqi.li/tags/PPTP/"/>
    
      <category term="VPN" scheme="http://chunqi.li/tags/VPN/"/>
    
      <category term="VPS" scheme="http://chunqi.li/tags/VPS/"/>
    
      <category term="Fire on the Wall" scheme="http://chunqi.li/categories/Fire-on-the-Wall/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[Hello World]]></title>
    <link href="http://chunqi.li/2015/05/19/hello-world/"/>
    <id>http://chunqi.li/2015/05/19/hello-world/</id>
    <published>2015-05-19T06:06:56.000Z</published>
    <updated>2015-11-10T06:58:22.000Z</updated>
    <content type="html"><![CDATA[<p>This is a Hello World page of Arthur Chunqi Li’s blog.</p>
<figure class="highlight c"><figcaption><span>hello_world.c</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    <span class="built_in">printf</span>(<span class="string">"Hello World!\n"</span>);</span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h2 id="Links:">Links:</h2><ul>
<li>Homepage: <a href="http://chunqi.li/me">http://chunqi.li/me</a></li>
<li>Blog: <a href="http://chunqi.li/">http://chunqi.li/</a> (<a href="http://xelatex.github.io" target="_blank" rel="external">backup</a>)</li>
<li>Facebook: <a href="http://facebook.chunqi.li" target="_blank" rel="external">http://facebook.chunqi.li</a></li>
<li>LinkedIn: <a href="http://linkedin.chunqi.li" target="_blank" rel="external">http://linkedin.chunqi.li</a></li>
</ul>
]]></content>
    <summary type="html">
    <![CDATA[<p>This is a Hello World page of Arthur Chunqi Li’s blog.</p>
<figure class="highlight c"><figcaption><span>hello_world.c</span></figcaption]]>
    </summary>
    
      <category term="Hello World" scheme="http://chunqi.li/tags/Hello-World/"/>
    
      <category term="Hello World" scheme="http://chunqi.li/categories/Hello-World/"/>
    
  </entry>
  
</feed>
