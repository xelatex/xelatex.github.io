<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title><![CDATA[Arthur Chunqi Li's Blog]]></title>
  
  <link href="/atom.xml" rel="self"/>
  <link href="http://chunqi.li/"/>
  <updated>2015-11-09T07:34:05.000Z</updated>
  <id>http://chunqi.li/</id>
  
  <author>
    <name><![CDATA[Arthur Chunqi Li]]></name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title><![CDATA[Docker Multi-host Overlay Networking with Etcd]]></title>
    <link href="http://chunqi.li/2015/11/09/docker-multi-host-networking/"/>
    <id>http://chunqi.li/2015/11/09/docker-multi-host-networking/</id>
    <published>2015-11-09T06:27:55.000Z</published>
    <updated>2015-11-09T07:34:05.000Z</updated>
    <content type="html"><![CDATA[<p><a href="http://docker.io/" target="_blank" rel="external">Docker</a> has released its newest version v1.9 (<a href="https://blog.docker.com/2015/11/docker-1-9-production-ready-swarm-multi-host-networking/" target="_blank" rel="external">see details</a>) on November 3, 2015. This big release put Swarm and multi-host networking into production-ready status. This blog illustrates the configuration and a few evaluations of Docker multi-host overlay networking.</p>
<h1 id="Multi-host_Networking">Multi-host Networking</h1><p><a href="https://blog.docker.com/2015/06/networking-receives-an-upgrade/" target="_blank" rel="external">Multi-host Networking was announced as part of experimental release in June, 2015</a>, and turns to stable release of Docker Engine this month. There are already several Multi-host networking solutions for docker, such as <a href="/2015/11/06/calico-docker/">Calico</a> and <a href="https://coreos.com/blog/introducing-rudder/" target="_blank" rel="external">Flannel</a>. Docker multi-host networking uses VXLAN-based solution with the help of <code>libnetwork</code> and <code>libkv</code> library. So the <code>overlay</code> network requires a valid key-value store service to exchange informations between different docker engines. Docker implements a built-in <a href="https://datatracker.ietf.org/doc/rfc7348/" target="_blank" rel="external">VXLAN-based overlay network driver</a> in <code>libnetwork</code> library to support a wide range virtual network between multiple hosts.</p>
<h1 id="Prerequisite">Prerequisite</h1><h2 id="Environment_Preparation">Environment Preparation</h2><p>Before using Docker overlay networking, check the version of docker with <code>docker -v</code> to confirm that docker version is no less than v1.9. In this blog I prepare an environment with two Linux nodes (node1/node2) with IP 192.168.236.130/131 and connect them physically or virtually, and confirm they have network access to each other.</p>
<p>ownload and run etcd, replace {node} with node0/1 seperately. We need at least two etcd node since the new version of etcd cannot run on single node.</p>
<figure class="highlight bash"><figcaption><span>Download and run etcd</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">curl -L  https://github.com/coreos/etcd/releases/download/v2.<span class="number">2.1</span>/etcd-v2.<span class="number">2.1</span>-linux-amd64.tar.gz -o etcd-v2.<span class="number">2.1</span>-linux-amd64.tar.gz</span><br><span class="line">tar xzvf etcd-v2.<span class="number">2.1</span>-linux-amd64.tar.gz</span><br><span class="line"><span class="built_in">cd</span> etcd-v2.<span class="number">2.1</span>-linux-amd64</span><br><span class="line">./etcd -name &#123;node&#125; -initial-advertise-peer-urls http://<span class="number">0.0</span>.<span class="number">0.0</span>:<span class="number">2380</span> \</span><br><span class="line">  -listen-peer-urls http://<span class="number">0.0</span>.<span class="number">0.0</span>:<span class="number">2380</span> \</span><br><span class="line">  -listen-client-urls http://<span class="number">0.0</span>.<span class="number">0.0</span>:<span class="number">2379</span>,http://<span class="number">127.0</span>.<span class="number">0.1</span>:<span class="number">4001</span> \</span><br><span class="line">  -advertise-client-urls http://<span class="number">0.0</span>.<span class="number">0.0</span>:<span class="number">2379</span> \</span><br><span class="line">  -initial-cluster-token etcd-cluster \</span><br><span class="line">  -initial-cluster node1=http://<span class="number">192.168</span>.<span class="number">236.130</span>:<span class="number">2380</span>,node2=http://<span class="number">192.168</span>.<span class="number">236.131</span>:<span class="number">2380</span> \</span><br><span class="line">  -initial-cluster-state new</span><br></pre></td></tr></table></figure>
<h2 id="Start_Docker_Daemon_With_Cluster_Parameters">Start Docker Daemon With Cluster Parameters</h2><p>Docker Engine daemon should be started with cluster parameters <code>--cluster-store</code> and <code>--cluster-advertise</code>, thus all Docker Engine running on different nodes could communicate and cooperate with each other. Here we need to set <code>--cluster-store</code> with Etcd service host and port and <code>--cluster-advertise</code> with IP and Docker Daemon port on this node. Stop current docker daemon and start with new params.</p>
<p>On node1:<br><figure class="highlight bash"><figcaption><span>Run Docker daemon with cluster params</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">sudo service docker stop</span><br><span class="line">sudo /usr/bin/docker daemon -H tcp://<span class="number">0.0</span>.<span class="number">0.0</span>:<span class="number">2375</span> -H unix:///var/run/docker.sock --cluster-store=etcd://<span class="number">192.168</span>.<span class="number">236.130</span>:<span class="number">2379</span> --cluster-advertise=<span class="number">192.168</span>.<span class="number">236.130</span>:<span class="number">2375</span></span><br></pre></td></tr></table></figure></p>
<p>On node2:<br><figure class="highlight bash"><figcaption><span>Run Docker daemon with cluster params</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">sudo service docker stop</span><br><span class="line">sudo /usr/bin/docker daemon -H tcp://<span class="number">0.0</span>.<span class="number">0.0</span>:<span class="number">2375</span> -H unix:///var/run/docker.sock --cluster-store=etcd://<span class="number">192.168</span>.<span class="number">236.131</span>:<span class="number">2379</span> --cluster-advertise=<span class="number">192.168</span>.<span class="number">236.131</span>:<span class="number">2375</span></span><br></pre></td></tr></table></figure></p>
<p>All preparations are done until now.</p>
<h1 id="Create_Overlay_Network">Create Overlay Network</h1><p>On either node, we can execute <code>docker network ls</code> to see the network configuration of Docker. Here’s the example of node1:<br><figure class="highlight bash"><figcaption><span>Docker network configuration</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">docker@node1:~<span class="comment"># sudo docker network ls</span></span><br><span class="line">NETWORK ID          NAME                DRIVER</span><br><span class="line"><span class="number">80</span>a36a28041f        bridge              bridge</span><br><span class="line"><span class="number">6</span>b7eab031544        none                null</span><br><span class="line"><span class="number">464</span>fe03753fb        host                host</span><br></pre></td></tr></table></figure></p>
<p>Then we also use <code>docker network</code> command to create a new overlay network.<br><figure class="highlight bash"><figcaption><span>Docker network configuration</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">docker@node1:~<span class="comment"># sudo docker network create -d overlay myapp</span></span><br><span class="line"><span class="number">904</span>f9dc335b0f91fe155b26829287c7de7c17af5cfeb9c386a1ccf75c42<span class="built_in">cd</span>3eb</span><br></pre></td></tr></table></figure></p>
<p>Wait for a minute and we can see the output of this command is the ID of this overlay network. Then execute <code>docker network ls</code> on either node:<br><figure class="highlight bash"><figcaption><span>Docker network configuration</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">docker@node1:~<span class="comment"># sudo docker network ls</span></span><br><span class="line">NETWORK ID          NAME                DRIVER</span><br><span class="line"><span class="number">904</span>f9dc335b0        myapp               overlay</span><br><span class="line"><span class="number">80</span>a36a28041f        bridge              bridge</span><br><span class="line"><span class="number">6</span>b7eab031544        none                null</span><br><span class="line"><span class="number">464</span>fe03753fb        host                host</span><br><span class="line"><span class="number">52</span>e9119e18d5        docker_gwbridge     bridge</span><br></pre></td></tr></table></figure></p>
<p>On both node1 and node2, two network <code>myapp</code> and <code>docker_gwbridge</code> are added with type <code>overlay</code> and <code>bridge</code> seperately. Thus <code>myapp</code> represents the overlay network associated with <code>eth0</code> in containers, and <code>docker_gwbridge</code> represents the bridge network connecting Internet associated with <code>eth1</code> in containers.</p>
<h1 id="Create_Containers_With_Overlay_Network">Create Containers With Overlay Network</h1><p>On node1:<br><figure class="highlight bash"><figcaption><span>Docker network configuration</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker@node1:~<span class="comment"># sudo docker run -itd --name=worker-1 --net=myapp ubuntu</span></span><br></pre></td></tr></table></figure></p>
<p>And on node2:<br><figure class="highlight bash"><figcaption><span>Docker network configuration</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker@node1:~<span class="comment"># sudo docker run -itd --name=worker-2 --net=myapp ubuntu</span></span><br></pre></td></tr></table></figure></p>
<p>Then test the connection between two containers. On node1, execute:<br><figure class="highlight bash"><figcaption><span>Docker networks</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line">docker@node1:~/etcd-v2.<span class="number">0.9</span>-linux-amd64<span class="comment"># sudo docker exec worker-1 ifconfig</span></span><br><span class="line">eth0      Link encap:Ethernet  HWaddr <span class="number">02</span>:<span class="number">42</span>:<span class="number">0</span>a:<span class="number">00</span>:<span class="number">00</span>:<span class="number">02</span></span><br><span class="line">          inet addr:<span class="number">10.0</span>.<span class="number">0.2</span>  Bcast:<span class="number">0.0</span>.<span class="number">0.0</span>  Mask:<span class="number">255.255</span>.<span class="number">255.0</span></span><br><span class="line">          inet6 addr: fe80::<span class="number">42</span>:aff:fe00:<span class="number">2</span>/<span class="number">64</span> Scope:Link</span><br><span class="line">          UP BROADCAST RUNNING MULTICAST  MTU:<span class="number">1450</span>  Metric:<span class="number">1</span></span><br><span class="line">          RX packets:<span class="number">5475264</span> errors:<span class="number">0</span> dropped:<span class="number">0</span> overruns:<span class="number">0</span> frame:<span class="number">0</span></span><br><span class="line">          TX packets:<span class="number">846008</span> errors:<span class="number">0</span> dropped:<span class="number">0</span> overruns:<span class="number">0</span> carrier:<span class="number">0</span></span><br><span class="line">          collisions:<span class="number">0</span> txqueuelen:<span class="number">0</span></span><br><span class="line">          RX bytes:<span class="number">7999457912</span> (<span class="number">7.9</span> GB)  TX bytes:<span class="number">55842488</span> (<span class="number">55.8</span> MB)</span><br><span class="line"></span><br><span class="line">eth1      Link encap:Ethernet  HWaddr <span class="number">02</span>:<span class="number">42</span>:ac:<span class="number">12</span>:<span class="number">00</span>:<span class="number">02</span></span><br><span class="line">          inet addr:<span class="number">172.18</span>.<span class="number">0.2</span>  Bcast:<span class="number">0.0</span>.<span class="number">0.0</span>  Mask:<span class="number">255.255</span>.<span class="number">0.0</span></span><br><span class="line">          inet6 addr: fe80::<span class="number">42</span>:acff:fe12:<span class="number">2</span>/<span class="number">64</span> Scope:Link</span><br><span class="line">          UP BROADCAST RUNNING MULTICAST  MTU:<span class="number">1500</span>  Metric:<span class="number">1</span></span><br><span class="line">          RX packets:<span class="number">12452</span> errors:<span class="number">0</span> dropped:<span class="number">0</span> overruns:<span class="number">0</span> frame:<span class="number">0</span></span><br><span class="line">          TX packets:<span class="number">6883</span> errors:<span class="number">0</span> dropped:<span class="number">0</span> overruns:<span class="number">0</span> carrier:<span class="number">0</span></span><br><span class="line">          collisions:<span class="number">0</span> txqueuelen:<span class="number">0</span></span><br><span class="line">          RX bytes:<span class="number">22021017</span> (<span class="number">22.0</span> MB)  TX bytes:<span class="number">376719</span> (<span class="number">376.7</span> KB)</span><br><span class="line"></span><br><span class="line">lo        Link encap:Local Loopback</span><br><span class="line">          inet addr:<span class="number">127.0</span>.<span class="number">0.1</span>  Mask:<span class="number">255.0</span>.<span class="number">0.0</span></span><br><span class="line">          inet6 addr: ::<span class="number">1</span>/<span class="number">128</span> Scope:Host</span><br><span class="line">          UP LOOPBACK RUNNING  MTU:<span class="number">65536</span>  Metric:<span class="number">1</span></span><br><span class="line">          RX packets:<span class="number">0</span> errors:<span class="number">0</span> dropped:<span class="number">0</span> overruns:<span class="number">0</span> frame:<span class="number">0</span></span><br><span class="line">          TX packets:<span class="number">0</span> errors:<span class="number">0</span> dropped:<span class="number">0</span> overruns:<span class="number">0</span> carrier:<span class="number">0</span></span><br><span class="line">          collisions:<span class="number">0</span> txqueuelen:<span class="number">0</span></span><br><span class="line">          RX bytes:<span class="number">0</span> (<span class="number">0.0</span> B)  TX bytes:<span class="number">0</span> (<span class="number">0.0</span> B)</span><br></pre></td></tr></table></figure></p>
<p>Here we can see two NICs in container with IP 10.0.0.2 and 172.18.0.2. <code>eth0</code> connects to the overlay network and <code>eth1</code> connects to docker_gwbridge. Thus the container will both have access to containers on other host as well as Google. Run the same command on node2 and we can see the IP of <code>eth0</code> in worker-2 is 10.0.0.3, which is assigned continuously.</p>
<p>Then test the connections between worker-1 and worker-2, execute command on node1:<br><figure class="highlight bash"><figcaption><span>Docker network configuration</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">docker@node1:~<span class="comment"># sudo docker exec worker-1 ping -c 4 10.0.0.3</span></span><br><span class="line">PING <span class="number">10.0</span>.<span class="number">0.3</span> (<span class="number">10.0</span>.<span class="number">0.3</span>) <span class="number">56</span>(<span class="number">84</span>) bytes of data.</span><br><span class="line"><span class="number">64</span> bytes from <span class="number">10.0</span>.<span class="number">0.3</span>: icmp_seq=<span class="number">1</span> ttl=<span class="number">64</span> time=<span class="number">0.735</span> ms</span><br><span class="line"><span class="number">64</span> bytes from <span class="number">10.0</span>.<span class="number">0.3</span>: icmp_seq=<span class="number">2</span> ttl=<span class="number">64</span> time=<span class="number">0.581</span> ms</span><br><span class="line"><span class="number">64</span> bytes from <span class="number">10.0</span>.<span class="number">0.3</span>: icmp_seq=<span class="number">3</span> ttl=<span class="number">64</span> time=<span class="number">0.444</span> ms</span><br><span class="line"><span class="number">64</span> bytes from <span class="number">10.0</span>.<span class="number">0.3</span>: icmp_seq=<span class="number">4</span> ttl=<span class="number">64</span> time=<span class="number">0.447</span> ms</span><br><span class="line"></span><br><span class="line">--- <span class="number">10.0</span>.<span class="number">0.3</span> ping statistics ---</span><br><span class="line"><span class="number">4</span> packets transmitted, <span class="number">4</span> received, <span class="number">0</span>% packet loss, time <span class="number">3000</span>ms</span><br><span class="line">rtt min/avg/max/mdev = <span class="number">0.444</span>/<span class="number">0.551</span>/<span class="number">0.735</span>/<span class="number">0.122</span> ms</span><br></pre></td></tr></table></figure></p>
<h1 id="Performance_Tests">Performance Tests</h1><p>I did a simple performance test between two containers with <code>iperf</code>, and here is the result.</p>
<p>First I tested the native network performance between node1 and node2:</p>
<pre><code>docker@node2:~<span class="preprocessor"># iperf -c <span class="number">192.168</span><span class="number">.236</span><span class="number">.130</span></span>
------------------------------------------------------------
Client connecting to <span class="number">192.168</span><span class="number">.236</span><span class="number">.130</span>, TCP port <span class="number">5001</span>
TCP window size:  <span class="number">136</span> KByte (<span class="keyword">default</span>)
------------------------------------------------------------
[  <span class="number">3</span>] local <span class="number">192.168</span><span class="number">.236</span><span class="number">.131</span> port <span class="number">36910</span> connected with <span class="number">192.168</span><span class="number">.236</span><span class="number">.130</span> port <span class="number">5001</span>
[ ID] Interval       Transfer     Bandwidth
[  <span class="number">3</span>]  <span class="number">0.0</span>-<span class="number">10.0</span> sec  <span class="number">2.59</span> GBytes  <span class="number">2.22</span> Gbits/sec
</code></pre><p>Then network performance between worker-1 and worker-2:</p>
<pre><code>root@<span class="number">3f</span>8bc51fb458:~<span class="preprocessor"># iperf -c <span class="number">10.0</span><span class="number">.0</span><span class="number">.2</span></span>
------------------------------------------------------------
Client connecting to <span class="number">10.0</span><span class="number">.0</span><span class="number">.2</span>, TCP port <span class="number">5001</span>
TCP window size: <span class="number">81.0</span> KByte (<span class="keyword">default</span>)
------------------------------------------------------------
[  <span class="number">3</span>] local <span class="number">10.0</span><span class="number">.0</span><span class="number">.3</span> port <span class="number">48096</span> connected with <span class="number">10.0</span><span class="number">.0</span><span class="number">.2</span> port <span class="number">5001</span>
[ ID] Interval       Transfer     Bandwidth
[  <span class="number">3</span>]  <span class="number">0.0</span>-<span class="number">10.0</span> sec  <span class="number">1.84</span> GBytes  <span class="number">1.58</span> Gbits/sec
</code></pre><p>The overlay network performance is a bit worse than native. It’s also a little worse than <a href="/2015/11/06/calico-docker/#Performance Tests">Calico</a>, which is almost the same as native performance. Since Calico uses a pure 3-Layer protocol and Docker Multi-host Overlay Network uses VXLAN solution (MAC on UDP), Calico does make sense to gain a better performance.</p>
<h1 id="VXLAN_Technology">VXLAN Technology</h1><p>Virtual Extensible LAN (VXLAN) is a network virtualization technology that attempts to ameliorate the scalability problems associated with large cloud computing deployments. It uses a VLAN-like encapsulation technique to encapsulate MAC-based OSI layer 2 Ethernet frames within layer 4 UDP packets. <a href="https://en.wikipedia.org/wiki/Open_vSwitch" target="_blank" rel="external">Open vSwitch</a> is a former implementation of VXLAN, but Docker Engine implements a built-in VXLAN driver in libnetwork.</p>
<p>For more VXLAN details, you can see its <a href="https://datatracker.ietf.org/doc/rfc7348/" target="_blank" rel="external">official RFC</a> and a <a href="https://www.emulex.com/artifacts/d658610a-d3b6-457c-bf2d-bf8d476c6a98/elx_wp_all_VXLAN.pdf" target="_blank" rel="external">white paper</a> from EMulex. I’d like to post another blog to have more detailed discussion on VXLAN Technology.</p>
<h1 id="References">References</h1><p>[1] Docker Multi-host Networking Post: <a href="http://blog.docker.com/2015/11/docker-multi-host-networking-ga/" target="_blank" rel="external">http://blog.docker.com/2015/11/docker-multi-host-networking-ga/</a><br>[2] Docker Network Docs: <a href="http://docs.docker.com/engine/userguide/networking/dockernetworks/" target="_blank" rel="external">http://docs.docker.com/engine/userguide/networking/dockernetworks/</a><br>[3] Get Started Overlay Network for Docker: <a href="https://docs.docker.com/engine/userguide/networking/get-started-overlay/" target="_blank" rel="external">https://docs.docker.com/engine/userguide/networking/get-started-overlay/</a><br>[4] Docker v1.9 Announcemount: <a href="https://blog.docker.com/2015/11/docker-1-9-production-ready-swarm-multi-host-networking/" target="_blank" rel="external">https://blog.docker.com/2015/11/docker-1-9-production-ready-swarm-multi-host-networking/</a><br>[5] VXLAN Official RFC: <a href="https://datatracker.ietf.org/doc/rfc7348/" target="_blank" rel="external">https://datatracker.ietf.org/doc/rfc7348/</a><br>[6] VXLAN White Paper: <a href="https://www.emulex.com/artifacts/d658610a-d3b6-457c-bf2d-bf8d476c6a98/elx_wp_all_VXLAN.pdf" target="_blank" rel="external">https://www.emulex.com/artifacts/d658610a-d3b6-457c-bf2d-bf8d476c6a98/elx_wp_all_VXLAN.pdf</a></p>
]]></content>
    <summary type="html">
    <![CDATA[<p><a href="http://docker.io/" target="_blank" rel="external">Docker</a> has released its newest version v1.9 (<a href="https://blog.docker.]]>
    </summary>
    
      <category term="Docker" scheme="http://chunqi.li/tags/Docker/"/>
    
      <category term="Multi-host Network" scheme="http://chunqi.li/tags/Multi-host-Network/"/>
    
      <category term="VXLAN" scheme="http://chunqi.li/tags/VXLAN/"/>
    
      <category term="Docker Network" scheme="http://chunqi.li/categories/Docker-Network/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[Calico: A Solution of Multi-host Network For Docker]]></title>
    <link href="http://chunqi.li/2015/11/06/calico-docker/"/>
    <id>http://chunqi.li/2015/11/06/calico-docker/</id>
    <published>2015-11-06T06:06:56.000Z</published>
    <updated>2015-11-09T06:50:43.000Z</updated>
    <content type="html"><![CDATA[<p><a href="https://github.com/projectcalico/calico" target="_blank" rel="external">Calico</a> is a pure 3-layer protocol to support multi-host network communication for OpenStacks VMs and Docker containers. Calico does not use overlay network such as <a href="https://github.com/coreos/flannel" target="_blank" rel="external">falnnel</a> and <a href="https://github.com/docker/libnetwork/blob/master/docs/overlay.md" target="_blank" rel="external">libnetwork overlay driver</a>, it is a pure Layer 3 approach with a vRouter implementation instead of a vSwitcher. Each vRouter propagates workload reachability information (routes) to the rest of the data center using BGP protocol.</p>
<p>This post focus on how to setup a multi-host networking for Docker containers with <a href="https://github.com/projectcalico/calico-docker" target="_blank" rel="external">calico-docker</a> and some advanced features.</p>
<h1 id="Environment">Environment</h1><h2 id="Environment_Prerequisite">Environment Prerequisite</h2><ul>
<li>Two linux nodes (node1 and node2) with Ubuntu Linux distribution, either VM or physical machine is OK.</li>
<li>Install docker on both nodes.</li>
<li>Etcd cluster.</li>
</ul>
<h2 id="Configuration_&amp;_Download">Configuration &amp; Download</h2><p>Setup two linux nodes with IP 192.168.236.130/131 and connect them physically or virtually, confirm that they can ping each other succesfully. Setup docker bridge (default is docker0) on two nodes. Let’s set two docker bridges with different network. Netowrk configuration details are as follows:</p>
<p>Node1</p>
<ul>
<li>IP: 192.168.236.130</li>
<li>Docker bridge network: 192.168.1.0/24</li>
</ul>
<p>Node2</p>
<ul>
<li>IP: 192.168.236.131</li>
<li>Docker bridge network: 172.17.0.0/16</li>
</ul>
<p>Install Docker, should be no error here.</p>
<figure class="highlight bash"><figcaption><span>Install Docker</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">sudo apt-get install docker.io</span><br><span class="line">docker ps</span><br></pre></td></tr></table></figure>
<p>Download and run etcd, replace {node} with node0/1 seperately. We need at least two etcd node since the new version of etcd cannot run on single node.</p>
<figure class="highlight bash"><figcaption><span>Download and run etcd</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">curl -L  https://github.com/coreos/etcd/releases/download/v2.<span class="number">2.1</span>/etcd-v2.<span class="number">2.1</span>-linux-amd64.tar.gz -o etcd-v2.<span class="number">2.1</span>-linux-amd64.tar.gz</span><br><span class="line">tar xzvf etcd-v2.<span class="number">2.1</span>-linux-amd64.tar.gz</span><br><span class="line"><span class="built_in">cd</span> etcd-v2.<span class="number">2.1</span>-linux-amd64</span><br><span class="line">./etcd -name &#123;node&#125; -initial-advertise-peer-urls http://<span class="number">0.0</span>.<span class="number">0.0</span>:<span class="number">2380</span> \</span><br><span class="line">  -listen-peer-urls http://<span class="number">0.0</span>.<span class="number">0.0</span>:<span class="number">2380</span> \</span><br><span class="line">  -listen-client-urls http://<span class="number">0.0</span>.<span class="number">0.0</span>:<span class="number">2379</span>,http://<span class="number">127.0</span>.<span class="number">0.1</span>:<span class="number">4001</span> \</span><br><span class="line">  -advertise-client-urls http://<span class="number">0.0</span>.<span class="number">0.0</span>:<span class="number">2379</span> \</span><br><span class="line">  -initial-cluster-token etcd-cluster \</span><br><span class="line">  -initial-cluster node1=http://<span class="number">192.168</span>.<span class="number">236.130</span>:<span class="number">2380</span>,node2=http://<span class="number">192.168</span>.<span class="number">236.131</span>:<span class="number">2380</span> \</span><br><span class="line">  -initial-cluster-state new</span><br></pre></td></tr></table></figure>
<p>Download calicoctl<br><figure class="highlight bash"><figcaption><span>Download calicoctl</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">wget https://github.com/projectcalico/calico-docker/releases/download/v0.<span class="number">10.0</span>/calicoctl</span><br></pre></td></tr></table></figure></p>
<h1 id="Start_Calico_Services">Start Calico Services</h1><p>Calico services in Docker environment are running as a Docker container using host network configuration. All containers configured with Calico services with use calico-node to communicate with each other and Internet.</p>
<p>Run the following commands on node1/2 to start calico-node</p>
<figure class="highlight bash"><figcaption><span>Run calico-node</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo calicoctl node --ip=&#123;host_ip&#125;</span><br></pre></td></tr></table></figure>
<p>You should see output like this on each node</p>
<pre><code>calico@node1:~<span class="comment"># docker ps</span>
CONTAINER ID        IMAGE                COMMAND             CREATED             STATUS              PORTS               NAMES
<span class="number">40</span>b177803c97        calico/<span class="keyword">node</span><span class="identifier"></span><span class="title">:v0</span>.<span class="number">9.0</span>   <span class="string">"/sbin/my_init"</span>     <span class="number">27</span> seconds ago      Up <span class="number">27</span> seconds                           calico-<span class="keyword">node</span><span class="identifier"></span><span class="title"></span>
</code></pre><p>Before starting any containers, we need to configure an IP pool with the <code>ipip</code> and <code>nat-outgoing</code> options. Thus containers with an valid profile could have access to Internet. Run the following command on either node.</p>
<figure class="highlight bash"><figcaption><span>Configure IP pool</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">calicoctl pool add <span class="number">192.168</span>.<span class="number">100.0</span>/<span class="number">24</span> --ipip --nat-outgoing</span><br></pre></td></tr></table></figure>
<h1 id="Container_Networking_Configuration">Container Networking Configuration</h1><h2 id="Start_Containers">Start Containers</h2><p>Firstly run a few containers on each host.</p>
<p>On node1:<br><figure class="highlight bash"><figcaption><span>Run container on node1</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">docker run --net=none --name worker-<span class="number">1</span> -tid ubuntu</span><br><span class="line">docker run --net=none --name worker-<span class="number">2</span> -tid ubuntu</span><br></pre></td></tr></table></figure></p>
<p>On node2:<br><figure class="highlight bash"><figcaption><span>Run container on node2</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker run --net=none --name worker-<span class="number">3</span> -tid ubuntu</span><br></pre></td></tr></table></figure></p>
<h2 id="Configure_Calico_Networking">Configure Calico Networking</h2><p>Now that all the containers are running without any network devices. Use Calico to assign network devices to these containers. Notice that IPs assigned to containers should be in the range of IP pools.</p>
<p>On node1:<br><figure class="highlight bash"><figcaption><span>Configure network on node1</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">sudo calicoctl container add worker-<span class="number">1</span> <span class="number">192.168</span>.<span class="number">100.1</span></span><br><span class="line">sudo calicoctl container add worker-<span class="number">2</span> <span class="number">192.168</span>.<span class="number">100.2</span></span><br></pre></td></tr></table></figure></p>
<p>On node2:<br><figure class="highlight bash"><figcaption><span>Configure network on node2</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo calicoctl container add worker-<span class="number">3</span> <span class="number">192.168</span>.<span class="number">100.3</span></span><br></pre></td></tr></table></figure></p>
<p>Once containers have Calico networking, they gain a network device with corresponding IP address. At this point them have access neither to each other nor to Internet since no profiles are created and assigned to them.</p>
<p>Create some profiles on either node:<br><figure class="highlight bash"><figcaption><span>Create profiles</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">calicoctl profile add PROF_1</span><br><span class="line">calicoctl profile add PROF_2</span><br></pre></td></tr></table></figure></p>
<p>Then assign profiles to containers. Containers in same profile have access to each other. And containers in the IP poll created before won’t have access to Internet until added to a profile.</p>
<p>On node1:<br><figure class="highlight bash"><figcaption><span>Assign profiles to containers on node1</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">calicoctl container worker-<span class="number">1</span> profile append PROF_1</span><br><span class="line">calicoctl container worker-<span class="number">2</span> profile append PROF_2</span><br></pre></td></tr></table></figure></p>
<p>On node2:<br><figure class="highlight bash"><figcaption><span>Assign profiles to containers on node2</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">calicoctl container worker-<span class="number">3</span> profile append PROF_1</span><br></pre></td></tr></table></figure></p>
<p>Until now all configurations are done and we will test network connections of these containers afterwards.</p>
<h1 id="Testing">Testing</h1><p>Now check the connectivities of each containers. At this point every containers should have access to Internet, try and ping google.com:<br><figure class="highlight bash"><figcaption><span>Check Internet access</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">docker <span class="built_in">exec</span> worker-<span class="number">1</span> ping -c <span class="number">4</span> www.google.com</span><br><span class="line">docker <span class="built_in">exec</span> worker-<span class="number">2</span> ping -c <span class="number">4</span> www.google.com</span><br></pre></td></tr></table></figure></p>
<p>Then check connections of containers in same profile:<br><figure class="highlight bash"><figcaption><span>Check inner profile access</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker <span class="built_in">exec</span> worker-<span class="number">1</span> ping -c <span class="number">4</span> <span class="number">192.168</span>.<span class="number">100.3</span></span><br></pre></td></tr></table></figure></p>
<p>And containers not in same profile cannot ping each other:<br><figure class="highlight bash"><figcaption><span>Check access outer profile</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker <span class="built_in">exec</span> worker-<span class="number">1</span> ping -c <span class="number">4</span> <span class="number">192.168</span>.<span class="number">100.2</span></span><br></pre></td></tr></table></figure></p>
<p>If we add worker-2 into profile PROF_1, then worker-2 could ping worker-1 and worker-3.<br>On node1:<br><figure class="highlight bash"><figcaption><span>Advanced check</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">calicoctl container worker-<span class="number">2</span> profile append PROF_1</span><br><span class="line">docker <span class="built_in">exec</span> worker-<span class="number">2</span> ping -c <span class="number">4</span> <span class="number">192.168</span>.<span class="number">100.1</span></span><br><span class="line">docker <span class="built_in">exec</span> worker-<span class="number">2</span> ping -c <span class="number">4</span> <span class="number">192.168</span>.<span class="number">100.3</span></span><br></pre></td></tr></table></figure></p>
<h1 id="Performance_Tests">Performance Tests</h1><h2 id="Simple_Test">Simple Test</h2><p>I perform a simple performance test using <code>iperf</code> to evaluate the network between two Calico containers. Run <code>iperf -s</code> on worker-1 and <code>iperf -c 192.168.100.1</code> on worker-3. We can get the result:</p>
<pre><code>root@<span class="number">39f</span>db1701da4:~<span class="preprocessor"># ./iperf -c <span class="number">192.168</span><span class="number">.101</span><span class="number">.2</span></span>
------------------------------------------------------------
Client connecting to <span class="number">192.168</span><span class="number">.101</span><span class="number">.2</span>, TCP port <span class="number">5001</span>
TCP window size: <span class="number">85.0</span> KByte (<span class="keyword">default</span>)
------------------------------------------------------------
[  <span class="number">3</span>] local <span class="number">192.168</span><span class="number">.101</span><span class="number">.1</span> port <span class="number">39187</span> connected with <span class="number">192.168</span><span class="number">.101</span><span class="number">.2</span> port <span class="number">5001</span>
[ ID] Interval       Transfer     Bandwidth
[  <span class="number">3</span>]  <span class="number">0.0</span>-<span class="number">10.0</span> sec  <span class="number">1.08</span> GBytes   <span class="number">927</span> Mbits/sec
</code></pre><p>Then run the same test on native host (node1 and node2):</p>
<pre><code>calico@node2:~<span class="preprocessor"># iperf -c <span class="number">192.168</span><span class="number">.236</span><span class="number">.130</span></span>
------------------------------------------------------------
Client connecting to <span class="number">192.168</span><span class="number">.236</span><span class="number">.130</span>, TCP port <span class="number">5001</span>
TCP window size: <span class="number">85.0</span> KByte (<span class="keyword">default</span>)
------------------------------------------------------------
[  <span class="number">3</span>] local <span class="number">192.168</span><span class="number">.236</span><span class="number">.131</span> port <span class="number">54584</span> connected with <span class="number">192.168</span><span class="number">.236</span><span class="number">.130</span> port <span class="number">5001</span>
[ ID] Interval       Transfer     Bandwidth
[  <span class="number">3</span>]  <span class="number">0.0</span>-<span class="number">10.0</span> sec  <span class="number">2.57</span> GBytes  <span class="number">2.21</span> Gbits/sec
</code></pre><p>From the result we can see there’s a great gap between Calico network and native network. But according to the official documents and evaluations, calico network should be similar to the native network. <strong>WHY???</strong></p>
<h2 id="Dive_Deeper">Dive Deeper</h2><p>To find out the reason of slow network, firstly I test the network performance between workker-1 and worker-2, which are in the same host. The result is as follows:</p>
<pre><code>root@<span class="number">51</span>b78d9e6153:/<span class="preprocessor"># iperf -c <span class="number">192.168</span><span class="number">.100</span><span class="number">.2</span></span>
------------------------------------------------------------
Client connecting to <span class="number">192.168</span><span class="number">.100</span><span class="number">.3</span>, TCP port <span class="number">5001</span>
TCP window size: <span class="number">85.0</span> KByte (<span class="keyword">default</span>)
------------------------------------------------------------
[  <span class="number">3</span>] local <span class="number">192.168</span><span class="number">.100</span><span class="number">.2</span> port <span class="number">36476</span> connected with <span class="number">192.168</span><span class="number">.100</span><span class="number">.3</span> port <span class="number">5001</span>
[ ID] Interval       Transfer     Bandwidth
[  <span class="number">3</span>]  <span class="number">0.0</span>-<span class="number">10.0</span> sec  <span class="number">47.3</span> GBytes  <span class="number">40.6</span> Gbits/sec
</code></pre><p>Since speed of my net card is only 1Gbits/sec, it seems that containers on the same host connects each other directly without going through any network device. That really make all sense.</p>
<p>Then I dived deep into the documents and configurations of Calico and found such configuration of IP pool:<br><figure class="highlight bash"><figcaption><span>Configure IP pool</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">calicoctl pool add <span class="number">192.168</span>.<span class="number">100.0</span>/<span class="number">24</span> --ipip --nat-outgoing</span><br></pre></td></tr></table></figure></p>
<p>We use <code>--ipip</code> option when creating IP pool, which means <code>Use IP-over-IP encapsulation across hosts</code>. This option will enforce another layer of IP-over-IP encapsulation when packages traveling across hosts. Since our hosts node1 and node2 are in the same network (192.168.236.0/24), we could avoid this option and the speed should increase as supposed.</p>
<p>Run the following command on either node to override the previous IP pool configuration.<br><figure class="highlight bash"><figcaption><span>Configure IP pool</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">calicoctl pool add <span class="number">192.168</span>.<span class="number">100.0</span>/<span class="number">24</span> --nat-outgoing</span><br><span class="line">calicoctl pool show</span><br></pre></td></tr></table></figure></p>
<p>Then test networking between worker-1 and worker-3 again:</p>
<pre><code>root@<span class="number">39f</span>db1701da4:~<span class="preprocessor"># ./iperf -c <span class="number">192.168</span><span class="number">.101</span><span class="number">.2</span></span>
------------------------------------------------------------
Client connecting to <span class="number">192.168</span><span class="number">.101</span><span class="number">.2</span>, TCP port <span class="number">5001</span>
TCP window size: <span class="number">85.0</span> KByte (<span class="keyword">default</span>)
------------------------------------------------------------
[  <span class="number">3</span>] local <span class="number">192.168</span><span class="number">.101</span><span class="number">.1</span> port <span class="number">39187</span> connected with <span class="number">192.168</span><span class="number">.101</span><span class="number">.2</span> port <span class="number">5001</span>
[ ID] Interval       Transfer     Bandwidth
[  <span class="number">3</span>]  <span class="number">0.0</span>-<span class="number">10.0</span> sec  <span class="number">2.74</span> GBytes  <span class="number">2.35</span> Gbits/sec
</code></pre><p>Hurray!!! That’s the native speed!</p>
<h1 id="References">References</h1><p>[1] Project Calico: <a href="https://github.com/projectcalico/calico" target="_blank" rel="external">https://github.com/projectcalico/calico</a><br>[2] Calico Docker: <a href="https://github.com/projectcalico/calico-docker" target="_blank" rel="external">https://github.com/projectcalico/calico-docker</a><br>[3] Demenstration on calico-docker: <a href="https://github.com/projectcalico/calico-docker" target="_blank" rel="external">https://github.com/projectcalico/calico-docker</a><br>[4] Calico-docker in Yixin: <a href="http://mp.weixin.qq.com/s?__biz=MzAwMDU1MTE1OQ==&amp;mid=400983139&amp;idx=1&amp;sn=f033e3dca32ca9f0b7c9779528523e7e&amp;scene=1&amp;srcid=1101jklWCo9jNFjdnUum85PG&amp;from=singlemessage&amp;isappinstalled=0#wechat_redirect" target="_blank" rel="external">Paper URL</a></p>
]]></content>
    <summary type="html">
    <![CDATA[<p><a href="https://github.com/projectcalico/calico" target="_blank" rel="external">Calico</a> is a pure 3-layer protocol to support multi-h]]>
    </summary>
    
      <category term="Calico" scheme="http://chunqi.li/tags/Calico/"/>
    
      <category term="Docker" scheme="http://chunqi.li/tags/Docker/"/>
    
      <category term="Multi-host Network" scheme="http://chunqi.li/tags/Multi-host-Network/"/>
    
      <category term="Docker Network" scheme="http://chunqi.li/categories/Docker-Network/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[Hello World]]></title>
    <link href="http://chunqi.li/2015/11/05/hello-world/"/>
    <id>http://chunqi.li/2015/11/05/hello-world/</id>
    <published>2015-11-05T06:06:56.000Z</published>
    <updated>2015-11-09T01:06:52.000Z</updated>
    <content type="html"><![CDATA[<p>This is a Hello World page of Arthur Chunqi Li’s blog.</p>
<figure class="highlight c"><figcaption><span>hello_world.c</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    <span class="built_in">printf</span>(<span class="string">"Hello World!\n"</span>);</span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h2 id="Links:">Links:</h2><ul>
<li>Homepage: <a href="http://chunqi.li/me">http://chunqi.li/me</a></li>
<li>Blog: <a href="http://chunqi.li/">http://chunqi.li/</a> (<a href="http://xelatex.github.io" target="_blank" rel="external">backup</a>)</li>
<li>Facebook: <a href="http://facebook.chunqi.li" target="_blank" rel="external">http://facebook.chunqi.li</a></li>
<li>LinkedIn: <a href="http://linkedin.chunqi.li" target="_blank" rel="external">http://linkedin.chunqi.li</a></li>
</ul>
]]></content>
    <summary type="html">
    <![CDATA[<p>This is a Hello World page of Arthur Chunqi Li’s blog.</p>
<figure class="highlight c"><figcaption><span>hello_world.c</span></figcaption]]>
    </summary>
    
      <category term="Hello World" scheme="http://chunqi.li/tags/Hello-World/"/>
    
      <category term="Hello World" scheme="http://chunqi.li/categories/Hello-World/"/>
    
  </entry>
  
</feed>
