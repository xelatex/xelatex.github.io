<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title><![CDATA[Arthur Chunqi Li's Blog]]></title>
  
  <link href="/atom.xml" rel="self"/>
  <link href="http://chunqi.li/"/>
  <updated>2015-11-16T04:02:03.000Z</updated>
  <id>http://chunqi.li/</id>
  
  <author>
    <name><![CDATA[Arthur Chunqi Li]]></name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title><![CDATA[Battlefield: Calico, Flannel, Weave and Docker Overlay Network]]></title>
    <link href="http://chunqi.li/2015/11/15/Battlefield-Calico-Flannel-Weave-and-Docker-Overlay-Network/"/>
    <id>http://chunqi.li/2015/11/15/Battlefield-Calico-Flannel-Weave-and-Docker-Overlay-Network/</id>
    <published>2015-11-15T06:10:17.000Z</published>
    <updated>2015-11-16T04:02:03.000Z</updated>
    <content type="html"><![CDATA[<p>From the previous posts, I have analysed 4 different Docker multi-host network solutions - Calico, Flannel, Weave and Docker Overlay Network. You can see more details on how to install, config and tune features of them from previous posts:</p>
<ul>
<li><a href="/2015/09/06/calico-docker/">Calico: A Solution of Multi-host Network For Docker</a></li>
<li><a href="/2015/10/10/Flannel-for-Docker-Overlay-Network/">Flannel for Docker Overlay Network</a></li>
<li><a href="/2015/11/14/Weave-Network-Management-for-Docker/">Weave: Network Management for Docker</a></li>
<li><a href="/2015/11/09/docker-multi-host-networking/">Docker Multi-host Overlay Networking with Etcd</a></li>
</ul>
<p>This post provides a battlefiled for these 4 Docker multi-host network solutions, including features and performances.</p>
<p><strong>If you want to see the results directly, directly jump to the <a href="/2015/11/15/Battlefield-Calico-Flannel-Weave-and-Docker-Overlay-Network/#Conclusion">Conclusion</a> chapter.</strong></p>
<h1 id="Docker_Multi-host_Networking_Introduction">Docker Multi-host Networking Introduction</h1><p>Docker kicked out with a simple single-host networking from the very beginning. Unfortunately, this prevents Docker clusters from scale out to multiple hosts. A number of projects put their focus on this problem such as Calico, Flannel and Weave, and also since Nov. 2015, Docker support the Multi-host Overlay Networking itself.</p>
<p>What these projects have in common is trying to control the container’s networking configurations, thus to capture and inject network packets. Consequently, every containers located on different hosts can get IPs in the same subnet and communicate with each other as if they are connected to the same L2 switch. In this way, containers could spread out on multiple hosts, even on multiple data centers.</p>
<p>While there are also a lot of differences between them from technical models, network topology and features. This post will mainly focus on the differences between Calico, Flannel, Weave and Docker Overlay Network, and you could choose the right solution which fits best to your requirements.</p>
<h1 id="Battlefield_Overview">Battlefield Overview</h1><p>According the features these Big Four support, I will compare them in the following aspects:</p>
<ul>
<li><strong>Network Model</strong> - What kind of network model are used to support multi-host network.</li>
<li><strong>Application Isolation</strong> - Support what level and kind of application isolation of containers.</li>
<li><strong>Name Service</strong> - DNS lookup with simple hostname or DNS rules.</li>
<li><strong>Distributed Storage Requirements</strong> - Whether an external distributed storage is required, e.g. etcd or consul.</li>
<li><strong>Encryption Channel</strong> - Whether data and infomation tranvers can put in an encryption channel.</li>
<li><strong>Partially Connected Network Support</strong> - Whether the system can run on a partially connected host network.</li>
<li><strong>Seperate vNIC for Container</strong> - Whether a seperate NIC is generated for container.</li>
<li><strong>IP Overlap Support</strong> - Whether the same IP can be allocated to different containers.</li>
<li><strong>Container Subnet Restriction</strong> - Whether container’s subnet should not be the same as host’s.</li>
<li><strong>Protocol Support</strong> - What kind of Layer-3 or Layer-4 protocols are supported.</li>
</ul>
<p>Now let’s see more details of these aspects on Calico, Flannel, Weave and Docker Overlay Network.</p>
<h1 id="Network_Model">Network Model</h1><p>Multi-host networking means aggregating containers on different hosts to a same virtual network, and also these networking providers (Calico, etc.) are organized as a clustering network, too. The cluster organizations are called network model in this post. Technically, these four solutions uses different network model to organize their own network topology.</p>
<p><strong>Calico</strong> implements a pure Layer 3 approach to achieve a simpler, higher scaling, better performance and more efficient multi-host networking. So Calico can not be treated as an <code>overlay network</code>. The pure Layer 3 approach avoids the packet encapsulation associated with the Layer 2 solution which simplifies diagnostics, reduces transport overhead and improves performance. Calico also implements BGP protocl for routing combined with a pure IP network, thus allows Internetl scaling for virtual networks.</p>
<p><strong>Flannel</strong> has two different network model to choose. One is called UDP backend, which is a simple IP-over-IP solutions which uses a TUN device to encapsulate every IP fragment in a UDP packet, thus forming an overlay network; the other is a VxLAN backend, which is same as Docker Overlay Network. I have run a simple test for these two models, VxLAN is much more faster than UDP backend. The reason, I suggest, is that VxLAN is well supported by Linux Kernel, while UDP backend implements a pure software-layer encapsulation. Flannel requires a Etcd cluster to store the network configuration, allocate subnets and auxiliary data (such as host’s IP). And the packet routing also requires the cooperation of Etcd cluster. Besides, Flannel runs a seperate process <code>flanneld</code> on host environment to support packet switching. Apart from Docker, flannel can also used for traditional VMs.</p>
<p><strong>Weave</strong> also has two different connection modes. One is called <code>sleeve</code>, which implements a UDP channel to tranverse IP packets from containers. The main differences between Weave sleeve mode and Flannel UDP backend mode is that, Weave will merge multiple container’s packet to one packet and transfer via UDP channel, so technically Weave sleeve mode will be a bit faster than Flannel UDP backend mode in most cases. The other connection mode of Weave is called <code>fastdp</code> mode, which also implements a VxLAN solutions. Though there’s no official documents clarifying the VxLAN usage, we still can found the usage of VxLAN from Weave codes. Weave runs a Docker container performing the same role as <code>flanneld</code>.</p>
<p><strong>Docker Overlay Network</strong> implements a VxLAN-based solution with the help of <code>libnetwork</code> and <code>libkv</code>, and, of course, is integrated into Docker succesfully without any seperate process or containers.</p>
<p>So a brief conclusion of network model is in the following table:</p>
<table>
<thead>
<tr>
<th></th>
<th>Calico</th>
<th>Flannel</th>
<th>Weave</th>
<th>Docker Overlay Network</th>
</tr>
</thead>
<tbody>
<tr>
<td>Network Model</td>
<td>Pure Layer-3 Solution</td>
<td>VxLAN or UDP Channel</td>
<td>VxLAN or UDP Channel</td>
<td>VxLAN</td>
</tr>
</tbody>
</table>
<h1 id="Application_Isolation">Application Isolation</h1><p>Since containers are connected to each other, we need a method to put containers into different groups and isolate containers in different group.</p>
<p><strong>Flannel</strong>, <strong>Weave</strong> and <strong>Docker Overlay Network</strong> uses the same application isolation schema - the traditional CIDR isolation. The traditional CIDR isolation uses netmask to identify different subnet, and machines in different subnet cannot talk to each other. For example, w1/w2/w3 has IP 192.168.0.2/24 192.168.0.3/24 and 192.168.1.2/24 seperately. w1 and w2 can talk to each other since they are in the same subnet 192.168.0.0/24, but w3 cannot talk to w1 and w2.</p>
<p><strong>Calico</strong> implements another type of application isolation schema - profile. You can create a batch of profiles and append containers with Calico network into different profiles. Only containers in the same profile could talk to each other. Containers in differen profile cannot access to each other even though they are in the same CIDR subnet.</p>
<p><strong>Brief conclusion:</strong></p>
<table>
<thead>
<tr>
<th></th>
<th>Calico</th>
<th>Flannel</th>
<th>Weave</th>
<th>Docker Overlay Network</th>
</tr>
</thead>
<tbody>
<tr>
<td>Application Isolation</td>
<td>Profile Schema</td>
<td>CIDR Schema</td>
<td>CIDR Schema</td>
<td>CIDR Schema</td>
</tr>
</tbody>
</table>
<h1 id="Protocol_Support">Protocol Support</h1><p>Since <strong>Calico</strong> is a pure Layer-3 solution, not all Layer-3 or Layer-4 protocols are supported. From the official github forum, developers of Calico declaims only <strong>TCP</strong>, <strong>UDP</strong>, <strong>ICMP</strong> ad <strong>ICMPv6</strong> are supported by Calico. It does make sense that supporting other protocols are a bit harder in such a Layer-3 solution.</p>
<p>Other solutions support all protocols. It’s easy for them to achieve so because either udp encapsulation or VxLAN can support encapsulate L2 packets over L3. So it doesn’t matter what kind of protocol the packet holds.</p>
<p><strong>Brief conclusion:</strong></p>
<table>
<thead>
<tr>
<th></th>
<th>Calico</th>
<th>Flannel</th>
<th>Weave</th>
<th>Docker Overlay Network</th>
</tr>
</thead>
<tbody>
<tr>
<td>Protocol Support</td>
<td>TCP, UDP, ICMP &amp; ICMPv6</td>
<td>ALL</td>
<td>ALL</td>
<td>ALL</td>
</tr>
</tbody>
</table>
<h1 id="Name_Service">Name Service</h1><p><strong>Weave</strong> supports a name service between containers. When you create a container, Weave will put it into a DNS name service with format {hostname}.weave.local. Thus you can access to any container with {hostname}.weave.local or simply use {hostname}. The suffix (weave.local) can be changed to other strings, and the DNS lookup service can also be turned off.</p>
<p>The others don’t have such feature.</p>
<p><strong>Brief conclusion:</strong></p>
<table>
<thead>
<tr>
<th></th>
<th>Calico</th>
<th>Flannel</th>
<th>Weave</th>
<th>Docker Overlay Network</th>
</tr>
</thead>
<tbody>
<tr>
<td>Name Service</td>
<td>No</td>
<td>No</td>
<td>Yes</td>
<td>No</td>
</tr>
</tbody>
</table>
<h1 id="Distributed_Storage_Requirements">Distributed Storage Requirements</h1><p>As to <strong>Calico</strong>, <strong>Flannel</strong> and <strong>Docker Overlay Network</strong>, a distributed storage such as Etcd and Consul is a requirement to change routing and host information. Docker Overlay Network can also cooperate with Docker Swarm’s discovery services to build a cluster.</p>
<p><strong>Weave</strong>, however, doesn’t need a distributed storage because Weave itself has a node discovery service using Rumor Protocol. This design decouples with another distributed storage system while introduces complexity and consistency concern of IP allocations, as well as the IPAM performance when cluster grows larger.</p>
<p><strong>Brief conclusion:</strong></p>
<table>
<thead>
<tr>
<th></th>
<th>Calico</th>
<th>Flannel</th>
<th>Weave</th>
<th>Docker Overlay Network</th>
</tr>
</thead>
<tbody>
<tr>
<td>Distributed Storage Requirements</td>
<td>Yes</td>
<td>Yes</td>
<td>No</td>
<td>Yes</td>
</tr>
</tbody>
</table>
<h1 id="Encryption_Channel">Encryption Channel</h1><p><strong>Flannel</strong> supports TLS encryption channel between Flannel and Etcd, as well as data path between Flannel peers. You can see more details on <code>flanneld --help</code> with <code>-etcd-certfile</code> and <code>-remote-certfile</code> parameters.</p>
<p><strong>Weave</strong> can be configured to encrypt both control data passing over TCP connections and the payloads of UDP packets sent between peers. This is accomplished with the <a href="http://nacl.cr.yp.to/" target="_blank" rel="external">NaCl</a> crypto libraries employing Curve25519, XSalsa20 and Poly1305 to encrypt and authenticate messages. Weave protects against injection and replay attacks for traffic forwarded between peers.</p>
<p><strong>Calico</strong> and <strong>Docker Overlay Network</strong> doesn’t support any kinds of encryption method, neither Calico-Etcd channel nor data path between Calico peers. But Calico achieves best performance among these four solutions, so it’s better fit for an internal environment or if you don’t care about data safety.</p>
<p><strong>Brief conclusion:</strong></p>
<table>
<thead>
<tr>
<th></th>
<th>Calico</th>
<th>Flannel</th>
<th>Weave</th>
<th>Docker Overlay Network</th>
</tr>
</thead>
<tbody>
<tr>
<td>Encryption Channel</td>
<td>No</td>
<td>TLS</td>
<td>NaCl Library</td>
<td>No</td>
</tr>
</tbody>
</table>
<h1 id="Partially_Connected_Network_Support">Partially Connected Network Support</h1><p><strong>Weave</strong> can be deployed in a partially connected network, a brief example is as follows:</p>
<img src="/images/wave-topology.png">
<p>There are four peers with peer 1~3 connect with each other and peer 4 only connects to peer3. Weave can be deployed on peer 1~4. Any traffic from containers on peer 1 to containers on peer 4 will be traversed via peer 3.</p>
<p>This feature allows Weave connects hosts aparted by a firewall, thus connects hosts with internal IP address in different data centers.</p>
<p>Others don’t have such feature.</p>
<p><strong>Brief conclusion:</strong></p>
<table>
<thead>
<tr>
<th></th>
<th>Calico</th>
<th>Flannel</th>
<th>Weave</th>
<th>Docker Overlay Network</th>
</tr>
</thead>
<tbody>
<tr>
<td>Partially Connected Network Support</td>
<td>No</td>
<td>No</td>
<td>Yes</td>
<td>No</td>
</tr>
</tbody>
</table>
<h1 id="Seperate_vNIC_for_Container">Seperate vNIC for Container</h1><p>Since <strong>Flannel</strong>, <strong>Weave</strong> and <strong>Docker Overlay Network</strong> create a bridged device and a veth inner containers, they create a seperate vNIC for containers. Routing table of container is also changed, thus bypass all packets of clustered network to this newly created NIC. Other connections, such as google.com, will route to the original vNIC.</p>
<p><strong>Calico</strong> can use a unified vNIC for container because it’s a pure Layer-3 solution. Calico can configure NAT for out-going requests and forward subnet packages to other Calico peers. Calico can also use Docker bridged NIC for out-going requests with some manual configuration inner containers. In this way, you need to add <code>-cap-add=Net_Admin</code> parameter when execute <code>docker run</code>.</p>
<p><strong>Brief conclusion:</strong></p>
<table>
<thead>
<tr>
<th></th>
<th>Calico</th>
<th>Flannel</th>
<th>Weave</th>
<th>Docker Overlay Network</th>
</tr>
</thead>
<tbody>
<tr>
<td>Seperate vNIC for Container</td>
<td>No</td>
<td>Yes</td>
<td>Yes</td>
<td>yes</td>
</tr>
</tbody>
</table>
<h1 id="IP_Overlap_Support">IP Overlap Support</h1><p>Technically, for VxLAN-based solutions, tenant networks can have overlapping internal IP address, though IP addresses assigned to hosts must be unique. According to VxLAN speculations, <strong>Weave</strong>, <strong>Flannel</strong> and <strong>Docker Overlay Network</strong> can support IP overlap for containers. But on my testing environment, I cannot configure any of these three support IP overlap. So I can only say they have <strong>potential</strong> to support IP overlap.</p>
<p><strong>Calico</strong> cannot support IP overlap technically, but Calico official documents emphasize that they can put overlapping IPv4 containers’ packets on IPv6 network. Although this is an alternative solution for IPv4 network, I prefer to treate Calico not support IP overlap.</p>
<p><strong>Brief conclusion:</strong></p>
<table>
<thead>
<tr>
<th></th>
<th>Calico</th>
<th>Flannel</th>
<th>Weave</th>
<th>Docker Overlay Network</th>
</tr>
</thead>
<tbody>
<tr>
<td>IP Overlap Support</td>
<td>No</td>
<td>Maybe</td>
<td>Maybe</td>
<td>Maybe</td>
</tr>
</tbody>
</table>
<h1 id="Container_Subnet_Restriction">Container Subnet Restriction</h1><p>This section focus on whether container subnet can overlap with host network.</p>
<p><strong>Flannel</strong> creates a real bridged network on the host with the subnet address, and use host Linux routing table to forward container packages to this bridge device. So container’s subnet of Flannel cannot be overlap with host network, or host’s routing table will be confused.</p>
<p><strong>Calico</strong> is a pure Layer-3 implementation and packets from container to outter world will tranverse NAT table. So Calico also has such restriction that container subnet cannot overlap with host network.</p>
<p><strong>Weave</strong> doesn’t use host routing table to differentiate packages from containers, but use the <code>pcap</code> feature to deliver packages to the right place. So Weave doesn’t need to obey the subnet restriction and it’s free to allocate container a same IP address as host. Besides you can also change IP configurations inner container and the container could be reached by the new IP.</p>
<p><strong>Docker Overlay Network</strong> allows container and host in the same subnet and achieve the isolation between them. But Docker Overlay Network rely on etcd to record routing information, so changing container’s IP address manually will mess the routing process can lead container beyond reach.</p>
<p><strong>Brief conclusion:</strong></p>
<table>
<thead>
<tr>
<th></th>
<th>Calico</th>
<th>Flannel</th>
<th>Weave</th>
<th>Docker Overlay Network</th>
</tr>
</thead>
<tbody>
<tr>
<td>Container Subnet Restriction</td>
<td>No</td>
<td>No</td>
<td>Yes, configurable after start</td>
<td>Yes, not configurable after start</td>
</tr>
</tbody>
</table>
<h1 id="Conclusion">Conclusion</h1><p>So let’s give a final conclusion of all the aspects into one table. This table is one of the best references for you to choose a right multi-host networking solution.</p>
<table>
<thead>
<tr>
<th></th>
<th>Calico</th>
<th>Flannel</th>
<th>Weave</th>
<th>Docker Overlay Network</th>
</tr>
</thead>
<tbody>
<tr>
<td>Network Model</td>
<td>Pure Layer-3 Solution</td>
<td>VxLAN or UDP Channel</td>
<td>VxLAN or UDP Channel</td>
<td>VxLAN</td>
</tr>
<tr>
<td>Application Isolation</td>
<td>Profile Schema</td>
<td>CIDR Schema</td>
<td>CIDR Schema</td>
<td>CIDR Schema</td>
</tr>
<tr>
<td>Protocol Support</td>
<td>TCP, UDP, ICMP &amp; ICMPv6</td>
<td>ALL</td>
<td>ALL</td>
<td>ALL</td>
</tr>
<tr>
<td>Name Service</td>
<td>No</td>
<td>No</td>
<td>Yes</td>
<td>No</td>
</tr>
<tr>
<td>Distributed Storage Requirements</td>
<td>Yes</td>
<td>Yes</td>
<td>No</td>
<td>Yes</td>
</tr>
<tr>
<td>Encryption Channel</td>
<td>No</td>
<td>TLS</td>
<td>NaCl Library</td>
<td>No</td>
</tr>
<tr>
<td>Partially Connected Network Support</td>
<td>No</td>
<td>No</td>
<td>Yes</td>
<td>No</td>
</tr>
<tr>
<td>Seperate vNIC for Container</td>
<td>No</td>
<td>Yes</td>
<td>Yes</td>
<td>yes</td>
</tr>
<tr>
<td>IP Overlap Support</td>
<td>No</td>
<td>Maybe</td>
<td>Maybe</td>
<td>Maybe</td>
</tr>
<tr>
<td>Container Subnet Restriction</td>
<td>No</td>
<td>No</td>
<td>Yes, configurable after start</td>
<td>Yes, not configurable after start</td>
</tr>
</tbody>
</table>
<p>My future plan is to test the performance of these four multi-host network solutions. Since there are too many contents on this post, I will create a new post to show the details of performance test.</p>
]]></content>
    <summary type="html">
    <![CDATA[<p>From the previous posts, I have analysed 4 different Docker multi-host network solutions - Calico, Flannel, Weave and Docker Overlay Netw]]>
    </summary>
    
      <category term="Calico" scheme="http://chunqi.li/tags/Calico/"/>
    
      <category term="Docker" scheme="http://chunqi.li/tags/Docker/"/>
    
      <category term="Flannel" scheme="http://chunqi.li/tags/Flannel/"/>
    
      <category term="Multi-host Network" scheme="http://chunqi.li/tags/Multi-host-Network/"/>
    
      <category term="Overlay Network" scheme="http://chunqi.li/tags/Overlay-Network/"/>
    
      <category term="Weave" scheme="http://chunqi.li/tags/Weave/"/>
    
      <category term="Docker Multi-host Network" scheme="http://chunqi.li/categories/Docker-Multi-host-Network/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[Weave: Network Management for Docker]]></title>
    <link href="http://chunqi.li/2015/11/14/Weave-Network-Management-for-Docker/"/>
    <id>http://chunqi.li/2015/11/14/Weave-Network-Management-for-Docker/</id>
    <published>2015-11-14T11:38:28.000Z</published>
    <updated>2015-11-15T05:46:07.000Z</updated>
    <content type="html"><![CDATA[<p><a href="https://github.com/weaveworks/weave" target="_blank" rel="external">Weave</a> is developed by <a href="http://weave.works/" target="_blank" rel="external">Weaveworks</a> for developers to control and observe Docker containers network. Similar to <a href="/2015/10/10/Flannel-for-Docker-Overlay-Network/">Flannel</a>, <a href="/2015/09/06/calico-docker/">Calico</a> and <a href="/2015/11/09/docker-multi-host-networking/">Docker Overlay Network</a>, Weave handles Docker multi-host networking and management which can merge network of Docker’s laying on different hosts. Compared with the rest three solutions, weave provides more features and choices. I will write another blog to compare these four solutions in details. In this blog, I will focus on Weave’s install, features and technology inside.</p>
<h1 id="About_Weave">About Weave</h1><p>Weave creates a virtual network that connects Dockers deployed across multiple hosts as well as their DNS discovery. Dockers on different hosts can communicate with each other just the same as they are in the same LAN, and broadcast is also well supported in such LAN network. Besides Dockers can discover each other by hostname implemented by Weave DNS discovery module, which is not supported by other multi-host network solutions.</p>
<p>Weave can also tranverse the firewall and operate in partially connected networks. Packets will tranvers via a shortest path to the destination host contains Docker, even though the host hides behind a firewall and the sender host cannot access destination host directly. Traffic can also be encrypted, allowing hosts connect each other via untrusted network.</p>
<p>Weave cooperates with Docker current single host or overlay network also, so there would be a seperate NIC for weave in Docker, as well as a weave virtual NIC on the host to capture all the packets send from Dockers.</p>
<h1 id="Installation_and_Configuration">Installation and Configuration</h1><h2 id="Prerequsites">Prerequsites</h2><p>Two or more hosts (VM or PM) are need to setup a Docker cluster via weave. Here I use two Ubuntu 15.10 VM located on VMs running on my Mac. Let’s name these two hosts node1 and node2 with IP 10.156.75.101 and 10.156.75.102 seperately. Please ensure you are running Linux (Kernel 3.8 or later) and have Docker (version 1.3.1 or later) installed. <code>curl</code> or any alternative software (e.g. wget) is also necessary to download weave binary file.</p>
<h2 id="Installation_and_Run_Weave_Cluster">Installation and Run Weave Cluster</h2><p>Then run such commands to finish weave installation:</p>
<figure class="highlight bash"><figcaption><span>Install weave</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">sudo curl -L git.io/weave -o /usr/<span class="built_in">local</span>/bin/weave</span><br><span class="line">sudo chmod a+x /usr/<span class="built_in">local</span>/bin/weave</span><br></pre></td></tr></table></figure>
<p>Thus weave is installed succesfully. It’s so easy, right? The most important part for weave is not the binary itself. When weave starts, two Dockers <code>weaveworks/weaveexec</code> and <code>weaveworks/weave</code> will run to handle all the network configurations and network discovery service.</p>
<p>Run on node1 to start weave service:</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">root@node1:~<span class="comment"># weave launch</span></span><br><span class="line">root@node1:~<span class="comment"># docker ps</span></span><br><span class="line">CONTAINER ID        IMAGE                        COMMAND                  CREATED             STATUS              PORTS               NAMES</span><br><span class="line"><span class="number">81799</span>b4eff2e        weaveworks/weaveexec:<span class="number">1.2</span>.<span class="number">1</span>   <span class="string">"/home/weave/weavepro"</span>   <span class="number">28</span> seconds ago      Up <span class="number">28</span> seconds                           weaveproxy</span><br><span class="line"><span class="number">676</span>b4d58ead4        weaveworks/weave:<span class="number">1.2</span>.<span class="number">1</span>       <span class="string">"/home/weave/weaver -"</span>   <span class="number">29</span> seconds ago      Up <span class="number">29</span> seconds                           weave</span><br></pre></td></tr></table></figure>
<p>You can see two weave Dockers here. Then on node2, launch weave with it’s partener node1 (10.156.75.102):</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">root@node2:~<span class="comment"># weave launch 10.156.75.101</span></span><br></pre></td></tr></table></figure>
<p>To confirm that weave cluster starts sucessfully, run following command on node1:</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">root@node1:~<span class="comment"># weave status connections</span></span><br><span class="line">&lt;- <span class="number">10.156</span>.<span class="number">75.102</span>:<span class="number">32854</span>   established fastdp <span class="number">66</span>:b4:a1:<span class="number">85</span>:da:<span class="number">65</span>(node2)</span><br></pre></td></tr></table></figure>
<p>Now you sucessfully setup a weave connection between node1 and node2.</p>
<h2 id="Run_Docker_and_Test_Network">Run Docker and Test Network</h2><p>After weave cluster started, you could run Docker on node1 and node2 </p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">root@node1:~<span class="comment"># weave run -itd --name=w1 ubuntu</span></span><br><span class="line">root@node2:~<span class="comment"># weave run -itd --name=w2 ubuntu</span></span><br></pre></td></tr></table></figure>
<p>Then these two Dockers can communicate with each other. Test with a simple <code>ping</code>:</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">root@node1:~<span class="comment"># docker exec w1 ping -c4 w2</span></span><br></pre></td></tr></table></figure>
<h1 id="Simple_Speed_Test">Simple Speed Test</h1><p>After setting up Weave network, I use <code>perf</code> to perform a simple performance test between Dockers on same/different hosts and compare them with native network performance.</p>
<p>Here is the native performance between two hosts:</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">root@node1:~<span class="comment"># iperf -c node2</span></span><br><span class="line">------------------------------------------------------------</span><br><span class="line">Client connecting to node2, TCP port <span class="number">5001</span></span><br><span class="line">TCP window size: <span class="number">85.0</span> KByte (default)</span><br><span class="line">------------------------------------------------------------</span><br><span class="line">[  <span class="number">3</span>] <span class="built_in">local</span> <span class="number">10.156</span>.<span class="number">75.101</span> port <span class="number">50534</span> connected with <span class="number">10.156</span>.<span class="number">75.102</span> port <span class="number">5001</span></span><br><span class="line">[ ID] Interval       Transfer     Bandwidth</span><br><span class="line">[  <span class="number">3</span>]  <span class="number">0.0</span>-<span class="number">10.0</span> sec  <span class="number">2.57</span> GBytes  <span class="number">2.21</span> Gbits/sec</span><br></pre></td></tr></table></figure>
<p>And the performance between Dockers on different hosts:</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">root@w3:/<span class="comment"># iperf -c w1</span></span><br><span class="line">------------------------------------------------------------</span><br><span class="line">Client connecting to w1, TCP port <span class="number">5001</span></span><br><span class="line">TCP window size: <span class="number">76.5</span> KByte (default)</span><br><span class="line">------------------------------------------------------------</span><br><span class="line">[  <span class="number">3</span>] <span class="built_in">local</span> <span class="number">10.2</span>.<span class="number">1.65</span> port <span class="number">43966</span> connected with <span class="number">10.2</span>.<span class="number">1.2</span> port <span class="number">5001</span></span><br><span class="line">[ ID] Interval       Transfer     Bandwidth</span><br><span class="line">[  <span class="number">3</span>]  <span class="number">0.0</span>-<span class="number">10.0</span> sec  <span class="number">1.87</span> GBytes  <span class="number">1.61</span> Gbits/sec</span><br></pre></td></tr></table></figure>
<p>The performance between Dockers on the same host:</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">root@a1:/<span class="comment"># iperf -c w1</span></span><br><span class="line">------------------------------------------------------------</span><br><span class="line">Client connecting to w1, TCP port <span class="number">5001</span></span><br><span class="line">TCP window size: <span class="number">45.0</span> KByte (default)</span><br><span class="line">------------------------------------------------------------</span><br><span class="line">[  <span class="number">3</span>] <span class="built_in">local</span> <span class="number">10.2</span>.<span class="number">1.1</span> port <span class="number">33750</span> connected with <span class="number">10.2</span>.<span class="number">1.2</span> port <span class="number">5001</span></span><br><span class="line">[ ID] Interval       Transfer     Bandwidth</span><br><span class="line">[  <span class="number">3</span>]  <span class="number">0.0</span>-<span class="number">10.2</span> sec  <span class="number">54.0</span> GBytes  <span class="number">46.3</span> Gbits/sec</span><br></pre></td></tr></table></figure>
<p>You can see network between Dockers on the same host is quite faster, the reason is Weave use <code>pcap</code> to identify whether the packet’s destination is located on the same host or not. Thus for the communications of Dockers on the same host, Weave could directly forward the packets to the right destination.</p>
<p>This is only a simple performance test. I will perform a detailed test in the following blog with the comparison of Weave, Calico, Flannel, Docker Overlay Netowrk.</p>
<h1 id="Dive_Deep_into_Weave">Dive Deep into Weave</h1><h2 id="Weave_Network_Topology">Weave Network Topology</h2><p>The main difference between Weave and other Docker multi-host network solutions is that Weave network uses a number <code>peers</code> to perform as the routers residing on different hosts. These routers build a network of these hosts and sends or routes packets to the right destination. Each peer has a human friendly nickname and a unique identifier which is different on its each run.</p>
<p>Weave routers establish TCP connections to each other to perform starting handshakes and topology exchange on the runtime. Peers also establish UDP tunnels to carry encapsulate network packets. These packets can tranverse firewall with the help of other routers.</p>
<p>Weave creates a network bridge on each host, and each container is connected to this bridge. After you start a Docker with Weave network, you could find the created bridge via <code>ifconfig</code>.</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">root@node1:~<span class="comment"># ifconfig</span></span><br><span class="line">...</span><br><span class="line">weave     Link encap:Ethernet  HWaddr <span class="number">4</span>a:<span class="number">15</span>:<span class="number">49</span>:<span class="number">23</span>:bf:<span class="number">9</span>c</span><br><span class="line">          inet6 addr: fe80::<span class="number">4815</span>:<span class="number">49</span>ff:fe23:bf9c/<span class="number">64</span> Scope:Link</span><br><span class="line">          UP BROADCAST RUNNING MULTICAST  MTU:<span class="number">1410</span>  Metric:<span class="number">1</span></span><br><span class="line">          RX packets:<span class="number">726</span> errors:<span class="number">0</span> dropped:<span class="number">0</span> overruns:<span class="number">0</span> frame:<span class="number">0</span></span><br><span class="line">          TX packets:<span class="number">8</span> errors:<span class="number">0</span> dropped:<span class="number">0</span> overruns:<span class="number">0</span> carrier:<span class="number">0</span></span><br><span class="line">          collisions:<span class="number">0</span> txqueuelen:<span class="number">0</span></span><br><span class="line">          RX bytes:<span class="number">33956</span> (<span class="number">33.9</span> KB)  TX bytes:<span class="number">648</span> (<span class="number">648.0</span> B)</span><br><span class="line">...</span><br></pre></td></tr></table></figure>
<p>This bridge performs the packets forwarding to and from the Dockers. Besides, Dockers connected to this bridge also creates a veth NIC. The container side veth is given an IP address and netmask by Weave’s IPAM module. The Weave router captures Ethernet packets from the bridged interface using <code>pcap</code> feature. This typically bypass packets tranversing between local containers, which will gain a better local containers networking performance. For packets between different hosts, Weave router will choose a best routing and send the packet to the next hop.</p>
<h2 id="Partially_Connected_Network_Support">Partially Connected Network Support</h2><p>Differ from other solutions, Weave doesn’t rely on distributed storage (e.g. etcd and consul) to exchange routing information. Weave peers build a routing network themselves and implement rumour protocol to exchange networking topology when new peer adds and exits. Weave can also perform on a partially connected network and exchange packets with the help of other peers. Given a partially connected network as follows:</p>
<img src="/images/wave-topology.png">
<p>Peer 1/2/3 are connected to each other while peer 4 only connects to peer 3. If containers on peer 1 want to talk to containers on peer 4, the packets will first be send to peer 3 and then to peer 4. The connections between two directly connected host could achieve a <code>fastdp</code> connection and the indirect connections can only use <code>sleeve</code> connection. These two different connections have a huge gap in speed. I run a simple test with <code>iperf</code> on three containers, <code>w1</code> &amp; <code>w2</code> and <code>w1</code> &amp; <code>w3</code> locate on the directly connected hosts but <code>w2</code> &amp; <code>w3</code> locate on the indirectly connected hosts. From the indirectly connected host <code>node-pub-1</code>, you could run <code>weave status connections</code> to retrieve the connections:</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">root@node-pub-<span class="number">1</span>:~<span class="comment"># weave status connections</span></span><br><span class="line">-&gt; <span class="number">10.156</span>.<span class="number">75.102</span>:<span class="number">6783</span>    established sleeve <span class="number">66</span>:b4:a1:<span class="number">85</span>:da:<span class="number">65</span>(node2)</span><br><span class="line">-&gt; <span class="number">192.168</span>.<span class="number">70.201</span>:<span class="number">6783</span>   established fastdp <span class="number">4</span>a:<span class="number">15</span>:<span class="number">49</span>:<span class="number">23</span>:bf:<span class="number">9</span>c(node1)</span><br></pre></td></tr></table></figure>
<p>Then speed test results are as follows:</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">root@w3:/<span class="comment"># iperf -c w2</span></span><br><span class="line">------------------------------------------------------------</span><br><span class="line">Client connecting to w2, TCP port <span class="number">5001</span></span><br><span class="line">TCP window size: <span class="number">45.0</span> KByte (default)</span><br><span class="line">------------------------------------------------------------</span><br><span class="line">[  <span class="number">3</span>] <span class="built_in">local</span> <span class="number">10.2</span>.<span class="number">1.65</span> port <span class="number">54304</span> connected with <span class="number">10.2</span>.<span class="number">1.129</span> port <span class="number">5001</span></span><br><span class="line">[ ID] Interval       Transfer     Bandwidth</span><br><span class="line">[  <span class="number">3</span>]  <span class="number">0.0</span>-<span class="number">10.0</span> sec   <span class="number">146</span> MBytes   <span class="number">123</span> Mbits/sec</span><br><span class="line"></span><br><span class="line">root@w3:/<span class="comment"># iperf -c w1</span></span><br><span class="line">------------------------------------------------------------</span><br><span class="line">Client connecting to w1, TCP port <span class="number">5001</span></span><br><span class="line">TCP window size: <span class="number">76.5</span> KByte (default)</span><br><span class="line">------------------------------------------------------------</span><br><span class="line">[  <span class="number">3</span>] <span class="built_in">local</span> <span class="number">10.2</span>.<span class="number">1.65</span> port <span class="number">43966</span> connected with <span class="number">10.2</span>.<span class="number">1.2</span> port <span class="number">5001</span></span><br><span class="line">[ ID] Interval       Transfer     Bandwidth</span><br><span class="line">[  <span class="number">3</span>]  <span class="number">0.0</span>-<span class="number">10.0</span> sec  <span class="number">1.77</span> GBytes  <span class="number">1.52</span> Gbits/sec</span><br></pre></td></tr></table></figure>
<p>We could see the directly connected w1 and w3 achieve a quite high performance of 1.52 Gbits/sec, which indirectly connected w2 and w3 only get about 10% bandwidth. This could be a bottlenet for Weave developers to overcome.</p>
<h2 id="Cooperate_with_Docker_Control_API">Cooperate with Docker Control API</h2><p>Weave provides a Docker API proxy to control weave docker in the same way of control Docker instead of using <code>weave run</code>. This allows you using the ordinary Docker <a href="https://docs.docker.com/reference/commandline/cli/" target="_blank" rel="external">command-line interface</a> or <a href="https://docs.docker.com/reference/api/docker_remote_api/" target="_blank" rel="external">remote API</a> to CRUD Dockers with Weave network.</p>
<p>In the previous chapters, we use <code>weave launch</code> to run Weave directly, and we could see Weave-related Dockers created on the host:</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">root@node1:~<span class="comment"># docker ps</span></span><br><span class="line">CONTAINER ID        IMAGE                        COMMAND                  CREATED             STATUS              PORTS               NAMES</span><br><span class="line">ccb596b5a13a        weaveworks/weaveexec:<span class="number">1.2</span>.<span class="number">1</span>   <span class="string">"/home/weave/weavepro"</span>   <span class="number">16</span> seconds ago      Up <span class="number">16</span> seconds       weaveproxy</span><br><span class="line"><span class="number">790</span>cc66660fd        weaveworks/weave:<span class="number">1.2</span>.<span class="number">1</span>       <span class="string">"/home/weave/weaver -"</span>   <span class="number">15</span> hours ago        Up <span class="number">15</span> hours         weave</span><br></pre></td></tr></table></figure>
<p>For these two Weave services, <code>weave</code> perform the main functions for Weave network, such as network configuration and DNS lookup. <code>weaveproxy</code> performs the role of a proxy between Docker client (command line or API) and the Docker daemon, intercepting the communication between these two components.</p>
<p>Actually, <code>weave launch</code> performs <code>weave launch-router</code> and <code>weave launch-proxy</code> in a batch, you could run <code>weave launch-router</code> and <code>weave launch-proxy</code> seperately with different parameters. For example, if you want to control Weave via a TCP port instead of a unix file socket, you just need to add <code>-H</code> parameter to <code>weave launch-proxy</code>. You can run <code>weave stop-proxy</code> if you already use <code>weave launch</code> to launch both <code>router</code> and <code>proxy</code>.</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">root@node1:~<span class="comment"># weave stop-proxy</span></span><br><span class="line">root@node1:~<span class="comment"># weave launch-proxy -H tcp://0.0.0.0:9999</span></span><br><span class="line">root@node1:~<span class="comment"># weave env</span></span><br><span class="line"><span class="built_in">export</span> DOCKER_HOST=tcp://<span class="number">127.0</span>.<span class="number">0.1</span>:<span class="number">9999</span> ORIG_DOCKER_HOST=</span><br></pre></td></tr></table></figure>
<p>From <code>weave env</code>, you can see the current intercepted DOCKER_HOST from Weave is <code>tcp://127.0.0.1:9999</code>, you can use <code>docker -H 127.0.0.1:9999 &lt;command&gt;</code> to control Docker with Weave network support.</p>
<p>You can also use following commands to add <code>tcp://127.0.0.1:9999</code> to <code>DOCKER_HOST</code> env params, thus you could use <code>docker</code> directly without assigning the API address.</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">root@node1:~<span class="comment"># eval $(weave env)</span></span><br></pre></td></tr></table></figure>
<p>For more details about weave proxy, you can see the <a href="http://docs.weave.works/weave/latest_release/proxy.html" target="_blank" rel="external">official weave proxy documentation page</a>.</p>
<h2 id="IP_Allocation_Strategy_&amp;_Application_Isolation">IP Allocation Strategy &amp; Application Isolation</h2><p>Some more parameters can be set when launching weave to make IP allocation more flexible, thus could achieve application isolation via the CIDR network isolation speculations. From <code>weave help</code> you can see more detailed parameters for weave launch. The bad things is that there’s no more details on these params than listing them directly. But from the name of these params, you could guess what they are figuring out:</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">root@node1:~<span class="comment"># weave help</span></span><br><span class="line">Usage:</span><br><span class="line">...</span><br><span class="line">weave launch        [--password &lt;password&gt;] [--nickname &lt;nickname&gt;]</span><br><span class="line">                      [--ipalloc-range &lt;cidr&gt; [--ipalloc-default-subnet &lt;cidr&gt;]]</span><br><span class="line">                      [--no-discovery] [--init-peer-count &lt;count&gt;] &lt;peer&gt; ...</span><br><span class="line">...</span><br></pre></td></tr></table></figure>
<p>For these params:</p>
<ul>
<li><code>--password</code> : password for weave cluster, newer weave node must use this password to join</li>
<li><code>--nickname</code> : alias of weave node instead of its hostname</li>
<li><code>--ipalloc-range</code> : IP range allocated for Docker</li>
<li><code>--ipalloc-default-subnet</code> : default subnet allocated for Docker, you can use <code>-e WEAVE_CIDR=net:${CIDR}</code> when running a docker with other IP allocation method. See next chapter for more details.</li>
<li><code>--no-discovery</code> : don’t use DNS discovery service</li>
<li><code>--init-peer-count &lt;count&gt;</code> : start service after <code>&lt;count&gt;</code> peers connect to the cluster</li>
</ul>
<p>So if you want more flexible IP allocation methods, run the following commands on node1 and node2:</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">root@node1:~<span class="comment"># weave launch --ipalloc-range 10.2.0.0/16 --ipalloc-default-subnet 10.2.1.0/24</span></span><br><span class="line">root@node1:~<span class="comment"># eval $(weave env)</span></span><br></pre></td></tr></table></figure>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">root@node2:~<span class="comment"># weave launch --ipalloc-range 10.2.0.0/16 --ipalloc-default-subnet 10.2.1.0/24 $node1</span></span><br><span class="line">root@node2:~<span class="comment"># eval $(weave env)</span></span><br></pre></td></tr></table></figure>
<p>This delegates the entire 10.2.0.0/16 subnet to weave, and instructs it to allocate from 10.2.1.0/24 within that if no specific subnet is specified. Now we can launch some containers in the default subnet:</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">root@node1:~<span class="comment"># docker run --name a1 -ti ubuntu</span></span><br><span class="line">root@node2:~<span class="comment"># docker run --name a2 -ti ubuntu</span></span><br></pre></td></tr></table></figure>
<p>And some more containers in a different subnet:</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">root@node1:~<span class="comment"># docker run -e WEAVE_CIDR=net:10.2.2.0/24 --name b1 -ti ubuntu</span></span><br><span class="line">root@node2:~<span class="comment"># docker run -e WEAVE_CIDR=net:10.2.2.0.24 --name b2 -ti ubuntu</span></span><br></pre></td></tr></table></figure>
<p>A quick <code>ping</code> test could illustrates network connections betwwen a1~a2 and b1~b2:</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">root@node1:~<span class="comment"># docker exec a1 ping -c 4 a2</span></span><br><span class="line">root@node1:~<span class="comment"># docker exec b1 ping -c 4 b2</span></span><br></pre></td></tr></table></figure>
<p>While no connections between a1~b2 or b1~a2:</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">root@node1:~<span class="comment"># docker exec a1 ping -c 4 b2</span></span><br><span class="line">root@node1:~<span class="comment"># docker exec b1 ping -c 4 a2</span></span><br></pre></td></tr></table></figure>
<h1 id="Conclusion">Conclusion</h1><p>Weave is a good networking management tools for Docker and provides the most functions compared with other solutions. You could find more feature details on its <a href="http://docs.weave.works/weave/latest_release/features.html" target="_blank" rel="external">official feature document</a>.</p>
<h1 id="References">References</h1><p>[1] Weaveworks homepage, <a href="http://weave.works/" target="_blank" rel="external">http://weave.works/</a><br>[2] Weave GitHub homepage, <a href="https://github.com/weaveworks/weave" target="_blank" rel="external">https://github.com/weaveworks/weave</a><br>[3] Weave features, <a href="http://docs.weave.works/weave/latest_release/features.html" target="_blank" rel="external">http://docs.weave.works/weave/latest_release/features.html</a><br>[4] Weave proxy reference, <a href="http://docs.weave.works/weave/latest_release/proxy.html" target="_blank" rel="external">http://docs.weave.works/weave/latest_release/proxy.html</a></p>
]]></content>
    <summary type="html">
    <![CDATA[<p><a href="https://github.com/weaveworks/weave" target="_blank" rel="external">Weave</a> is developed by <a href="http://weave.works/" targ]]>
    </summary>
    
      <category term="Docker" scheme="http://chunqi.li/tags/Docker/"/>
    
      <category term="Multi-host Network" scheme="http://chunqi.li/tags/Multi-host-Network/"/>
    
      <category term="Weave" scheme="http://chunqi.li/tags/Weave/"/>
    
      <category term="Docker Multi-host Network" scheme="http://chunqi.li/categories/Docker-Multi-host-Network/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[Docker Multi-host Overlay Networking with Etcd]]></title>
    <link href="http://chunqi.li/2015/11/09/docker-multi-host-networking/"/>
    <id>http://chunqi.li/2015/11/09/docker-multi-host-networking/</id>
    <published>2015-11-09T06:27:55.000Z</published>
    <updated>2015-11-15T06:19:48.000Z</updated>
    <content type="html"><![CDATA[<p><a href="http://docker.io/" target="_blank" rel="external">Docker</a> has released its newest version v1.9 (<a href="https://blog.docker.com/2015/11/docker-1-9-production-ready-swarm-multi-host-networking/" target="_blank" rel="external">see details</a>) on November 3, 2015. This big release put Swarm and multi-host networking into production-ready status. This blog illustrates the configuration and a few evaluations of Docker multi-host overlay networking.</p>
<h1 id="Multi-host_Networking">Multi-host Networking</h1><p><a href="https://blog.docker.com/2015/06/networking-receives-an-upgrade/" target="_blank" rel="external">Multi-host Networking was announced as part of experimental release in June, 2015</a>, and turns to stable release of Docker Engine this month. There are already several Multi-host networking solutions for docker, such as <a href="/2015/09/06/calico-docker/">Calico</a> and <a href="/2015/10/10/Flannel-for-Docker-Overlay-Network/">Flannel</a>. Docker multi-host networking uses VXLAN-based solution with the help of <code>libnetwork</code> and <code>libkv</code> library. So the <code>overlay</code> network requires a valid key-value store service to exchange informations between different docker engines. Docker implements a built-in <a href="https://datatracker.ietf.org/doc/rfc7348/" target="_blank" rel="external">VXLAN-based overlay network driver</a> in <code>libnetwork</code> library to support a wide range virtual network between multiple hosts.</p>
<h1 id="Prerequisite">Prerequisite</h1><h2 id="Environment_Preparation">Environment Preparation</h2><p>Before using Docker overlay networking, check the version of docker with <code>docker -v</code> to confirm that docker version is no less than v1.9. In this blog I prepare an environment with two Linux nodes (node1/node2) with IP 192.168.236.130/131 and connect them physically or virtually, and confirm they have network access to each other.</p>
<p>ownload and run etcd, replace {node} with node0/1 seperately. We need at least two etcd node since the new version of etcd cannot run on single node.</p>
<figure class="highlight bash"><figcaption><span>Download and run etcd</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">curl -L  https://github.com/coreos/etcd/releases/download/v2.<span class="number">2.1</span>/etcd-v2.<span class="number">2.1</span>-linux-amd64.tar.gz -o etcd-v2.<span class="number">2.1</span>-linux-amd64.tar.gz</span><br><span class="line">tar xzvf etcd-v2.<span class="number">2.1</span>-linux-amd64.tar.gz</span><br><span class="line"><span class="built_in">cd</span> etcd-v2.<span class="number">2.1</span>-linux-amd64</span><br><span class="line">./etcd -name &#123;node&#125; -initial-advertise-peer-urls http://<span class="number">0.0</span>.<span class="number">0.0</span>:<span class="number">2380</span> \</span><br><span class="line">  -listen-peer-urls http://<span class="number">0.0</span>.<span class="number">0.0</span>:<span class="number">2380</span> \</span><br><span class="line">  -listen-client-urls http://<span class="number">0.0</span>.<span class="number">0.0</span>:<span class="number">2379</span>,http://<span class="number">127.0</span>.<span class="number">0.1</span>:<span class="number">4001</span> \</span><br><span class="line">  -advertise-client-urls http://<span class="number">0.0</span>.<span class="number">0.0</span>:<span class="number">2379</span> \</span><br><span class="line">  -initial-cluster-token etcd-cluster \</span><br><span class="line">  -initial-cluster node1=http://<span class="number">192.168</span>.<span class="number">236.130</span>:<span class="number">2380</span>,node2=http://<span class="number">192.168</span>.<span class="number">236.131</span>:<span class="number">2380</span> \</span><br><span class="line">  -initial-cluster-state new</span><br></pre></td></tr></table></figure>
<h2 id="Start_Docker_Daemon_With_Cluster_Parameters">Start Docker Daemon With Cluster Parameters</h2><p>Docker Engine daemon should be started with cluster parameters <code>--cluster-store</code> and <code>--cluster-advertise</code>, thus all Docker Engine running on different nodes could communicate and cooperate with each other. Here we need to set <code>--cluster-store</code> with Etcd service host and port and <code>--cluster-advertise</code> with IP and Docker Daemon port on this node. Stop current docker daemon and start with new params.</p>
<p>On node1:<br><figure class="highlight bash"><figcaption><span>Run Docker daemon with cluster params</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">sudo service docker stop</span><br><span class="line">sudo /usr/bin/docker daemon -H tcp://<span class="number">0.0</span>.<span class="number">0.0</span>:<span class="number">2375</span> -H unix:///var/run/docker.sock --cluster-store=etcd://<span class="number">192.168</span>.<span class="number">236.130</span>:<span class="number">2379</span> --cluster-advertise=<span class="number">192.168</span>.<span class="number">236.130</span>:<span class="number">2375</span></span><br></pre></td></tr></table></figure></p>
<p>On node2:<br><figure class="highlight bash"><figcaption><span>Run Docker daemon with cluster params</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">sudo service docker stop</span><br><span class="line">sudo /usr/bin/docker daemon -H tcp://<span class="number">0.0</span>.<span class="number">0.0</span>:<span class="number">2375</span> -H unix:///var/run/docker.sock --cluster-store=etcd://<span class="number">192.168</span>.<span class="number">236.131</span>:<span class="number">2379</span> --cluster-advertise=<span class="number">192.168</span>.<span class="number">236.131</span>:<span class="number">2375</span></span><br></pre></td></tr></table></figure></p>
<p>All preparations are done until now.</p>
<h1 id="Create_Overlay_Network">Create Overlay Network</h1><p>On either node, we can execute <code>docker network ls</code> to see the network configuration of Docker. Here’s the example of node1:<br><figure class="highlight bash"><figcaption><span>Docker network configuration</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">docker@node1:~<span class="comment"># sudo docker network ls</span></span><br><span class="line">NETWORK ID          NAME                DRIVER</span><br><span class="line"><span class="number">80</span>a36a28041f        bridge              bridge</span><br><span class="line"><span class="number">6</span>b7eab031544        none                null</span><br><span class="line"><span class="number">464</span>fe03753fb        host                host</span><br></pre></td></tr></table></figure></p>
<p>Then we also use <code>docker network</code> command to create a new overlay network.<br><figure class="highlight bash"><figcaption><span>Docker network configuration</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">docker@node1:~<span class="comment"># sudo docker network create -d overlay myapp</span></span><br><span class="line"><span class="number">904</span>f9dc335b0f91fe155b26829287c7de7c17af5cfeb9c386a1ccf75c42<span class="built_in">cd</span>3eb</span><br></pre></td></tr></table></figure></p>
<p>Wait for a minute and we can see the output of this command is the ID of this overlay network. Then execute <code>docker network ls</code> on either node:<br><figure class="highlight bash"><figcaption><span>Docker network configuration</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">docker@node1:~<span class="comment"># sudo docker network ls</span></span><br><span class="line">NETWORK ID          NAME                DRIVER</span><br><span class="line"><span class="number">904</span>f9dc335b0        myapp               overlay</span><br><span class="line"><span class="number">80</span>a36a28041f        bridge              bridge</span><br><span class="line"><span class="number">6</span>b7eab031544        none                null</span><br><span class="line"><span class="number">464</span>fe03753fb        host                host</span><br><span class="line"><span class="number">52</span>e9119e18d5        docker_gwbridge     bridge</span><br></pre></td></tr></table></figure></p>
<p>On both node1 and node2, two network <code>myapp</code> and <code>docker_gwbridge</code> are added with type <code>overlay</code> and <code>bridge</code> seperately. Thus <code>myapp</code> represents the overlay network associated with <code>eth0</code> in containers, and <code>docker_gwbridge</code> represents the bridge network connecting Internet associated with <code>eth1</code> in containers.</p>
<h1 id="Create_Containers_With_Overlay_Network">Create Containers With Overlay Network</h1><p>On node1:<br><figure class="highlight bash"><figcaption><span>Docker network configuration</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker@node1:~<span class="comment"># sudo docker run -itd --name=worker-1 --net=myapp ubuntu</span></span><br></pre></td></tr></table></figure></p>
<p>And on node2:<br><figure class="highlight bash"><figcaption><span>Docker network configuration</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker@node1:~<span class="comment"># sudo docker run -itd --name=worker-2 --net=myapp ubuntu</span></span><br></pre></td></tr></table></figure></p>
<p>Then test the connection between two containers. On node1, execute:<br><figure class="highlight bash"><figcaption><span>Docker networks</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line">docker@node1:~/etcd-v2.<span class="number">0.9</span>-linux-amd64<span class="comment"># sudo docker exec worker-1 ifconfig</span></span><br><span class="line">eth0      Link encap:Ethernet  HWaddr <span class="number">02</span>:<span class="number">42</span>:<span class="number">0</span>a:<span class="number">00</span>:<span class="number">00</span>:<span class="number">02</span></span><br><span class="line">          inet addr:<span class="number">10.0</span>.<span class="number">0.2</span>  Bcast:<span class="number">0.0</span>.<span class="number">0.0</span>  Mask:<span class="number">255.255</span>.<span class="number">255.0</span></span><br><span class="line">          inet6 addr: fe80::<span class="number">42</span>:aff:fe00:<span class="number">2</span>/<span class="number">64</span> Scope:Link</span><br><span class="line">          UP BROADCAST RUNNING MULTICAST  MTU:<span class="number">1450</span>  Metric:<span class="number">1</span></span><br><span class="line">          RX packets:<span class="number">5475264</span> errors:<span class="number">0</span> dropped:<span class="number">0</span> overruns:<span class="number">0</span> frame:<span class="number">0</span></span><br><span class="line">          TX packets:<span class="number">846008</span> errors:<span class="number">0</span> dropped:<span class="number">0</span> overruns:<span class="number">0</span> carrier:<span class="number">0</span></span><br><span class="line">          collisions:<span class="number">0</span> txqueuelen:<span class="number">0</span></span><br><span class="line">          RX bytes:<span class="number">7999457912</span> (<span class="number">7.9</span> GB)  TX bytes:<span class="number">55842488</span> (<span class="number">55.8</span> MB)</span><br><span class="line"></span><br><span class="line">eth1      Link encap:Ethernet  HWaddr <span class="number">02</span>:<span class="number">42</span>:ac:<span class="number">12</span>:<span class="number">00</span>:<span class="number">02</span></span><br><span class="line">          inet addr:<span class="number">172.18</span>.<span class="number">0.2</span>  Bcast:<span class="number">0.0</span>.<span class="number">0.0</span>  Mask:<span class="number">255.255</span>.<span class="number">0.0</span></span><br><span class="line">          inet6 addr: fe80::<span class="number">42</span>:acff:fe12:<span class="number">2</span>/<span class="number">64</span> Scope:Link</span><br><span class="line">          UP BROADCAST RUNNING MULTICAST  MTU:<span class="number">1500</span>  Metric:<span class="number">1</span></span><br><span class="line">          RX packets:<span class="number">12452</span> errors:<span class="number">0</span> dropped:<span class="number">0</span> overruns:<span class="number">0</span> frame:<span class="number">0</span></span><br><span class="line">          TX packets:<span class="number">6883</span> errors:<span class="number">0</span> dropped:<span class="number">0</span> overruns:<span class="number">0</span> carrier:<span class="number">0</span></span><br><span class="line">          collisions:<span class="number">0</span> txqueuelen:<span class="number">0</span></span><br><span class="line">          RX bytes:<span class="number">22021017</span> (<span class="number">22.0</span> MB)  TX bytes:<span class="number">376719</span> (<span class="number">376.7</span> KB)</span><br><span class="line"></span><br><span class="line">lo        Link encap:Local Loopback</span><br><span class="line">          inet addr:<span class="number">127.0</span>.<span class="number">0.1</span>  Mask:<span class="number">255.0</span>.<span class="number">0.0</span></span><br><span class="line">          inet6 addr: ::<span class="number">1</span>/<span class="number">128</span> Scope:Host</span><br><span class="line">          UP LOOPBACK RUNNING  MTU:<span class="number">65536</span>  Metric:<span class="number">1</span></span><br><span class="line">          RX packets:<span class="number">0</span> errors:<span class="number">0</span> dropped:<span class="number">0</span> overruns:<span class="number">0</span> frame:<span class="number">0</span></span><br><span class="line">          TX packets:<span class="number">0</span> errors:<span class="number">0</span> dropped:<span class="number">0</span> overruns:<span class="number">0</span> carrier:<span class="number">0</span></span><br><span class="line">          collisions:<span class="number">0</span> txqueuelen:<span class="number">0</span></span><br><span class="line">          RX bytes:<span class="number">0</span> (<span class="number">0.0</span> B)  TX bytes:<span class="number">0</span> (<span class="number">0.0</span> B)</span><br></pre></td></tr></table></figure></p>
<p>Here we can see two NICs in container with IP 10.0.0.2 and 172.18.0.2. <code>eth0</code> connects to the overlay network and <code>eth1</code> connects to docker_gwbridge. Thus the container will both have access to containers on other host as well as Google. Run the same command on node2 and we can see the IP of <code>eth0</code> in worker-2 is 10.0.0.3, which is assigned continuously.</p>
<p>Then test the connections between worker-1 and worker-2, execute command on node1:<br><figure class="highlight bash"><figcaption><span>Docker network configuration</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">docker@node1:~<span class="comment"># sudo docker exec worker-1 ping -c 4 10.0.0.3</span></span><br><span class="line">PING <span class="number">10.0</span>.<span class="number">0.3</span> (<span class="number">10.0</span>.<span class="number">0.3</span>) <span class="number">56</span>(<span class="number">84</span>) bytes of data.</span><br><span class="line"><span class="number">64</span> bytes from <span class="number">10.0</span>.<span class="number">0.3</span>: icmp_seq=<span class="number">1</span> ttl=<span class="number">64</span> time=<span class="number">0.735</span> ms</span><br><span class="line"><span class="number">64</span> bytes from <span class="number">10.0</span>.<span class="number">0.3</span>: icmp_seq=<span class="number">2</span> ttl=<span class="number">64</span> time=<span class="number">0.581</span> ms</span><br><span class="line"><span class="number">64</span> bytes from <span class="number">10.0</span>.<span class="number">0.3</span>: icmp_seq=<span class="number">3</span> ttl=<span class="number">64</span> time=<span class="number">0.444</span> ms</span><br><span class="line"><span class="number">64</span> bytes from <span class="number">10.0</span>.<span class="number">0.3</span>: icmp_seq=<span class="number">4</span> ttl=<span class="number">64</span> time=<span class="number">0.447</span> ms</span><br><span class="line"></span><br><span class="line">--- <span class="number">10.0</span>.<span class="number">0.3</span> ping statistics ---</span><br><span class="line"><span class="number">4</span> packets transmitted, <span class="number">4</span> received, <span class="number">0</span>% packet loss, time <span class="number">3000</span>ms</span><br><span class="line">rtt min/avg/max/mdev = <span class="number">0.444</span>/<span class="number">0.551</span>/<span class="number">0.735</span>/<span class="number">0.122</span> ms</span><br></pre></td></tr></table></figure></p>
<h1 id="Performance_Tests">Performance Tests</h1><p>I did a simple performance test between two containers with <code>iperf</code>, and here is the result.</p>
<p>First I tested the native network performance between node1 and node2:</p>
<pre><code>docker@node2:~<span class="preprocessor"># iperf -c <span class="number">192.168</span><span class="number">.236</span><span class="number">.130</span></span>
------------------------------------------------------------
Client connecting to <span class="number">192.168</span><span class="number">.236</span><span class="number">.130</span>, TCP port <span class="number">5001</span>
TCP window size:  <span class="number">136</span> KByte (<span class="keyword">default</span>)
------------------------------------------------------------
[  <span class="number">3</span>] local <span class="number">192.168</span><span class="number">.236</span><span class="number">.131</span> port <span class="number">36910</span> connected with <span class="number">192.168</span><span class="number">.236</span><span class="number">.130</span> port <span class="number">5001</span>
[ ID] Interval       Transfer     Bandwidth
[  <span class="number">3</span>]  <span class="number">0.0</span>-<span class="number">10.0</span> sec  <span class="number">2.59</span> GBytes  <span class="number">2.22</span> Gbits/sec
</code></pre><p>Then network performance between worker-1 and worker-2:</p>
<pre><code>root@<span class="number">3f</span>8bc51fb458:~<span class="preprocessor"># iperf -c <span class="number">10.0</span><span class="number">.0</span><span class="number">.2</span></span>
------------------------------------------------------------
Client connecting to <span class="number">10.0</span><span class="number">.0</span><span class="number">.2</span>, TCP port <span class="number">5001</span>
TCP window size: <span class="number">81.0</span> KByte (<span class="keyword">default</span>)
------------------------------------------------------------
[  <span class="number">3</span>] local <span class="number">10.0</span><span class="number">.0</span><span class="number">.3</span> port <span class="number">48096</span> connected with <span class="number">10.0</span><span class="number">.0</span><span class="number">.2</span> port <span class="number">5001</span>
[ ID] Interval       Transfer     Bandwidth
[  <span class="number">3</span>]  <span class="number">0.0</span>-<span class="number">10.0</span> sec  <span class="number">1.84</span> GBytes  <span class="number">1.58</span> Gbits/sec
</code></pre><p>The overlay network performance is a bit worse than native. It’s also a little worse than <a href="/2015/11/06/calico-docker/#Performance Tests">Calico</a>, which is almost the same as native performance. Since Calico uses a pure 3-Layer protocol and Docker Multi-host Overlay Network uses VXLAN solution (MAC on UDP), Calico does make sense to gain a better performance.</p>
<h1 id="VXLAN_Technology">VXLAN Technology</h1><p>Virtual Extensible LAN (VXLAN) is a network virtualization technology that attempts to ameliorate the scalability problems associated with large cloud computing deployments. It uses a VLAN-like encapsulation technique to encapsulate MAC-based OSI layer 2 Ethernet frames within layer 4 UDP packets. <a href="https://en.wikipedia.org/wiki/Open_vSwitch" target="_blank" rel="external">Open vSwitch</a> is a former implementation of VXLAN, but Docker Engine implements a built-in VXLAN driver in libnetwork.</p>
<p>For more VXLAN details, you can see its <a href="https://datatracker.ietf.org/doc/rfc7348/" target="_blank" rel="external">official RFC</a> and a <a href="https://www.emulex.com/artifacts/d658610a-d3b6-457c-bf2d-bf8d476c6a98/elx_wp_all_VXLAN.pdf" target="_blank" rel="external">white paper</a> from EMulex. I’d like to post another blog to have more detailed discussion on VXLAN Technology.</p>
<h1 id="References">References</h1><p>[1] Docker Multi-host Networking Post: <a href="http://blog.docker.com/2015/11/docker-multi-host-networking-ga/" target="_blank" rel="external">http://blog.docker.com/2015/11/docker-multi-host-networking-ga/</a><br>[2] Docker Network Docs: <a href="http://docs.docker.com/engine/userguide/networking/dockernetworks/" target="_blank" rel="external">http://docs.docker.com/engine/userguide/networking/dockernetworks/</a><br>[3] Get Started Overlay Network for Docker: <a href="https://docs.docker.com/engine/userguide/networking/get-started-overlay/" target="_blank" rel="external">https://docs.docker.com/engine/userguide/networking/get-started-overlay/</a><br>[4] Docker v1.9 Announcemount: <a href="https://blog.docker.com/2015/11/docker-1-9-production-ready-swarm-multi-host-networking/" target="_blank" rel="external">https://blog.docker.com/2015/11/docker-1-9-production-ready-swarm-multi-host-networking/</a><br>[5] VXLAN Official RFC: <a href="https://datatracker.ietf.org/doc/rfc7348/" target="_blank" rel="external">https://datatracker.ietf.org/doc/rfc7348/</a><br>[6] VXLAN White Paper: <a href="https://www.emulex.com/artifacts/d658610a-d3b6-457c-bf2d-bf8d476c6a98/elx_wp_all_VXLAN.pdf" target="_blank" rel="external">https://www.emulex.com/artifacts/d658610a-d3b6-457c-bf2d-bf8d476c6a98/elx_wp_all_VXLAN.pdf</a></p>
]]></content>
    <summary type="html">
    <![CDATA[<p><a href="http://docker.io/" target="_blank" rel="external">Docker</a> has released its newest version v1.9 (<a href="https://blog.docker.]]>
    </summary>
    
      <category term="Docker" scheme="http://chunqi.li/tags/Docker/"/>
    
      <category term="Etcd" scheme="http://chunqi.li/tags/Etcd/"/>
    
      <category term="Multi-host Network" scheme="http://chunqi.li/tags/Multi-host-Network/"/>
    
      <category term="Overlay Network" scheme="http://chunqi.li/tags/Overlay-Network/"/>
    
      <category term="VXLAN" scheme="http://chunqi.li/tags/VXLAN/"/>
    
      <category term="Docker Multi-host Network" scheme="http://chunqi.li/categories/Docker-Multi-host-Network/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[Flannel for Docker Overlay Network]]></title>
    <link href="http://chunqi.li/2015/10/10/Flannel-for-Docker-Overlay-Network/"/>
    <id>http://chunqi.li/2015/10/10/Flannel-for-Docker-Overlay-Network/</id>
    <published>2015-10-10T10:03:49.000Z</published>
    <updated>2015-11-16T03:52:45.000Z</updated>
    <content type="html"><![CDATA[<p>In previous post, some overlay network technologies for Docker are analysised. On this post let’s focus on <a href="https://github.com/coreos/flannel" target="_blank" rel="external">Flannel</a>, a virtual network that creates subnet work Dockers across different hosts.</p>
<h1 id="Introduction_to_Flannel">Introduction to Flannel</h1><p><a href="https://github.com/coreos/flannel" target="_blank" rel="external">Flannel</a>, similar to <a href="https://github.com/projectcalico/calico-docker" target="_blank" rel="external">Calico</a>, <a href="https://datatracker.ietf.org/doc/rfc7348/" target="_blank" rel="external">VXLAN</a> and <a href="https://github.com/weaveworks/weave" target="_blank" rel="external">Weave</a>, provides a configurable virtual overlay network for Docker. Flannel runs an agent, flanneld, on each host and is responsible for allocating subnet lease out of a preconfigured address space. Flannel uses <a href="https://github.com/coreos/etcd" target="_blank" rel="external">etcd</a> to store network configurations. I copied this architecture image from <a href="https://github.com/coreos/flannel" target="_blank" rel="external">Flannel GitHub page</a> to illustrate the details of the path a packet take as it tranverse the overlay network.</p>
<img src="/images/flannel-01.png">
<h1 id="Config_Etcd">Config Etcd</h1><h2 id="Download_and_Run_Etcd">Download and Run Etcd</h2><p>Since Flannel depends on Etcd, you need to download, run and config Etcd before starting flanneld. Assume that you have two Linux VM (or physical machine) with hostname node1/node2 and IP 192.168.236.130/131 seperately. On each node download and run Etcd as follows:</p>
<figure class="highlight bash"><figcaption><span>Download and run etcd</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">curl -L  https://github.com/coreos/etcd/releases/download/v2.<span class="number">2.1</span>/etcd-v2.<span class="number">2.1</span>-linux-amd64.tar.gz -o etcd-v2.<span class="number">2.1</span>-linux-amd64.tar.gz</span><br><span class="line">tar xzvf etcd-v2.<span class="number">2.1</span>-linux-amd64.tar.gz</span><br><span class="line"><span class="built_in">cd</span> etcd-v2.<span class="number">2.1</span>-linux-amd64</span><br><span class="line">./etcd -name &#123;node&#125; -initial-advertise-peer-urls http://<span class="number">0.0</span>.<span class="number">0.0</span>:<span class="number">2380</span> \</span><br><span class="line">  -listen-peer-urls http://<span class="number">0.0</span>.<span class="number">0.0</span>:<span class="number">2380</span> \</span><br><span class="line">  -listen-client-urls http://<span class="number">0.0</span>.<span class="number">0.0</span>:<span class="number">2379</span>,http://<span class="number">127.0</span>.<span class="number">0.1</span>:<span class="number">4001</span> \</span><br><span class="line">  -advertise-client-urls http://<span class="number">0.0</span>.<span class="number">0.0</span>:<span class="number">2379</span> \</span><br><span class="line">  -initial-cluster-token etcd-cluster \</span><br><span class="line">  -initial-cluster node1=http://<span class="number">192.168</span>.<span class="number">236.130</span>:<span class="number">2380</span>,node2=http://<span class="number">192.168</span>.<span class="number">236.131</span>:<span class="number">2380</span> \</span><br><span class="line">  -initial-cluster-state new</span><br></pre></td></tr></table></figure>
<h2 id="Config_Etcd-1">Config Etcd</h2><p>Flannel reads its configuration from etcd. By default, it will read the configuration from <code>/coreos.com/network/config</code> (can be overridden via –etcd-prefix). You need to use <code>etcdctl</code> utility to set values in etcd. On the directory you downloaded Etcd previously, run following commands:</p>
<figure class="highlight bash"><figcaption><span>Config Etcd</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">./etcdctl <span class="built_in">set</span> /coreos.com/network/config 	\</span><br><span class="line">true<span class="string">'&#123;"Network": "10.0.0.0/8",				\</span><br><span class="line">true"SubnetLen": 20,						\</span><br><span class="line">true"SubnetMin": "10.10.0.0",				\</span><br><span class="line">true"SubnetMax": "10.99.0.0",				\</span><br><span class="line">true"Backend": &#123;							\</span><br><span class="line">truetrue"Type": "udp",						\</span><br><span class="line">truetrue"Port": 7890&#125;&#125; '</span></span><br></pre></td></tr></table></figure>
<h1 id="Build_and_Run_Flannel">Build and Run Flannel</h1><h2 id="Build_Flannel">Build Flannel</h2><ul>
<li>Step 1: On ubuntu, run <code>sudo apt-get install linux-libc-dev golang gcc</code>. On Fedora/Redhat, run <code>sudo yum install kernel-headers golang gcc</code>.</li>
<li>Step 2: Git clone the flannel repo: git clone <a href="https://github.com/coreos/flannel.git" target="_blank" rel="external">https://github.com/coreos/flannel.git</a></li>
<li>Step 3: Run the build script: cd flannel; ./build</li>
</ul>
<p>If Flannel build failed on your local environment, you can also build flannel inside a Docker container. Confirm that you have install Docker first with <code>docker -v</code>, and then execute:</p>
<figure class="highlight bash"><figcaption><span>Install Docker</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">cd</span> flannel</span><br><span class="line">docker build .</span><br></pre></td></tr></table></figure>
<h2 id="Run_Flannel">Run Flannel</h2><p>After Etcd is set up, you need to run flanneld on both nodes:</p>
<figure class="highlight bash"><figcaption><span>Run flannel</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo ./bin/flanneld &amp;</span><br></pre></td></tr></table></figure>
<p>Use <code>ifconfig</code> to confirm the network of flanned was setup successfully, the outputs should be something like this:</p>
<pre><code>flannel0  Link encap:UNSPEC  HWaddr <span class="number">00</span>-<span class="number">00</span>-<span class="number">00</span>-<span class="number">00</span>-<span class="number">00</span>-<span class="number">00</span>-<span class="number">00</span>-<span class="number">00</span>-<span class="number">00</span>-<span class="number">00</span>-<span class="number">00</span>-<span class="number">00</span>-<span class="number">00</span>-<span class="number">00</span>-<span class="number">00</span>-<span class="number">00</span>
          inet addr:<span class="number">10.15</span><span class="number">.240</span><span class="number">.0</span>  P-t-P:<span class="number">10.15</span><span class="number">.240</span><span class="number">.0</span>  Mask:<span class="number">255.0</span><span class="number">.0</span><span class="number">.0</span>
          UP POINTOPOINT RUNNING NOARP MULTICAST  MTU:<span class="number">1472</span>  Metric:<span class="number">1</span>
          RX packets:<span class="number">606921</span> errors:<span class="number">0</span> dropped:<span class="number">0</span> overruns:<span class="number">0</span> frame:<span class="number">0</span>
          TX packets:<span class="number">308311</span> errors:<span class="number">0</span> dropped:<span class="number">0</span> overruns:<span class="number">0</span> carrier:<span class="number">0</span>
          collisions:<span class="number">0</span> txqueuelen:<span class="number">500</span>
          RX bytes:<span class="number">893358516</span> (<span class="number">893.3</span> MB)  TX bytes:<span class="number">16225380</span> (<span class="number">16.2</span> MB)
</code></pre><p>After Flannel is running, you need to config network for docker0 and restart docker daemon with Flannel network configuration, execute commands as follows:</p>
<figure class="highlight bash"><figcaption><span>Run flannel</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">service docker stop</span><br><span class="line"><span class="built_in">source</span> /run/flannel/subnet.env</span><br><span class="line">sudo ifconfig docker0 <span class="variable">$&#123;FLANNEL_SUBNET&#125;</span></span><br><span class="line">sudo docker daemon --bip=<span class="variable">$&#123;FLANNEL_SUBNET&#125;</span> --mtu=<span class="variable">$&#123;FLANNEL_MTU&#125;</span> &amp;</span><br><span class="line">docker ps</span><br></pre></td></tr></table></figure>
<h2 id="Start_Docker">Start Docker</h2><p>After Flannel set up, just start your docker without any differences without Flannel. Run the following command on node1:</p>
<figure class="highlight bash"><figcaption><span>Run Docker</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">sudo docker run -itd --name=worker-<span class="number">1</span> ubuntu</span><br><span class="line">sudo docker run -itd --name=worker-<span class="number">2</span> ubuntu</span><br></pre></td></tr></table></figure>
<p>Then run Docker on node2:</p>
<figure class="highlight bash"><figcaption><span>Run Docker</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo docker run -itd --name=worker-<span class="number">3</span> ubuntu</span><br></pre></td></tr></table></figure>
<p>Then use <code>sudo docker exec worker-N ifconfig</code> to get the IP of these workers (e.g. 10.15.240.2, 10.15.240.3 and 10.10.160.2 for worker-1/2/3). On node1, test connectivity to worker-3:</p>
<figure class="highlight bash"><figcaption><span>Test connectivity</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">sudo docker <span class="built_in">exec</span> worker-<span class="number">1</span> ping -c4 <span class="number">10.10</span>.<span class="number">160.2</span></span><br><span class="line">sudo docker <span class="built_in">exec</span> worker-<span class="number">1</span> ping www.google.com</span><br></pre></td></tr></table></figure>
<p>All these pings should return successfully.</p>
<h1 id="Simple_Performance_Test">Simple Performance Test</h1><p>Until now Flannel is setup for Docker and all the workers are connected with each other <strong>physically</strong>. Then I did a simple performance test with iperf between two Dockers in different/same hosts.</p>
<p>Firstly let’s see the native network performance between two hosts:</p>
<pre><code>flannel@node2:~<span class="preprocessor"># iperf -c <span class="number">192.168</span><span class="number">.236</span><span class="number">.130</span></span>
------------------------------------------------------------
Client connecting to <span class="number">192.168</span><span class="number">.236</span><span class="number">.130</span>, TCP port <span class="number">5001</span>
TCP window size: <span class="number">85.0</span> KByte (<span class="keyword">default</span>)
------------------------------------------------------------
[  <span class="number">3</span>] local <span class="number">192.168</span><span class="number">.236</span><span class="number">.131</span> port <span class="number">54584</span> connected with <span class="number">192.168</span><span class="number">.236</span><span class="number">.130</span> port <span class="number">5001</span>
[ ID] Interval       Transfer     Bandwidth
[  <span class="number">3</span>]  <span class="number">0.0</span>-<span class="number">10.0</span> sec  <span class="number">2.57</span> GBytes  <span class="number">2.21</span> Gbits/sec
</code></pre><p>Then dockers on different host:</p>
<pre><code>root@<span class="number">93</span>c451432761:~<span class="preprocessor"># iperf -c <span class="number">10.10</span><span class="number">.160</span><span class="number">.2</span></span>
------------------------------------------------------------
Client connecting to <span class="number">10.10</span><span class="number">.160</span><span class="number">.2</span>, TCP port <span class="number">5001</span>
TCP window size: <span class="number">85.0</span> KByte (<span class="keyword">default</span>)
------------------------------------------------------------
[  <span class="number">3</span>] local <span class="number">10.15</span><span class="number">.240</span><span class="number">.2</span> port <span class="number">57496</span> connected with <span class="number">10.10</span><span class="number">.160</span><span class="number">.2</span> port <span class="number">5001</span>
[ ID] Interval       Transfer     Bandwidth
[  <span class="number">3</span>]  <span class="number">0.0</span>-<span class="number">10.0</span> sec   <span class="number">418</span> MBytes   <span class="number">351</span> Mbits/sec
</code></pre><p>The performance of Dockers on the same host is pretty good.</p>
<pre><code>root@<span class="number">93</span>c451432761:~<span class="preprocessor"># iperf -c <span class="number">10.15</span><span class="number">.240</span><span class="number">.3</span></span>
------------------------------------------------------------
Client connecting to <span class="number">10.15</span><span class="number">.240</span><span class="number">.3</span>, TCP port <span class="number">5001</span>
TCP window size: <span class="number">85.0</span> KByte (<span class="keyword">default</span>)
------------------------------------------------------------
[  <span class="number">3</span>] local <span class="number">10.15</span><span class="number">.240</span><span class="number">.2</span> port <span class="number">38099</span> connected with <span class="number">10.15</span><span class="number">.240</span><span class="number">.3</span> port <span class="number">5001</span>
[ ID] Interval       Transfer     Bandwidth
[  <span class="number">3</span>]  <span class="number">0.0</span>-<span class="number">10.0</span> sec  <span class="number">39.2</span> GBytes  <span class="number">33.7</span> Gbits/sec
</code></pre><p><del><strong>The performace is so bad compared with native!!!!!</strong> I can’t figure out why the performance degrades too much with Flannel. Since Calico and Docker Multi-host Network can achieve more than 80% performance compared with native, Flannel does a aweful job apparently. If anyone knows why, please email me or comments under this blog.</del></p>
<p>After read through the configuration documents of Flannel, I found that flannel support two backends: UDP backend and VxLAN backend. Try VxLAN backend and the speed is much more fast and close to native performance.</p>
<h1 id="UDP_and_VxLAN_backends">UDP and VxLAN backends</h1><p>There are two different backends supported by Flannel. The previous configuration on this blog uses UDP backend, which is a pretty slow solution because all the packets are encrypted in userspace. VxLAN backend uses Linux Kernel VxLAN support as well as some hardware features to achieve a much more faster network.</p>
<p>It’s easy to use VxLAN backend. When configuring Etcd, just define the <code>backend</code> block with <code>vxlan</code>.</p>
<figure class="highlight bash"><figcaption><span>Config Etcd</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">./etcdctl <span class="built_in">set</span> /coreos.com/network/config 	\</span><br><span class="line">true<span class="string">'&#123;"Network": "10.0.0.0/8",				\</span><br><span class="line">true"SubnetLen": 20,						\</span><br><span class="line">true"SubnetMin": "10.10.0.0",				\</span><br><span class="line">true"SubnetMax": "10.99.0.0",				\</span><br><span class="line">true"Backend": &#123;							\</span><br><span class="line">truetrue"Type": "vxlan"&#125;&#125; '</span></span><br></pre></td></tr></table></figure>
<p>With VxLAN backend, the iperf result of two containers on different hosts are as follows:</p>
<pre><code>root@<span class="number">93</span>c451432761:~<span class="preprocessor"># iperf -c <span class="number">10.15</span><span class="number">.240</span><span class="number">.3</span></span>
------------------------------------------------------------
Client connecting to <span class="number">10.15</span><span class="number">.240</span><span class="number">.3</span>, TCP port <span class="number">5001</span>
TCP window size: <span class="number">85.0</span> KByte (<span class="keyword">default</span>)
------------------------------------------------------------
[  <span class="number">3</span>] local <span class="number">10.15</span><span class="number">.240</span><span class="number">.2</span> port <span class="number">38099</span> connected with <span class="number">10.15</span><span class="number">.240</span><span class="number">.3</span> port <span class="number">5001</span>
[ ID] Interval       Transfer     Bandwidth
[  <span class="number">3</span>]  <span class="number">0.0</span>-<span class="number">10.0</span> sec  <span class="number">1.80</span> GBytes  <span class="number">1.56</span> Gbits/sec
</code></pre><p>This is an acceptable result with about 80% performance compared with native network.</p>
<h1 id="References">References</h1><p>[1] Flannel code base, <a href="https://github.com/coreos/flannel" target="_blank" rel="external">https://github.com/coreos/flannel</a><br>[2] Using coreos flannel for docker networking, <a href="http://www.slideshare.net/lorispack/using-coreos-flannel-for-docker-networking" target="_blank" rel="external">http://www.slideshare.net/lorispack/using-coreos-flannel-for-docker-networking</a></p>
]]></content>
    <summary type="html">
    <![CDATA[<p>In previous post, some overlay network technologies for Docker are analysised. On this post let’s focus on <a href="https://github.com/co]]>
    </summary>
    
      <category term="Docker" scheme="http://chunqi.li/tags/Docker/"/>
    
      <category term="Flannel" scheme="http://chunqi.li/tags/Flannel/"/>
    
      <category term="Multi-host Network" scheme="http://chunqi.li/tags/Multi-host-Network/"/>
    
      <category term="Docker Multi-host Network" scheme="http://chunqi.li/categories/Docker-Multi-host-Network/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[Calico: A Solution of Multi-host Network For Docker]]></title>
    <link href="http://chunqi.li/2015/09/06/calico-docker/"/>
    <id>http://chunqi.li/2015/09/06/calico-docker/</id>
    <published>2015-09-06T06:06:56.000Z</published>
    <updated>2015-11-15T03:57:51.000Z</updated>
    <content type="html"><![CDATA[<p><a href="https://github.com/projectcalico/calico" target="_blank" rel="external">Calico</a> is a pure 3-layer protocol to support multi-host network communication for OpenStacks VMs and Docker containers. Calico does not use overlay network such as <a href="https://github.com/coreos/flannel" target="_blank" rel="external">falnnel</a> and <a href="https://github.com/docker/libnetwork/blob/master/docs/overlay.md" target="_blank" rel="external">libnetwork overlay driver</a>, it is a pure Layer 3 approach with a vRouter implementation instead of a vSwitcher. Each vRouter propagates workload reachability information (routes) to the rest of the data center using BGP protocol.</p>
<p>This post focus on how to setup a multi-host networking for Docker containers with <a href="https://github.com/projectcalico/calico-docker" target="_blank" rel="external">calico-docker</a> and some advanced features.</p>
<h1 id="Environment">Environment</h1><h2 id="Environment_Prerequisite">Environment Prerequisite</h2><ul>
<li>Two linux nodes (node1 and node2) with Ubuntu Linux distribution, either VM or physical machine is OK.</li>
<li>Install docker on both nodes.</li>
<li>Etcd cluster.</li>
</ul>
<h2 id="Configuration_&amp;_Download">Configuration &amp; Download</h2><p>Setup two linux nodes with IP 192.168.236.130/131 and connect them physically or virtually, confirm that they can ping each other succesfully. Setup docker bridge (default is docker0) on two nodes. Let’s set two docker bridges with different network. Netowrk configuration details are as follows:</p>
<p>Node1</p>
<ul>
<li>IP: 192.168.236.130</li>
<li>Docker bridge network: 192.168.1.0/24</li>
</ul>
<p>Node2</p>
<ul>
<li>IP: 192.168.236.131</li>
<li>Docker bridge network: 172.17.0.0/16</li>
</ul>
<p>Install Docker, should be no error here.</p>
<figure class="highlight bash"><figcaption><span>Install Docker</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">sudo apt-get install docker.io</span><br><span class="line">docker ps</span><br></pre></td></tr></table></figure>
<p>Download and run etcd, replace {node} with node0/1 seperately. We need at least two etcd node since the new version of etcd cannot run on single node.</p>
<figure class="highlight bash"><figcaption><span>Download and run etcd</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">curl -L  https://github.com/coreos/etcd/releases/download/v2.<span class="number">2.1</span>/etcd-v2.<span class="number">2.1</span>-linux-amd64.tar.gz -o etcd-v2.<span class="number">2.1</span>-linux-amd64.tar.gz</span><br><span class="line">tar xzvf etcd-v2.<span class="number">2.1</span>-linux-amd64.tar.gz</span><br><span class="line"><span class="built_in">cd</span> etcd-v2.<span class="number">2.1</span>-linux-amd64</span><br><span class="line">./etcd -name &#123;node&#125; -initial-advertise-peer-urls http://<span class="number">0.0</span>.<span class="number">0.0</span>:<span class="number">2380</span> \</span><br><span class="line">  -listen-peer-urls http://<span class="number">0.0</span>.<span class="number">0.0</span>:<span class="number">2380</span> \</span><br><span class="line">  -listen-client-urls http://<span class="number">0.0</span>.<span class="number">0.0</span>:<span class="number">2379</span>,http://<span class="number">127.0</span>.<span class="number">0.1</span>:<span class="number">4001</span> \</span><br><span class="line">  -advertise-client-urls http://<span class="number">0.0</span>.<span class="number">0.0</span>:<span class="number">2379</span> \</span><br><span class="line">  -initial-cluster-token etcd-cluster \</span><br><span class="line">  -initial-cluster node1=http://<span class="number">192.168</span>.<span class="number">236.130</span>:<span class="number">2380</span>,node2=http://<span class="number">192.168</span>.<span class="number">236.131</span>:<span class="number">2380</span> \</span><br><span class="line">  -initial-cluster-state new</span><br></pre></td></tr></table></figure>
<p>Download calicoctl<br><figure class="highlight bash"><figcaption><span>Download calicoctl</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">wget https://github.com/projectcalico/calico-docker/releases/download/v0.<span class="number">10.0</span>/calicoctl</span><br></pre></td></tr></table></figure></p>
<h1 id="Start_Calico_Services">Start Calico Services</h1><p>Calico services in Docker environment are running as a Docker container using host network configuration. All containers configured with Calico services with use calico-node to communicate with each other and Internet.</p>
<p>Run the following commands on node1/2 to start calico-node</p>
<figure class="highlight bash"><figcaption><span>Run calico-node</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo calicoctl node --ip=&#123;host_ip&#125;</span><br></pre></td></tr></table></figure>
<p>You should see output like this on each node</p>
<pre><code>calico@node1:~<span class="comment"># docker ps</span>
CONTAINER ID        IMAGE                COMMAND             CREATED             STATUS              PORTS               NAMES
<span class="number">40</span>b177803c97        calico/<span class="keyword">node</span><span class="identifier"></span><span class="title">:v0</span>.<span class="number">9.0</span>   <span class="string">"/sbin/my_init"</span>     <span class="number">27</span> seconds ago      Up <span class="number">27</span> seconds                           calico-<span class="keyword">node</span><span class="identifier"></span><span class="title"></span>
</code></pre><p>Before starting any containers, we need to configure an IP pool with the <code>ipip</code> and <code>nat-outgoing</code> options. Thus containers with an valid profile could have access to Internet. Run the following command on either node.</p>
<figure class="highlight bash"><figcaption><span>Configure IP pool</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">calicoctl pool add <span class="number">192.168</span>.<span class="number">100.0</span>/<span class="number">24</span> --ipip --nat-outgoing</span><br></pre></td></tr></table></figure>
<h1 id="Container_Networking_Configuration">Container Networking Configuration</h1><h2 id="Start_Containers">Start Containers</h2><p>Firstly run a few containers on each host.</p>
<p>On node1:<br><figure class="highlight bash"><figcaption><span>Run container on node1</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">docker run --net=none --name worker-<span class="number">1</span> -tid ubuntu</span><br><span class="line">docker run --net=none --name worker-<span class="number">2</span> -tid ubuntu</span><br></pre></td></tr></table></figure></p>
<p>On node2:<br><figure class="highlight bash"><figcaption><span>Run container on node2</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker run --net=none --name worker-<span class="number">3</span> -tid ubuntu</span><br></pre></td></tr></table></figure></p>
<h2 id="Configure_Calico_Networking">Configure Calico Networking</h2><p>Now that all the containers are running without any network devices. Use Calico to assign network devices to these containers. Notice that IPs assigned to containers should be in the range of IP pools.</p>
<p>On node1:<br><figure class="highlight bash"><figcaption><span>Configure network on node1</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">sudo calicoctl container add worker-<span class="number">1</span> <span class="number">192.168</span>.<span class="number">100.1</span></span><br><span class="line">sudo calicoctl container add worker-<span class="number">2</span> <span class="number">192.168</span>.<span class="number">100.2</span></span><br></pre></td></tr></table></figure></p>
<p>On node2:<br><figure class="highlight bash"><figcaption><span>Configure network on node2</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo calicoctl container add worker-<span class="number">3</span> <span class="number">192.168</span>.<span class="number">100.3</span></span><br></pre></td></tr></table></figure></p>
<p>Once containers have Calico networking, they gain a network device with corresponding IP address. At this point them have access neither to each other nor to Internet since no profiles are created and assigned to them.</p>
<p>Create some profiles on either node:<br><figure class="highlight bash"><figcaption><span>Create profiles</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">calicoctl profile add PROF_1</span><br><span class="line">calicoctl profile add PROF_2</span><br></pre></td></tr></table></figure></p>
<p>Then assign profiles to containers. Containers in same profile have access to each other. And containers in the IP poll created before won’t have access to Internet until added to a profile.</p>
<p>On node1:<br><figure class="highlight bash"><figcaption><span>Assign profiles to containers on node1</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">calicoctl container worker-<span class="number">1</span> profile append PROF_1</span><br><span class="line">calicoctl container worker-<span class="number">2</span> profile append PROF_2</span><br></pre></td></tr></table></figure></p>
<p>On node2:<br><figure class="highlight bash"><figcaption><span>Assign profiles to containers on node2</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">calicoctl container worker-<span class="number">3</span> profile append PROF_1</span><br></pre></td></tr></table></figure></p>
<p>Until now all configurations are done and we will test network connections of these containers afterwards.</p>
<h1 id="Testing">Testing</h1><p>Now check the connectivities of each containers. At this point every containers should have access to Internet, try and ping google.com:<br><figure class="highlight bash"><figcaption><span>Check Internet access</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">docker <span class="built_in">exec</span> worker-<span class="number">1</span> ping -c <span class="number">4</span> www.google.com</span><br><span class="line">docker <span class="built_in">exec</span> worker-<span class="number">2</span> ping -c <span class="number">4</span> www.google.com</span><br></pre></td></tr></table></figure></p>
<p>Then check connections of containers in same profile:<br><figure class="highlight bash"><figcaption><span>Check inner profile access</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker <span class="built_in">exec</span> worker-<span class="number">1</span> ping -c <span class="number">4</span> <span class="number">192.168</span>.<span class="number">100.3</span></span><br></pre></td></tr></table></figure></p>
<p>And containers not in same profile cannot ping each other:<br><figure class="highlight bash"><figcaption><span>Check access outer profile</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker <span class="built_in">exec</span> worker-<span class="number">1</span> ping -c <span class="number">4</span> <span class="number">192.168</span>.<span class="number">100.2</span></span><br></pre></td></tr></table></figure></p>
<p>If we add worker-2 into profile PROF_1, then worker-2 could ping worker-1 and worker-3.<br>On node1:<br><figure class="highlight bash"><figcaption><span>Advanced check</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">calicoctl container worker-<span class="number">2</span> profile append PROF_1</span><br><span class="line">docker <span class="built_in">exec</span> worker-<span class="number">2</span> ping -c <span class="number">4</span> <span class="number">192.168</span>.<span class="number">100.1</span></span><br><span class="line">docker <span class="built_in">exec</span> worker-<span class="number">2</span> ping -c <span class="number">4</span> <span class="number">192.168</span>.<span class="number">100.3</span></span><br></pre></td></tr></table></figure></p>
<h1 id="Performance_Tests">Performance Tests</h1><h2 id="Simple_Test">Simple Test</h2><p>I perform a simple performance test using <code>iperf</code> to evaluate the network between two Calico containers. Run <code>iperf -s</code> on worker-1 and <code>iperf -c 192.168.100.1</code> on worker-3. We can get the result:</p>
<pre><code>root@<span class="number">39f</span>db1701da4:~<span class="preprocessor"># ./iperf -c <span class="number">192.168</span><span class="number">.101</span><span class="number">.2</span></span>
------------------------------------------------------------
Client connecting to <span class="number">192.168</span><span class="number">.101</span><span class="number">.2</span>, TCP port <span class="number">5001</span>
TCP window size: <span class="number">85.0</span> KByte (<span class="keyword">default</span>)
------------------------------------------------------------
[  <span class="number">3</span>] local <span class="number">192.168</span><span class="number">.101</span><span class="number">.1</span> port <span class="number">39187</span> connected with <span class="number">192.168</span><span class="number">.101</span><span class="number">.2</span> port <span class="number">5001</span>
[ ID] Interval       Transfer     Bandwidth
[  <span class="number">3</span>]  <span class="number">0.0</span>-<span class="number">10.0</span> sec  <span class="number">1.08</span> GBytes   <span class="number">927</span> Mbits/sec
</code></pre><p>Then run the same test on native host (node1 and node2):</p>
<pre><code>calico@node2:~<span class="preprocessor"># iperf -c <span class="number">192.168</span><span class="number">.236</span><span class="number">.130</span></span>
------------------------------------------------------------
Client connecting to <span class="number">192.168</span><span class="number">.236</span><span class="number">.130</span>, TCP port <span class="number">5001</span>
TCP window size: <span class="number">85.0</span> KByte (<span class="keyword">default</span>)
------------------------------------------------------------
[  <span class="number">3</span>] local <span class="number">192.168</span><span class="number">.236</span><span class="number">.131</span> port <span class="number">54584</span> connected with <span class="number">192.168</span><span class="number">.236</span><span class="number">.130</span> port <span class="number">5001</span>
[ ID] Interval       Transfer     Bandwidth
[  <span class="number">3</span>]  <span class="number">0.0</span>-<span class="number">10.0</span> sec  <span class="number">2.57</span> GBytes  <span class="number">2.21</span> Gbits/sec
</code></pre><p>From the result we can see there’s a great gap between Calico network and native network. But according to the official documents and evaluations, calico network should be similar to the native network. <strong>WHY???</strong></p>
<h2 id="Dive_Deeper">Dive Deeper</h2><p>To find out the reason of slow network, firstly I test the network performance between workker-1 and worker-2, which are in the same host. The result is as follows:</p>
<pre><code>root@<span class="number">51</span>b78d9e6153:/<span class="preprocessor"># iperf -c <span class="number">192.168</span><span class="number">.100</span><span class="number">.2</span></span>
------------------------------------------------------------
Client connecting to <span class="number">192.168</span><span class="number">.100</span><span class="number">.3</span>, TCP port <span class="number">5001</span>
TCP window size: <span class="number">85.0</span> KByte (<span class="keyword">default</span>)
------------------------------------------------------------
[  <span class="number">3</span>] local <span class="number">192.168</span><span class="number">.100</span><span class="number">.2</span> port <span class="number">36476</span> connected with <span class="number">192.168</span><span class="number">.100</span><span class="number">.3</span> port <span class="number">5001</span>
[ ID] Interval       Transfer     Bandwidth
[  <span class="number">3</span>]  <span class="number">0.0</span>-<span class="number">10.0</span> sec  <span class="number">47.3</span> GBytes  <span class="number">40.6</span> Gbits/sec
</code></pre><p>Since speed of my net card is only 1Gbits/sec, it seems that containers on the same host connects each other directly without going through any network device. That really make all sense.</p>
<p>Then I dived deep into the documents and configurations of Calico and found such configuration of IP pool:<br><figure class="highlight bash"><figcaption><span>Configure IP pool</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">calicoctl pool add <span class="number">192.168</span>.<span class="number">100.0</span>/<span class="number">24</span> --ipip --nat-outgoing</span><br></pre></td></tr></table></figure></p>
<p>We use <code>--ipip</code> option when creating IP pool, which means <code>Use IP-over-IP encapsulation across hosts</code>. This option will enforce another layer of IP-over-IP encapsulation when packages traveling across hosts. Since our hosts node1 and node2 are in the same network (192.168.236.0/24), we could avoid this option and the speed should increase as supposed.</p>
<p>Run the following command on either node to override the previous IP pool configuration.<br><figure class="highlight bash"><figcaption><span>Configure IP pool</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">calicoctl pool add <span class="number">192.168</span>.<span class="number">100.0</span>/<span class="number">24</span> --nat-outgoing</span><br><span class="line">calicoctl pool show</span><br></pre></td></tr></table></figure></p>
<p>Then test networking between worker-1 and worker-3 again:</p>
<pre><code>root@<span class="number">39f</span>db1701da4:~<span class="preprocessor"># ./iperf -c <span class="number">192.168</span><span class="number">.101</span><span class="number">.2</span></span>
------------------------------------------------------------
Client connecting to <span class="number">192.168</span><span class="number">.101</span><span class="number">.2</span>, TCP port <span class="number">5001</span>
TCP window size: <span class="number">85.0</span> KByte (<span class="keyword">default</span>)
------------------------------------------------------------
[  <span class="number">3</span>] local <span class="number">192.168</span><span class="number">.101</span><span class="number">.1</span> port <span class="number">39187</span> connected with <span class="number">192.168</span><span class="number">.101</span><span class="number">.2</span> port <span class="number">5001</span>
[ ID] Interval       Transfer     Bandwidth
[  <span class="number">3</span>]  <span class="number">0.0</span>-<span class="number">10.0</span> sec  <span class="number">2.74</span> GBytes  <span class="number">2.35</span> Gbits/sec
</code></pre><p>Hurray!!! That’s the native speed!</p>
<h1 id="References">References</h1><p>[1] Project Calico: <a href="https://github.com/projectcalico/calico" target="_blank" rel="external">https://github.com/projectcalico/calico</a><br>[2] Calico Docker: <a href="https://github.com/projectcalico/calico-docker" target="_blank" rel="external">https://github.com/projectcalico/calico-docker</a><br>[3] Demenstration on calico-docker: <a href="https://github.com/projectcalico/calico-docker" target="_blank" rel="external">https://github.com/projectcalico/calico-docker</a><br>[4] Calico-docker in Yixin: <a href="http://mp.weixin.qq.com/s?__biz=MzAwMDU1MTE1OQ==&amp;mid=400983139&amp;idx=1&amp;sn=f033e3dca32ca9f0b7c9779528523e7e&amp;scene=1&amp;srcid=1101jklWCo9jNFjdnUum85PG&amp;from=singlemessage&amp;isappinstalled=0#wechat_redirect" target="_blank" rel="external">Paper URL</a></p>
]]></content>
    <summary type="html">
    <![CDATA[<p><a href="https://github.com/projectcalico/calico" target="_blank" rel="external">Calico</a> is a pure 3-layer protocol to support multi-h]]>
    </summary>
    
      <category term="Calico" scheme="http://chunqi.li/tags/Calico/"/>
    
      <category term="Docker" scheme="http://chunqi.li/tags/Docker/"/>
    
      <category term="Multi-host Network" scheme="http://chunqi.li/tags/Multi-host-Network/"/>
    
      <category term="Docker Multi-host Network" scheme="http://chunqi.li/categories/Docker-Multi-host-Network/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[L2TP+IPSec on VPS]]></title>
    <link href="http://chunqi.li/2015/05/21/L2TP-IPSec-on-VPS/"/>
    <id>http://chunqi.li/2015/05/21/L2TP-IPSec-on-VPS/</id>
    <published>2015-05-21T06:59:34.000Z</published>
    <updated>2015-11-10T07:10:03.000Z</updated>
    <content type="html"><![CDATA[<p>L2TP+IPSec is another way to setup VPN on a VPS. L2TP consumes 1701 TCP port to maintain connection and 500/4500 UDP to transfer data. It’s very easy to implement L2TP and IPSec on a Ubuntu 14.04 server.</p>
<p>Before setting up L2TP/IPSec environment, you need to enable PPP support for VPS. See details on section <strong>“Enable PPP Support of VPS”</strong> of my previous post “<a href="/2015/05/20/Setup-PPTP-Server-on-a-VPS/">Setup PPTP Server on a VPS</a>“ to enable PPP support on RamNode VPS.</p>
<p>When I first installed xl2tpd and openswan, it occured to me the following errors and refused my iPhone VPN connection:</p>
<pre><code><span class="type">May</span> <span class="number">19</span> <span class="number">05</span>:<span class="number">48</span>:<span class="number">46</span> xxx xl2tpd[<span class="number">1343</span>]: result_code_avp: <span class="literal">result</span> code endianness fix <span class="keyword">for</span> buggy <span class="type">Apple</span> client. network=<span class="number">768</span>, le=<span class="number">3</span>
</code></pre><p>If you get the same error message, just follow step by step with me to setup L2TP+IPSec VPN.</p>
<h1 id="Install_xl2tpd_and_openwan">Install xl2tpd and openwan</h1><p>Here I use openswan as my IPSec server. Just use the following commands to install xl2tpd and openswan:</p>
<figure class="highlight bash"><figcaption><span>Install pptpd</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo apt-get install openswan ppp xl2tpd</span><br></pre></td></tr></table></figure>
<h1 id="Configure_xl2tpd">Configure xl2tpd</h1><p>We need to configure two files for xl2tpd: <code>/etc/xl2tpd/xl2tpd.conf</code> and <code>/etc/ppp/options.xl2tpd</code></p>
<p>Here’s an example of <code>/etc/xl2tpd/xl2tpd.conf</code> :</p>
<pre><code>[global]
listen-addr = <span class="number">106.186</span><span class="number">.127</span><span class="number">.239</span>

[lns <span class="keyword">default</span>]
ip range = <span class="number">10.20</span><span class="number">.0</span><span class="number">.2</span>-<span class="number">10.20</span><span class="number">.0</span><span class="number">.100</span>
local ip = <span class="number">10.20</span><span class="number">.0</span><span class="number">.1</span>
assign ip = yes
length bit = yes
refuse pap = yes
require authentication = yes
pppoptfile = /etc/ppp/options.xl2tpd
</code></pre><p>“ip range” defined IPs distributed to the client side and “local ip” is assigned to the server side. pppoptfile defines the detailed config file for xl2tpd.</p>
<p>Then create file <code>/etc/ppp/options.xl2tpd</code> and add:</p>
<pre><code>ms-dns <span class="number">8.8</span><span class="number">.8</span><span class="number">.8</span>
ms-dns <span class="number">8.8</span><span class="number">.4</span><span class="number">.4</span>
noccp
asyncmap <span class="number">0</span>
auth
crtscts
lock
hide-password
modem
mru <span class="number">1200</span>
nodefaultroute
debug
mtu <span class="number">1200</span>
proxyarp
lcp-echo-interval <span class="number">30</span>
lcp-echo-failure <span class="number">4</span>
ipcp-accept-local
ipcp-accept-remote
noipx
idle <span class="number">1800</span>
connect-delay <span class="number">5000</span>
</code></pre><h1 id="Configure_OpenSwan_IPSec">Configure OpenSwan IPSec</h1><p>IPSec acts as a role to provide a secure routine for transferring data. OpenSwan is a good choice to set up a simple IPSec. Note that there are many IPSec choices and they should be exclusively installed in your system. And whatever IPSec server you installed, the command to call them is only “ipsec“. Use the following command to identify which IPSec service you’re using now.</p>
<pre><code>ipsec <span class="comment">--version</span>
</code></pre><p>The config file for OpenSwan is /etc/ipsec.conf. Actually this file name is identical for all IPSec service, which the content differs anyway. When you installed another IPSec service with apt-get, you need to change the format and contents of this file.</p>
<p>Here’s an example of this file:</p>
<pre><code>version <span class="number">2.0</span>

config setup
    dumpdir=/var/run/pluto/
    nat_traversal=yes
    virtual_private=%v4:<span class="number">10.0</span><span class="number">.0</span><span class="number">.0</span>/<span class="number">8</span>,%v4:<span class="number">192.168</span><span class="number">.0</span><span class="number">.0</span>/<span class="number">16</span>,%v4:<span class="number">172.16</span><span class="number">.0</span><span class="number">.0</span>/<span class="number">12</span>,%v4:<span class="number">25.0</span><span class="number">.0</span><span class="number">.0</span>/<span class="number">8</span>,%v6:fd00::/<span class="number">8</span>,%v6:fe80::/<span class="number">10</span>
    protostack=netkey
    force_keepalive=yes
    keep_alive=<span class="number">60</span>

conn l2tp-psk
    authby=secret
    pfs=no
    <span class="keyword">auto</span>=add
    keyingtries=<span class="number">3</span>
    type=transport
    left=<span class="number">1.2</span><span class="number">.3</span><span class="number">.4</span> <span class="preprocessor"># change to your own IP</span>
    leftprotoport=<span class="number">17</span>/<span class="number">1701</span>
    right=%any
    rightprotoport=<span class="number">17</span>/%any
</code></pre><p>The “virtual_private” line shows which network could use this IPSec routine, leave it as what it is. The only line you need to change is “left”, which should be your VPS IP address.</p>
<p>Then we need to create and edit file <code>/etc/ipsec.secrets</code>.</p>
<pre><code>: PSK <span class="string">"sharedpassword"</span>
</code></pre><p><strong>Note that there’s blank before and after colon!</strong></p>
<p>“sharedpassword” should be used as the “shared secret” when you connect L2TP.</p>
<h1 id="Add_L2TP_VPN_account">Add L2TP VPN account</h1><p>Edit file <code>/etc/ppp/chap-secrets</code>, which is the same as PPTP server. Use the format like this:</p>
<pre><code>yourname <span class="keyword">*</span> yourpassword <span class="keyword">*</span>
</code></pre><h1 id="Setup_IPv4_forwarding_and_iptables_rules">Setup IPv4 forwarding and iptables rules</h1><p>It’s also the same as PPTP server, you just need to edit file <code>/etc/sysctl.conf</code> and add (or change) a following line:</p>
<pre><code>net<span class="class">.ipv4</span><span class="class">.ip_forward</span>=<span class="number">1</span>
</code></pre><p>Then exit to shell and execute:</p>
<figure class="highlight bash"><figcaption><span>Install pptpd</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo sysctl -p</span><br></pre></td></tr></table></figure>
<p>To add iptables rules,  add the following lines in <code>/etc/rc.local</code> :</p>
<pre><code>iptables -t nat -A POSTROUTING -s <span class="number">10.20</span><span class="number">.0</span><span class="number">.0</span>/<span class="number">24</span> -o venet0 -j MASQUERADE
iptables -A FORWARD -p tcp --syn -s <span class="number">10.20</span><span class="number">.0</span><span class="number">.0</span>/<span class="number">24</span> -j TCPMSS --<span class="built_in">set</span>-mss <span class="number">1356</span>
</code></pre><p>Note “-s 10.20.0.0/24” should be the net range defined in “ip range” section of <code>/etc/xl2tpd/xl2tpd.conf</code> .</p>
<p>At last, restart xl2tpd and ipsec:</p>
<figure class="highlight bash"><figcaption><span>Install pptpd</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">sudo service xl2tpd restart</span><br><span class="line">sudo service ipsec restart</span><br></pre></td></tr></table></figure>
<p>Enjoy you surfing! ;)</p>
]]></content>
    <summary type="html">
    <![CDATA[<p>L2TP+IPSec is another way to setup VPN on a VPS. L2TP consumes 1701 TCP port to maintain connection and 500/4500 UDP to transfer data. It]]>
    </summary>
    
      <category term="IPSec" scheme="http://chunqi.li/tags/IPSec/"/>
    
      <category term="L2TP" scheme="http://chunqi.li/tags/L2TP/"/>
    
      <category term="VPN" scheme="http://chunqi.li/tags/VPN/"/>
    
      <category term="VPS" scheme="http://chunqi.li/tags/VPS/"/>
    
      <category term="Fire on the Wall" scheme="http://chunqi.li/categories/Fire-on-the-Wall/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[Setup PPTP Server on a VPS]]></title>
    <link href="http://chunqi.li/2015/05/20/Setup-PPTP-Server-on-a-VPS/"/>
    <id>http://chunqi.li/2015/05/20/Setup-PPTP-Server-on-a-VPS/</id>
    <published>2015-05-20T06:45:45.000Z</published>
    <updated>2015-11-10T07:10:02.000Z</updated>
    <content type="html"><![CDATA[<p>VPS is becoming more and more cheap, fast and powerful these years. Some cheap VPS, such as Linode, Ramnode, DigitalOcean, DirectSpace, are provided for individuals. There are a lot of comparison article across the Internet and you can chose the one fit for you. Here I will list a bunch of methods to surf the internet across a firewall, and also some using experiments.</p>
<h1 id="Traditional_VPN_solutions">Traditional VPN solutions</h1><p>The traditional VPN solutions includes PPTP and L2TP+IPSec solutions. Both are the most popular VPN solutions which are support by almost any smart devices. PPTP and L2TP are all TCP-based VPN, which means a TCP connection must be contained between both ends to keep the status of VPN connection. Thus data lose or connection interrupted on these TCP connections will terminate the VPN connection. Besides these two VPNs are unable to change their TCP connection ports, that’s why PPTP and L2TP are easy to detect and blocked by the firewall. PPTP consumes TCP port 1723 and L2TP takes 1701. It differs on data transfer between these two VPNs. PPTP uses GRE packages  with value 47, which L2TP uses UDP packages via port 500 and 4500, and L2TP may also utilize ESP packages with value 50.</p>
<h1 id="Install_PPTP_server_on_a_VPS">Install PPTP server on a VPS</h1><p>It’s very easy to setup PPTP VPN on any VPS running a Linux distro. I take Ubuntu 14.04 and a ramnode OpenVZ container VPS as an example (the same environment will be used in the following article), you just need to:</p>
<figure class="highlight bash"><figcaption><span>Install pptpd</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo apt-get install pptpd</span><br></pre></td></tr></table></figure>
<p>Then configure pptpd.conf</p>
<figure class="highlight bash"><figcaption><span>Install pptpd</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo nano /etc/pptpd.conf</span><br></pre></td></tr></table></figure>
<p>change the server IP and client IP</p>
<pre><code>localip <span class="number">192.168</span><span class="number">.0</span><span class="number">.1</span>
remoteip <span class="number">192.168</span><span class="number">.0</span><span class="number">.100</span>-<span class="number">200</span>
</code></pre><p>This set the pptp server IP 192.168.0.1 to its ppp device, and distribute 192.168.0.100-200 to the client side ppp device. You could change these to any value you like. But you’d better not change it besides IP range 192.168.0.0/16 and 10.0.0.0/8, since IPs in these two ranges are assigned to LAN. IPs in other range may used by the public servers, and the NAT mechanism (which will be discussed below) may confuse the traffic from the public servers and VPN clients. Localip and remoteip should be in the same network.</p>
<p>Then uncomment the ms-dns and add google like below or OpenDNS:</p>
<pre><code>ms-dns <span class="number">8.8</span><span class="number">.8</span><span class="number">.8</span>
ms-dns <span class="number">8.8</span><span class="number">.4</span><span class="number">.4</span>
</code></pre><p>Now add a VPN user in <code>/etc/ppp/chap-secrets</code> file.</p>
<figure class="highlight bash"><figcaption><span>Install pptpd</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo nano /etc/ppp/chap-secrets</span><br></pre></td></tr></table></figure>
<p>There are four columns in this file. The first is username, choose your favorite one. The second column is service name, such as pptpd or l2tpd. You can use <em> to allow all services using this config line. The third column is your password, stored in plain test (which is awful :-( ). The fourth column presents the IPs allowed to use this config line. Leave it </em> if you want to connect the VPN from anywhere. Here’s an example:</p>
<pre><code>yourname <span class="keyword">*</span> yourpassword <span class="keyword">*</span>
</code></pre><p>Until now we finished all the configuration of PPTP server and we need to restart it.</p>
<figure class="highlight bash"><figcaption><span>Install pptpd</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo /etc/init.d/pptpd restart</span><br></pre></td></tr></table></figure>
<figure class="highlight bash"><figcaption><span>Install pptpd</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo service pptpd restart</span><br></pre></td></tr></table></figure>
<h1 id="Setup_IPv4_Forwrding">Setup IPv4 Forwrding</h1><p>Besides of the configuration above, we need to enable IPv4 forwarding and setup the rules in iptables for SNAT. To enable IPv4 forwarding permanently, you need to edit file <code>/etc/sysctl.conf</code> and add (or change) a following line:</p>
<pre><code>net<span class="class">.ipv4</span><span class="class">.ip_forward</span>=<span class="number">1</span>
</code></pre><p>Then exit to shell and execute:</p>
<figure class="highlight bash"><figcaption><span>Install pptpd</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo sysctl -p</span><br></pre></td></tr></table></figure>
<h1 id="Setup_SNAT_in_iptables">Setup SNAT in iptables</h1><p>To add a rule in iptables, you can add the following lines in <code>/etc/rc.local</code> :</p>
<pre><code>iptables -t nat -A POSTROUTING -s <span class="number">192.168</span><span class="number">.0</span><span class="number">.0</span>/<span class="number">24</span> -o venet0 -j MASQUERADE
iptables -A FORWARD -p tcp --syn -s <span class="number">192.168</span><span class="number">.0</span><span class="number">.0</span>/<span class="number">24</span> -j TCPMSS --<span class="built_in">set</span>-mss <span class="number">1356</span>
</code></pre><p>The first line means SNAT all the traffics from net 192.168.0.0/24 to the IP of local network interface venet0. If you setup PPTP server on a real machine, it maybe eth0 or em0. Check it with command <code>ifconfig</code>. If you adjust 192.168.0.0/24 to your favorite IPs in <code>localip</code> and <code>remoteip</code> sections above, you should replace -s 192.168.0.0/24 with the same IP range here.</p>
<p>The second line is a little trivial and interesting. It means iptables will change MSS field of all the TCP packages with syn in header to 1356. MSS (Maximum Segment Size) defines the maximum size of a TCP package. The default value may be 1500 in some network (1500 is the maximum size in many Ethernet lines). Since VPN will consume a few spaces in the package header, the final size of a package may be larger than the maximum size which can hold by Ethernet line.</p>
<p>There could be some wired problems without setting the second line. Without this, I can ping/traceroute some website successfully but cannot access the pages in browsers.</p>
<p>Now you could use the username and password set in /etc/ppp/chap-secrets to use PPTP VPN. Remenber to enable MPPE encryption connection.</p>
<h1 id="Enable_PPP_Support_of_VPS">Enable PPP Support of VPS</h1><p>PPP support is disabled by default by some VPS providers. You need to enable it manually. For a ramnode VPS, you need to login to its vps control panel (<a href="https://vpscp.ramnode.com/login.php" target="_blank" rel="external">https://vpscp.ramnode.com/login.php</a>), choose “Settings” tab at the bottom of the page and turn PPP on.</p>
<img src="/images/Setup-PPTP-Server-on-a-VPS-01.png">
<p>PPTP and L2TP uses PPP support by kernel, and other VPNs such as AnyConnect, OpenVPN, ShadowVPN utilize TUN/TAP support. So enable TUN/TAP as well.</p>
<p>In the following post, I will introduce how to setup L2TP+IPSec VPN in a OpenVZ VPS.</p>
]]></content>
    <summary type="html">
    <![CDATA[<p>VPS is becoming more and more cheap, fast and powerful these years. Some cheap VPS, such as Linode, Ramnode, DigitalOcean, DirectSpace, a]]>
    </summary>
    
      <category term="PPTP" scheme="http://chunqi.li/tags/PPTP/"/>
    
      <category term="VPN" scheme="http://chunqi.li/tags/VPN/"/>
    
      <category term="VPS" scheme="http://chunqi.li/tags/VPS/"/>
    
      <category term="Fire on the Wall" scheme="http://chunqi.li/categories/Fire-on-the-Wall/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[Hello World]]></title>
    <link href="http://chunqi.li/2015/05/19/hello-world/"/>
    <id>http://chunqi.li/2015/05/19/hello-world/</id>
    <published>2015-05-19T06:06:56.000Z</published>
    <updated>2015-11-10T07:10:04.000Z</updated>
    <content type="html"><![CDATA[<p>This is a Hello World page of Arthur Chunqi Li’s blog.</p>
<figure class="highlight c"><figcaption><span>hello_world.c</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    <span class="built_in">printf</span>(<span class="string">"Hello World!\n"</span>);</span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h2 id="Links:">Links:</h2><ul>
<li>Homepage: <a href="http://chunqi.li/me">http://chunqi.li/me</a></li>
<li>Blog: <a href="http://chunqi.li/">http://chunqi.li/</a> (<a href="http://xelatex.github.io" target="_blank" rel="external">backup</a>)</li>
<li>Facebook: <a href="http://facebook.chunqi.li" target="_blank" rel="external">http://facebook.chunqi.li</a></li>
<li>LinkedIn: <a href="http://linkedin.chunqi.li" target="_blank" rel="external">http://linkedin.chunqi.li</a></li>
</ul>
]]></content>
    <summary type="html">
    <![CDATA[<p>This is a Hello World page of Arthur Chunqi Li’s blog.</p>
<figure class="highlight c"><figcaption><span>hello_world.c</span></figcaption]]>
    </summary>
    
      <category term="Hello World" scheme="http://chunqi.li/tags/Hello-World/"/>
    
      <category term="Hello World" scheme="http://chunqi.li/categories/Hello-World/"/>
    
  </entry>
  
</feed>
